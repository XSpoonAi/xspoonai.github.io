SpoonOS Cookbook – Combined Markdown Export
Generated by cookbook/scripts/generate-llm.js

---

FILE: docs/api-reference/index.md

# SpoonOS API Reference (auto-generated)

Generated from spoon_ai sources. Each module below has its own page.

- [spoon_ai](./spoon_ai.md)

---

FILE: docs/api-reference/spoon_ai.md

---
id: spoon_ai
slug: /api-reference/spoon_ai.md
title: spoon_ai
---

# Table of Contents

---

FILE: docs/cli/advanced-features.md

# Advanced CLI Features

This guide covers advanced  features including MCP integration, custom agents, scripting, and automation capabilities.

## Model Context Protocol (MCP) Integration

### MCP Overview

MCP allows  to integrate with external tools and services through standardized protocols.

### Configuring MCP Servers

Add MCP server configurations to your `config.json`:

```json
{
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/Users/username/Documents"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_your_token_here"
      }
    },
    "slack": {
      "command": "uvx",
      "args": ["mcp-server-slack"],
      "env": {
        "SLACK_BOT_TOKEN": "xoxb-your-bot-token"
      }
    }
  }
}
```

### MCP Agent Configuration

Create agents with MCP support:

```json
{
  "agents": {
    "mcp_agent": {
      "class_name": "SpoonReactMCP",
      "description": "Agent with full MCP integration",
      "mcp_servers": ["filesystem", "github", "slack"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229"
      }
    }
  }
}
```

### Testing MCP Connections

```bash
# Start spoon-cli and run:
# Validate MCP server configurations
validate-config --check-servers

# Load MCP-enabled agent
load-agent mcp_agent

# List available MCP tools
action list_mcp_tools
```

## Custom Agent Development

### Agent Configuration Structure

```json
{
  "agents": {
    "custom_agent": {
      "class_name": "SpoonReactAI",
      "description": "Description of your custom agent",
      "aliases": ["ca", "custom"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.7,
        "max_tokens": 4000,
        "custom_parameter": "value"
      },
      "tools": [
        "web_search",
        "calculator",
        "custom_tool_1",
        "custom_tool_2"
      ],
      "system_prompt": "Custom system prompt for specialized behavior"
    }
  }
}
```

### Specialized Agent Types

#### Research Agent

```json
{
  "research_agent": {
    "class_name": "SpoonReactAI",
    "description": "Specialized research and analysis agent",
    "config": {
      "llm_provider": "openai",
      "model_name": "gpt-4-turbo-preview",
      "temperature": 0.3
    },
    "tools": [
      "web_search",
      "academic_search",
      "data_analysis",
      "citation_manager"
    ]
  }
}
```

#### Trading Agent

```json
{
  "trading_agent": {
    "class_name": "SpoonReactAI",
    "description": "Cryptocurrency trading assistant",
    "config": {
      "llm_provider": "anthropic",
      "model_name": "claude-3-sonnet-20240229",
      "temperature": 0.2
    },
    "tools": [
      "crypto_price_lookup",
      "dex_swap",
      "wallet_balance",
      "market_data",
      "technical_indicators"
    ]
  }
}
```

#### Code Review Agent

```json
{
  "code_review_agent": {
    "class_name": "SpoonReactAI",
    "description": "Code review and development assistant",
    "config": {
      "llm_provider": "openai",
      "model_name": "gpt-4",
      "temperature": 0.1
    },
    "tools": [
      "file_operations",
      "git_operations",
      "code_analysis",
      "testing_tools"
    ]
  }
}
```

## Scripting and Automation

### Command Chaining

Execute multiple commands in sequence:

```bash
# Setup and start session (these would be run in spoon-cli interactive mode)
# load-agent research_agent
# load-toolkit-tools web_search academic
# action chat
```

### Batch Operations

Create scripts for automated workflows:

```bash
#!/bin/bash
# research_workflow.sh

# Note: This script would need to be run in spoon-cli interactive mode
# Start spoon-cli first, then run these commands:
# load-agent research_agent
# load-toolkit-tools web_search data_analysis
# load-docs ./research_papers/
# action chat

# For automation, you would need to:
echo "Starting automated research analysis..."
# spoon-cli action chat << EOF
Please analyze the loaded documents and provide a comprehensive summary of the key findings, methodologies, and conclusions.
EOF
```

### Environment-Specific Configurations

Use different configurations for different environments:

```json
{
  "profiles": {
    "development": {
      "default_agent": "debug_agent",
      "llm": {
        "timeout": 60,
        "retry_attempts": 5
      },
      "logging": {
        "level": "DEBUG"
      }
    },
    "production": {
      "default_agent": "production_agent",
      "llm": {
        "timeout": 30,
        "retry_attempts": 2
      },
      "logging": {
        "level": "WARNING"      }
    }
  }
}
```

Activate profiles:

```bash
export SPOON_CLI_PROFILE="production"
# Start spoon-cli and run:
system-info
```

## Advanced Tool Integration

### Custom Tool Development

#### Tool Factory Pattern

```json
{
  "tool_factories": {
    "custom_api_tool": {
      "factory": "my_package.tools.CustomAPIToolFactory",
      "config": {
        "api_endpoint": "https://api.example.com",
        "api_key": "${API_KEY}"
      }
    }
  }
}
```

#### Dynamic Tool Loading

```bash
# Start spoon-cli and run:
# Load tools from specific categories
load-toolkit-tools crypto blockchain

# Load all available tools (use with caution)
load-toolkit-tools all

# Load tools with specific configuration
load-toolkit-tools web_search --timeout 30 --max-results 20
```

### Tool Configuration

Advanced tool configuration in `config.json`:

```json
{
  "tool_configs": {
    "web_search": {
      "max_results": 10,
      "timeout": 30,
      "user_agent": "SpoonAI/1.0",
      "proxies": {
        "http": "http://proxy.company.com:8080",
        "https": "https://proxy.company.com:8080"
      }
    },
    "crypto_price_lookup": {
      "cache_timeout": 300,
      "preferred_exchanges": ["binance", "coinbase", "kraken"]
    }
  }
}
```

## Performance Optimization

### LLM Configuration Tuning

```json
{
  "llm": {
    "default_provider": "anthropic",
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3,
    "rate_limiting": {
      "requests_per_minute": 60,
      "burst_limit": 10
    },
    "caching": {
      "enabled": true,
      "ttl": 3600,
      "max_size": "1GB"
    }
  }
}
```

### Memory Management

```json
{
  "memory": {
    "max_chat_history": 100,
    "compress_old_messages": true,
    "auto_save_interval": 300,
    "cleanup_temp_files": true
  }
}
```

### Parallel Processing

```json
{
  "parallel_processing": {
    "max_concurrent_requests": 5,
    "thread_pool_size": 10,
    "async_timeout": 60
  }
}
```

## Logging and Debugging

### Advanced Logging Configuration

```json
{
  "logging": {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "file": "spoon_cli.log",
    "max_file_size": "10MB",
    "backup_count": 5,
    "handlers": {
      "console": {
        "level": "WARNING",
        "format": "%(levelname)s: %(message)s"
      },
      "file": {
        "level": "DEBUG",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
      }
    }
  }
}
```

### Debug Mode

Enable detailed debugging:

```bash
export SPOON_CLI_DEBUG=1

# Or use command-line flag when starting spoon-cli
# spoon-cli --debug

# Then run:
# system-info
```

### Performance Profiling

```bash
# Enable performance logging
export SPOON_CLI_PROFILE=1

# Start spoon-cli and run:
# action chat

# Check performance metrics
# system-info --performance
```

## Security Features

### API Key Management

```json
{
  "security": {
    "api_key_rotation": {
      "enabled": true,
      "interval_days": 30
    },
    "key_validation": {
      "enabled": true,
      "cache_timeout": 3600
    },
    "audit_logging": {
      "enabled": true,
      "log_sensitive_operations": false
    }
  }
}
```

### Network Security

```json
{
  "network": {
    "ssl_verification": true,
    "proxy": "http://proxy.company.com:8080",
    "timeout": 30,
    "retry_on_failure": true,
    "trusted_domains": [
      "*.openai.com",
      "*.anthropic.com",
      "*.googleapis.com"
    ]
  }
}
```

## Integration Features

### Webhook Support

Configure webhooks for external integrations:

```json
{
  "webhooks": {
    "telegram_bot": {
      "enabled": true,
      "token": "${TELEGRAM_BOT_TOKEN}",
      "allowed_users": ["user1", "user2"]
    },
    "slack_integration": {
      "enabled": false,
      "webhook_url": "${SLACK_WEBHOOK_URL}",
      "channel": "#ai-notifications"
    }
  }
}
```

### API Server Mode

Run as an API server:

```bash
# Start spoon-cli and run:
serve --host 0.0.0.0 --port 8000
```

### Database Integration

```json
{
  "database": {
    "type": "postgresql",
    "url": "${DATABASE_URL}",
    "connection_pool": {
      "min_size": 5,
      "max_size": 20
    }
  }
}
```

## Custom Extensions

### Plugin System

Load custom plugins:

```json
{
  "plugins": {
    "my_custom_plugin": {
      "path": "/path/to/my/plugin",
      "enabled": true,
      "config": {
        "custom_setting": "value"
      }
    }
  }
}
```

### Custom Commands

Extend CLI with custom commands:

```python
# custom_commands.py
from spoon_cli.commands import SpoonCommand

def my_custom_command(input_list):
    # Custom command implementation
    print("Custom command executed!")

# Register command
custom_command = SpoonCommand(
    name="my-command",
    description="My custom command",
    handler=my_custom_command
)
```

## Monitoring and Analytics

### Usage Analytics

```json
{
  "analytics": {
    "enabled": true,
    "metrics": {
      "commands_used": true,
      "response_times": true,
      "error_rates": true
    },
    "reporting": {
      "interval": "1h",
      "destination": "console"
    }
  }
}
```

### Health Monitoring

```bash
# Start spoon-cli and run:
# Continuous health monitoring
monitor --interval 60

# Export metrics
export-metrics --format json --output metrics.json
```

## Next Steps

- [Troubleshooting](./troubleshooting.md) - Solve advanced issues
- [Configuration Guide](./configuration.md) - Master configuration options
- [API Reference](../api-reference/index) - Complete command reference

---

FILE: docs/cli/basic-usage.md

# Basic CLI Usage

This guide covers the fundamental operations of spoon-cli, from starting your first session to performing common tasks.

## Starting the CLI

### Interactive Mode

The most common way to use spoon-cli is through interactive mode:

```bash
spoon-cli
```

This starts the interactive command-line interface where you can enter commands and chat with agents.

### Interactive Commands

Once in the spoon-cli interactive mode, you can run various commands:

```bash
# Load an agent and start chat
action chat

# List available agents
list-agents

# Show system information
system-info
```

## Core Commands

### Help System

Get help on available commands:

```bash
# Show all commands
help

# Get help for specific command
help load-agent
```

### Agent Management

#### List Available Agents

```bash
list-agents
```

Shows all configured agents with their descriptions and aliases.

#### Load an Agent

```bash
load-agent react
# or using alias
load-agent r
```

Loads the specified agent for use in subsequent operations.

#### Check Current Agent

The prompt shows the currently loaded agent:

```
Spoon AI (react) >
```

### Chat Operations

#### Start Chat Mode

```bash
action chat
```

Starts an interactive chat session with the current agent.

#### Create New Chat

```bash
new-chat
```

Clears the current chat history and starts fresh.

#### List Chat Histories

```bash
list-chats
```

Shows available saved chat sessions.

#### Load Previous Chat

```bash
load-chat react_session_001
```

Loads a previously saved chat session.

### Configuration Management

#### View Configuration

```bash
config
```

Shows all current configuration settings.

#### Set Configuration Values

```bash
# Set API key
config api_key openai "sk-your-key-here"

# Set default agent
config default_agent "my_agent"

# Set LLM provider
config llm.default_provider "anthropic"
```

#### Reload Configuration

```bash
reload-config
```

Reloads configuration after making changes to config files.

## Interactive Chat Mode

### Starting Chat

Once you enter chat mode (`action chat`), you'll see:

```
Spoon AI (react) > action chat
Starting chat with react
Type your message and press Enter to send.
Press Ctrl+C or Ctrl+D to exit chat mode.
Chat log will be saved to: chat_logs/chat_react_20241201_143022.txt

You >
```

### Basic Chat Interaction

```bash
You > Hello, can you help me analyze some cryptocurrency data?

react: I'd be happy to help you analyze cryptocurrency data. What specific cryptocurrencies or data are you interested in? I have access to various crypto analysis tools including price lookups, market data, and technical indicators.
```

### Special Commands in Chat

While in chat mode, you can use special commands:

- `Ctrl+C` or `Ctrl+D`: Exit chat mode
- Multi-line input: Continue typing, press Enter twice to send

### Chat History

All conversations are automatically saved to timestamped files in the `chat_logs/` directory.

## Tool Integration

### List Available Toolkits

```bash
list-toolkit-categories
```

Shows all available tool categories.

### List Tools in Category

```bash
list-toolkit-tools crypto
```

Shows tools available in the crypto category.

### Load Toolkit Tools

```bash
load-toolkit-tools crypto web_search
```

Loads tools from specified categories into the current agent.

## Document Operations

### Load Documents

```bash
load-docs /path/to/documents
load-docs /path/to/file.pdf
load-docs /path/to/folder --glob "*.txt"
```

Loads documents into the current agent for analysis and querying.

## System Information

### Health Check

```bash
system-info
```

Provides comprehensive system information including:
- Environment variables status
- Configuration file status
- Available agents and tools
- API key configuration
- Health score and recommendations

### LLM Status

```bash
llm-status
```

Shows LLM provider configuration and availability status.

## Blockchain Operations

### Token Information

```bash
token-info 0xA0b86a33E6441e88C5F2712C3E9b74E39E9f6E5a
token-by-symbol ETH
```

Get information about specific tokens.

### Transfer Tokens

```bash
transfer 0x742d35Cc6634C0532925a3b844Bc454e4438f44e 1.5
transfer 0x742d35Cc6634C0532925a3b844Bc454e4438f44e 100 USDC
```

Transfer native tokens or ERC-20 tokens.

### Token Swapping

```bash
swap ETH USDC 1.0
swap UNI WETH 100 --slippage 0.5
```

Swap tokens using integrated DEX aggregator.

## Social Media Integration

### Telegram Bot

```bash
telegram
```

Starts the Telegram bot client for social media interactions.

## Configuration Validation

### Validate Configuration

```bash
validate-config
```

Checks configuration for issues and missing requirements.

### Check Migration Status

```bash
check-config
```

Checks if configuration needs migration to new format.

### Migrate Configuration

```bash
migrate-config
```

Migrates legacy configuration to the new unified format.

## Command-line Options

### Global Options

```bash
--help              # Show help
--version           # Show version
--config FILE       # Use specific config file
--debug             # Enable debug mode
```

### Command-specific Options

```bash
migrate-config --dry-run    # Preview migration
validate-config --check-env # Check environment variables
```

## Examples

### Complete Workflow

```bash
# Start spoon-cli
spoon-cli

# Then run these commands in interactive mode:
# 1. Check system status
system-info

# 2. Configure API keys
config api_key openai "sk-your-key"
config api_key anthropic "sk-ant-your-key"

# 3. List and load agent
list-agents
load-agent react

# 4. Load useful tools
load-toolkit-tools crypto web_search

# 5. Start chatting
action chat
```

### Crypto Analysis Session

```bash
# Load crypto-focused agent
load-agent crypto_analyzer

# Load crypto tools
load-toolkit-tools crypto

# Analyze specific token
action chat
# Then ask: "Analyze the current market sentiment for BTC and ETH"
```

### Document Analysis

```bash
# Load agent and documents
load-agent react
load-docs ./research_papers/

# Start analysis
action chat
# Then ask: "Summarize the key findings from the loaded documents"
```

## Keyboard Shortcuts

### Main CLI
- `Ctrl+C`: Interrupt current operation
- `Ctrl+D`: Exit CLI (at main prompt)
- `↑/↓`: Navigate command history
- `Tab`: Auto-complete commands

### Chat Mode
- `Ctrl+C`: Exit chat mode
- `Ctrl+D`: Exit chat mode
- `↑/↓`: Navigate message history

## Best Practices

### Session Management

1. **Use descriptive agent names** for different tasks
2. **Save important chats** with meaningful filenames
3. **Regularly update configuration** as needs change

### Performance

1. **Load only needed tools** to reduce startup time
2. **Use appropriate models** for your tasks
3. **Monitor system resources** with `system-info`

### Security

1. **Never share API keys** in chat sessions
2. **Use environment variables** for sensitive data
3. **Regularly rotate API keys**

## Next Steps

- [Advanced Features](./advanced-features.md) - Learn advanced CLI capabilities
- [Configuration Guide](./configuration.md) - Deep dive into configuration options
- [Troubleshooting](./troubleshooting.md) - Solve common issues

---

FILE: docs/cli/configuration.md

# CLI Configuration

The  uses a flexible, multi-layered configuration system that allows you to customize behavior through various methods. This guide covers all configuration options and methods.

## Configuration Hierarchy

Configurations are applied in the following order (later sources override earlier ones):

1. **Built-in defaults** - Predefined sensible defaults
2. **Global config file** - `~/.spoon/config.json` or `~/.config/spoon/config.json`
3. **Project config file** - `./config.json` in current directory
4. **Environment variables** - Shell environment variables
5. **Command-line arguments** - Flags and options passed to commands

## Configuration File

The primary configuration method is through a JSON configuration file. Create `config.json` in your project root or home directory.

### Basic Configuration

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-your-openai-key-here",
    "anthropic": "sk-ant-your-anthropic-key-here"
  },
  "agents": {
    "my_agent": {
      "class_name": "SpoonReactAI",
      "description": "My custom agent",
      "config": {
        "llm_provider": "openai",
        "model_name": "gpt-4",
        "temperature": 0.7
      },
      "tools": ["web_search", "calculator"]
    }
  }
}
```

### Advanced Configuration

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-...",
    "anthropic": "sk-ant-...",
    "deepseek": "sk-...",
    "gemini": "your-gemini-key"
  },
  "agents": {
    "react": {
      "class_name": "SpoonReactAI",
      "description": "Standard React agent",
      "aliases": ["r"],
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.7,
        "max_tokens": 4000
      },
      "tools": ["web_search", "file_operations", "calculator"]
    },
    "mcp_agent": {
      "class_name": "SpoonReactMCP",
      "description": "Agent with MCP support",
      "config": {
        "llm_provider": "openai",
        "model_name": "gpt-4-turbo-preview"
      },
      "mcp_servers": ["filesystem", "github"]
    }
  },
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/Users/username/Documents"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_..."
      }
    },
    "slack": {
      "command": "uvx",
      "args": ["mcp-server-slack"],
      "env": {
        "SLACK_BOT_TOKEN": "xoxb-..."
      }
    }
  },
  "llm": {
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3
  },
  "logging": {
    "level": "INFO",
    "file": "spoon_cli.log",
    "max_file_size": "10MB",
    "backup_count": 5
  }
}
```

## Environment Variables

Environment variables provide an alternative way to configure sensitive information like API keys.

### LLM Provider API Keys

```bash
# Primary LLM providers
export OPENAI_API_KEY="sk-proj-..."
export ANTHROPIC_API_KEY="sk-ant-api03-..."
export DEEPSEEK_API_KEY="sk-..."

# Additional providers
export GEMINI_API_KEY="AIza..."
export OPENROUTER_API_KEY="sk-or-v1-..."
export TAVILY_API_KEY="tvly-..."
```

### Blockchain Configuration

```bash
export PRIVATE_KEY="0x..."
export RPC_URL="https://mainnet.infura.io/v3/YOUR_PROJECT_ID"
export CHAIN_ID="1"
export SCAN_URL="https://etherscan.io"
```

### Application Settings

```bash
export SPOON_CLI_CONFIG_FILE="/path/to/custom/config.json"
export SPOON_CLI_LOG_LEVEL="DEBUG"
export SPOON_CLI_DEFAULT_AGENT="my_agent"
```

## Configuration Management Commands

The CLI provides built-in commands to manage configuration:

### View Configuration

```bash
# Start spoon-cli and run:
# Show all configuration
config

# Show specific key
config api_keys.openai

# Show LLM status
llm-status
```

### Modify Configuration

```bash
# Start spoon-cli and run:
# Set API key
config api_key openai "sk-your-key"

# Set configuration value
config default_agent "my_custom_agent"

# Set nested configuration
config llm.timeout 60
```

### Configuration Validation

```bash
# Start spoon-cli and run:
# Validate current configuration
validate-config

# Check for migration needs
check-config

# Migrate legacy configuration
# Start spoon-cli and run:
migrate-config
```

## Configuration Sections

### Agents Configuration

Define custom agents with specific configurations:

```json
{
  "agents": {
    "crypto_trader": {
      "class_name": "SpoonReactAI",
      "description": "Cryptocurrency trading assistant",
      "config": {
        "llm_provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.3,
        "max_tokens": 2000
      },
      "tools": [
        "crypto_price_lookup",
        "dex_swap",
        "wallet_balance",
        "market_data"
      ]
    }
  }
}
```

### MCP Servers Configuration

Configure Model Context Protocol servers for extended capabilities:

```json
{
  "mcp_servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/allowed/path"],
      "env": {}
    },
    "git": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-git", "--repository", "/path/to/repo"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_..."
      }
    }
  }
}
```

### LLM Configuration

Configure LLM provider settings and fallback chains:

```json
{
  "llm": {
    "default_provider": "anthropic",
    "fallback_chain": ["anthropic", "openai", "deepseek"],
    "timeout": 30,
    "retry_attempts": 3,
    "providers": {
      "openai": {
        "base_url": null,
        "organization": null
      },
      "anthropic": {
        "max_tokens": 4096
      }
    }
  }
}
```

## Configuration Profiles

Use different configurations for different environments:

```json
{
  "profiles": {
    "development": {
      "default_agent": "debug_agent",
      "llm": {
        "timeout": 60,
        "retry_attempts": 5
      }
    },
    "production": {
      "default_agent": "production_agent",
      "logging": {
        "level": "WARNING"
      }
    }
  }
}
```

Activate a profile:

```bash
export SPOON_CLI_PROFILE="production"

# Then start spoon-cli with the profile active
```

## Configuration File Locations

The CLI searches for configuration files in the following order:

1. **Project-specific**: `./config.json`
2. **User-specific**: `~/.spoon/config.json`
3. **System-wide**: `/etc/spoon/config.json`
4. **XDG Base Directory**: `~/.config/spoon/config.json`

## Security Best Practices

### API Keys Management

1. **Never commit API keys** to version control
2. **Use environment variables** for sensitive data
3. **Rotate keys regularly**
4. **Use different keys** for different environments

### File Permissions

```bash
# Set restrictive permissions on config files
chmod 600 config.json
chmod 600 .env
```

### Environment Separation

Use different configurations for different environments:

```bash
# Development
cp config.json config.dev.json
export SPOON_CLI_CONFIG_FILE="config.dev.json"

# Production
cp config.json config.prod.json
export SPOON_CLI_CONFIG_FILE="config.prod.json"
```

## Troubleshooting Configuration

### Common Issues

1. **Configuration not loading**
   ```bash
# Start spoon-cli and run:
validate-config
# Check file syntax and permissions
   ```

2. **API keys not working**
   ```bash
# Start spoon-cli and run:
llm-status
# Verify key format and validity
   ```

3. **MCP servers not connecting**
   ```bash
# Start spoon-cli and run:
validate-config --check-servers
# Check server commands and environment variables
   ```

### Debug Configuration

Enable debug logging to troubleshoot configuration issues:

```bash
export SPOON_CLI_DEBUG=1
# Start spoon-cli and run:
config
```

## Next Steps

After configuring :
- [Basic Usage](./basic-usage.md) - Learn basic CLI operations
- [Advanced Features](./advanced-features.md) - Explore advanced capabilities
- [Troubleshooting](./troubleshooting.md) - Solve common issues

---

FILE: docs/cli/installation.md

# CLI Installation

The  is the command-line interface for SpoonAI, providing an easy way to interact with AI agents, manage configurations, and perform various operations.

## Installation Methods

### Method 1: Install from PyPI (Recommended)

```bash
pip install spoon-cli
```

This installs the latest stable version of  along with all necessary dependencies.

### Method 2: Install from Source

If you want to install from source or contribute to development:

```bash
# Clone the repository
git clone https://github.com/XSpoonAi/spoon-cli.git
cd spoon-cli/
# Install in development mode
pip install -e .
```

## System Requirements

### Minimum Requirements

- **Python**: 3.12 or higher
- **Operating System**: Linux, macOS, or Windows
- **Memory**: 4GB RAM minimum, 8GB recommended
- **Disk Space**: 500MB for installation

### Recommended Requirements

- **Python**: 3.12+
- **Memory**: 16GB RAM
- **CPU**: Multi-core processor

## Dependencies

The  automatically installs the following core dependencies:

- **spoon-ai-sdk**: Core SpoonAI framework
- **spoon-toolkits**: Additional tool collections
- **prompt_toolkit**: Enhanced command-line interface
- **python-dotenv**: Environment variable management
- **fastmcp**: MCP (Model Context Protocol) support
- **pydantic**: Data validation
- **websockets**: WebSocket communication

## Verification

After installation, verify the installation by checking the version:

```bash
# Check version
spoon-cli --version
```

Or start the interactive CLI:

```bash
# Start spoon-cli
spoon-cli
```

## Post-Installation Setup

### 1. Configure Environment Variables

Create a `.env` file in your project directory or set environment variables:

```bash
# LLM API Keys (choose at least one)
export OPENAI_API_KEY="sk-your-openai-key"
export ANTHROPIC_API_KEY="sk-ant-your-anthropic-key"
export DEEPSEEK_API_KEY="your-deepseek-key"

# Optional: Blockchain operations
export PRIVATE_KEY="your-wallet-private-key"
export RPC_URL="https://mainnet.infura.io/v3/YOUR_PROJECT_ID"
```

### 2. Create Configuration File

Create a `config.json` file for advanced configuration (optional but recommended):

```json
{
  "default_agent": "react",
  "api_keys": {
    "openai": "sk-your-key-here"
  }
}
```

### 3. Test Installation

Run the system health check:

```bash
# Start spoon-cli and run:
system-info
```

This command will show:
- System information
- Environment variable status
- Configuration file status
- Available agents and tools

## Troubleshooting Installation

### Common Issues

1. **Python Version Too Old**

```bash
python --version
# Should show 3.12 or higher
```

2. **Permission Denied**

```bash
# Use user installation
pip install --user
# Or use virtual environment
python -m venv spoon_env
source spoon_env/bin/activate  # Linux/macOS
# spoon_env\Scripts\activate    # Windows
pip install
```

3. **Dependency Conflicts**

```bash
# Upgrade pip
pip install --upgrade pip

# Install in isolated environment
pip install --isolated
```

### Windows-Specific Issues

On Windows, you might need to:

1. **Add Python to PATH** during installation
2. **Use Command Prompt or PowerShell** instead of bash
3. **Install Microsoft Visual C++ Redistributable** if required

## Upgrading

To upgrade to the latest version:

```bash
pip install --upgrade spoon-cli
```

## Uninstalling

To remove spoon-cli:

```bash
pip uninstall spoon-cli
```

Note: This will not remove configuration files or chat histories you may have created.

## Next Steps

After installation, proceed to:
- [Configuration Guide](./configuration.md) - Learn how to configure
- [Basic Usage](./basic-usage.md) - Start using
- [Advanced Features](./advanced-features.md) - Explore advanced capabilities

---

FILE: docs/cli/troubleshooting.md

# CLI Troubleshooting

This guide helps you diagnose and resolve common issues with . Follow the systematic approach to identify and fix problems.

## Quick Diagnosis

### System Health Check

Run the comprehensive health check first:

```bash
# Start  and run:
system-info
```

This command shows:
- Environment variables status
- Configuration file validity
- API key configuration
- Agent and tool availability
- Overall health score

### LLM Provider Status

Check LLM provider configuration:

```bash
# Start  and run:
llm-status
```

This shows:
- Available providers
- API key status (masked)
- Default provider
- Fallback chain configuration

## Configuration Issues

### Configuration File Problems

#### Invalid JSON Syntax

**Symptoms:**
- Configuration loading errors
- CLI fails to start

**Diagnosis:**
```bash
# Start spoon-cli and run:
validate-config
```

**Solutions:**
1. Check JSON syntax with online validator
2. Ensure proper quoting of strings
3. Validate file encoding (should be UTF-8)

#### Configuration Not Loading

**Symptoms:**
- Settings not applied
- Default values used instead

**Diagnosis:**
```bash
# Check if config file exists and is readable
ls -la config.json

# Start spoon-cli and run:
check-config
```

**Solutions:**
1. Ensure `config.json` is in current directory or `~/.spoon/config.json`
2. Check file permissions: `chmod 644 config.json`
3. Validate JSON format

### Environment Variable Issues

#### API Keys Not Recognized

**Symptoms:**
- LLM provider unavailable
- Authentication errors

**Diagnosis:**
```bash
# Check environment variables
env | grep -E "(OPENAI|ANTHROPIC|DEEPSEEK)"

# Test specific provider
 llm-status
```

**Solutions:**
1. Set environment variables correctly:
   ```bash
   export OPENAI_API_KEY="sk-your-key-here"
   export ANTHROPIC_API_KEY="sk-ant-your-key-here"
   ```

2. Use `.env` file for persistent configuration:
   ```bash
   echo "OPENAI_API_KEY=sk-your-key-here" > .env
   ```

3. Restart shell or reload environment

### Migration Issues

#### Legacy Configuration Detected

**Symptoms:**
- Migration warnings
- Some features not working

**Diagnosis:**
```bash
# Start spoon-cli and run:
check-config
```

**Solutions:**
```bash
# Preview migration
# Start spoon-cli and run:
migrate-config --dry-run

# Perform migration
# Start spoon-cli and run:
migrate-config

# Validate after migration
# Start spoon-cli and run:
validate-config
```

## Agent Loading Issues

### Agent Not Found

**Symptoms:**
- "Agent not found" error
- Cannot load specific agent

**Diagnosis:**
```bash
# Start spoon-cli and run:
# List available agents
list-agents

# Check configuration
config agents
```

**Solutions:**
1. Verify agent name in configuration
2. Check agent class name is correct
3. Ensure required dependencies are installed

### Tool Loading Failures

**Symptoms:**
- Tools not available after loading
- Agent lacks expected capabilities

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Check tool loading status
tool-status

# List available toolkits
list-toolkit-categories
```

**Solutions:**
1. Reload agent configuration:
   ```bash
# Start spoon-cli and run:
reload-config
   ```

2. Reinstall toolkits:
   ```bash
   pip install --upgrade spoon-toolkits
   ```

3. Check network connectivity for external tools

## Network and Connectivity Issues

### MCP Server Connection Problems

**Symptoms:**
- MCP tools unavailable
- Server connection timeouts

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Validate MCP configuration
validate-config --check-servers

# Test specific server
# Start spoon-cli and run:
load-agent mcp_agent
```

**Solutions:**
1. Verify server commands are installed:
   ```bash
   which npx
   which uvx
   ```

2. Check server environment variables
3. Update server configurations in `config.json`

### API Rate Limiting

**Symptoms:**
- Requests failing with rate limit errors
- Intermittent connectivity issues

**Solutions:**
1. Implement request throttling:
   ```json
   {
     "llm": {
       "rate_limiting": {
         "requests_per_minute": 30,
         "burst_limit": 5
       }
     }
   }
   ```

2. Switch to different provider temporarily
3. Upgrade API plan for higher limits

## Performance Issues

### Slow Startup

**Symptoms:**
- CLI takes long time to start
- Agent loading is slow

**Diagnosis:**
```bash
# Enable profiling
export SPOON_CLI_PROFILE=1
 system-info
```

**Solutions:**
1. Load fewer tools initially
2. Use faster LLM providers
3. Enable caching:
   ```json
   {
     "llm": {
       "caching": {
         "enabled": true,
         "ttl": 3600
       }
     }
   }
   ```

### High Memory Usage

**Symptoms:**
- System running out of memory
- Performance degradation

**Solutions:**
1. Reduce chat history size:
   ```json
   {
     "memory": {
       "max_chat_history": 50,
       "compress_old_messages": true
     }
   }
   ```

2. Enable memory cleanup:
   ```json
   {
     "memory": {
       "cleanup_temp_files": true,
       "auto_save_interval": 300
     }
   }
   ```

### Slow Response Times

**Diagnosis:**
```bash
# Start spoon-cli and run:
# Check LLM provider status
llm-status

# Test response time
time spoon-cli action chat <<< "Hello"
```

**Solutions:**
1. Switch to faster models
2. Adjust timeout settings:
   ```json
   {
     "llm": {
       "timeout": 60,
       "retry_attempts": 2
     }
   }
   ```

3. Enable response streaming for better perceived performance

## Installation Issues

### Import Errors

**Symptoms:**
- "Module not found" errors
- CLI fails to start

**Diagnosis:**
```bash
# Check Python path
python -c "import spoon_ai; print(spoon_ai.__file__)"

# Verify installation
pip list | grep spoon
```

**Solutions:**
1. Reinstall packages:
   ```bash
   pip uninstall  spoon-ai-sdk spoon-toolkits
   pip install    ```

2. Check Python version compatibility
3. Use virtual environment to avoid conflicts

### Permission Issues

**Symptoms:**
- Cannot write configuration files
- Installation fails

**Solutions:**
1. Use user installation:
   ```bash
   pip install --user    ```

2. Fix directory permissions:
   ```bash
   chmod 755 ~/.spoon/
   chmod 644 ~/.spoon/config.json
   ```

## Platform-Specific Issues

### Windows Issues

#### Command Prompt Problems

**Symptoms:**
- Interactive features not working
- Display issues

**Solutions:**
1. Use PowerShell instead of CMD
2. Install Windows Terminal
3. Enable ANSI color support

#### Path Issues

**Solutions:**
1. Add Python to PATH during installation
2. Use absolute paths in configuration
3. Avoid spaces in installation paths

### macOS Issues

#### SIP (System Integrity Protection)

**Solutions:**
1. Install in user directory:
   ```bash
   pip install --user    ```

2. Use Homebrew for Python:
   ```bash
   brew install python
   ```

### Linux Issues

#### Shared Library Problems

**Diagnosis:**
```bash
ldd $(which python) | grep "not found"
```

**Solutions:**
1. Install missing system libraries
2. Use system package manager:
   ```bash
   # Ubuntu/Debian
   sudo apt-get install python3-dev build-essential

   # CentOS/RHEL
   sudo yum install python3-devel gcc
   ```

## Debugging Techniques

### Enable Debug Mode

```bash
# Enable debug logging
export SPOON_CLI_DEBUG=1

# Or use flag
 --debug system-info
```

### Log Analysis

```bash
# Check log files
tail -f spoon_cli.log

# Increase log verbosity
export SPOON_CLI_LOG_LEVEL=DEBUG
```

### Network Debugging

```bash
# Test network connectivity
curl -I https://api.openai.com/v1/models

# Check proxy settings
env | grep -i proxy
```

### Configuration Debugging

```bash
# Show full configuration with debug
 --debug config

# Validate with detailed output
 validate-config --verbose
```

## Common Error Messages

### "Configuration validation failed"

**Cause:** Invalid configuration syntax or missing required fields

**Solution:**
```bash
 validate-config
# Fix reported issues
 reload-config
```

### "Agent class not found"

**Cause:** Invalid agent class name in configuration

**Solution:**
- Check class name spelling
- Ensure correct module imports
- Update to valid class names

### "Tool loading failed"

**Cause:** Tool dependencies not available

**Solution:**
```bash
pip install --upgrade spoon-toolkits
 reload-config
```

### "MCP server connection timeout"

**Cause:** Network issues or server configuration problems

**Solution:**
```bash
 validate-config --check-servers
# Fix server configurations
 reload-config
```

## Getting Help

### Community Resources

1. **GitHub Issues:** Report bugs and request features
2. **Documentation:** Check the full documentation
3. **Discord/Forum:** Community discussions

### Debug Information Collection

When reporting issues, include:

```bash
# System information
 system-info

# Configuration (mask sensitive data)
 config

# Error logs
tail -n 50 spoon_cli.log

# Python environment
python --version
pip list | grep spoon
```

## Preventive Maintenance

### Regular Updates

```bash
# Update CLI and dependencies
pip install --upgrade  spoon-ai-sdk spoon-toolkits

# Update configuration if needed
 migrate-config
```

### Configuration Backup

```bash
# Backup configuration
cp config.json config.json.backup

# Backup environment
env | grep SPOON > spoon_env.backup
```

### Health Monitoring

```bash
# Regular health checks
 system-info

# Monitor for issues
 validate-config
```

## Emergency Recovery

### Reset Configuration

```bash
# Remove corrupted config
rm config.json

# Start with minimal config
echo '{"default_agent": "react"}' > config.json

# Test basic functionality
 list-agents
```

### Clean Reinstall

```bash
# Complete cleanup
pip uninstall  spoon-ai-sdk spoon-toolkits
rm -rf ~/.spoon/
rm -rf chat_logs/

# Fresh install
pip install
# Basic setup
 config api_key openai "your-key"
```

## Next Steps

- [Installation](./installation.md) - Reinstall if needed
- [Configuration](./configuration.md) - Review configuration options
- [Basic Usage](./basic-usage.md) - Verify basic functionality works

---

FILE: docs/core-concepts/agents-detailed.md

---
sidebar_position: 5
---

# Agents (Detailed Guide)

This guide provides a comprehensive walkthrough for developing and configuring agents in SpoonOS. Agents combine language models with tools to create intelligent, autonomous systems that reason about problems, plan action sequences, execute tools, and adapt based on feedback.

## Agent Architecture

SpoonOS agents follow the **ReAct (Reasoning + Acting)** pattern:

```text
Thought → Action → Observation → Thought → Action → ...
```

### Core Components

#### Built-in Agent Classes

SpoonOS provides two main agent classes:

- **`SpoonReactAI`**: Standard agent for blockchain operations
- **`SpoonReactMCP`**: Enhanced agent with MCP protocol support

#### Tool Integration

Agents can use various types of tools:

- **Built-in Tools**: Python classes integrated directly into the framework
- **MCP Tools**: External tools accessed via Model Context Protocol
- **Custom Tools**: User-defined tools for specific functionality

### Technical Implementation

#### Creating Custom Agents

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools import ToolManager

class SpoonMacroAnalysisAgent(SpoonReactMCP):
    name: str = "SpoonMacroAnalysisAgent"
    system_prompt: str = (
        '''You are a cryptocurrency market analyst. Your task is to provide a comprehensive
        macroeconomic analysis of a given token.

        To do this, you will perform the following steps:
        1. Use the `crypto_power_data_cex` tool to get the latest candlestick data and
           technical indicators.
        2. Use the `tavily-search` tool to find the latest news and market sentiment.
        3. Synthesize the data from both tools to form a holistic analysis.'''
    )

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.available_tools = ToolManager([])
```

#### MCP Tool Configuration

**Stdio-based Transport:**
```python
from spoon_ai.tools import MCPTool

tavily_tool = MCPTool(
    name="tavily-search",
    description="Performs a web search using the Tavily API.",
    mcp_config={
        "command": "npx",
        "args": ["--yes", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
    }
)
```

**HTTP Transport:**
```python
firecrawl_tool = MCPTool(
    name="firecrawl-scraper",
    description="Advanced web scraping tool",
    mcp_config={
        "url": "https://mcp.firecrawl.dev/scrape",
        "headers": {
            "Authorization": f"Bearer {os.getenv('FIRECRAWL_API_KEY')}",
            "Content-Type": "application/json"
        }
    }
)
```

1. **LLM (Language Model)** - The "brain" that provides reasoning capabilities
2. **Tools** - External capabilities the agent can use
3. **Memory** - Context and conversation history
4. **System Prompt** - Instructions that define the agent's behavior

#### Built-in Tools Integration

```python
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool
from spoon_toolkits.crypto.crypto_data_tools.price_data import GetTokenPriceTool

# Initialize built-in tools
crypto_tool = CryptoPowerDataCEXTool(
    exchange="binance",
    symbol="BTC/USDT",
    timeframe="1h",
    limit=100
)

price_tool = GetTokenPriceTool(
    exchange="uniswap"
)

# Add tools to agent
agent.available_tools.add_tools(crypto_tool, price_tool)
```
5. **Execution Loop** - The ReAct cycle that drives agent behavior

## Agent Types

### 1. SpoonReactAI

The standard ReAct agent for tool-enabled interactions:

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools import ToolManager

class MyAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-4.1")

        # Set system prompt
        self.system_prompt = "You are a helpful AI assistant."

        # Configure execution parameters
        self.max_steps = 10

        # Set up tools (if any)
        self.available_tools = ToolManager([])
```

### 2. SpoonReactMCP

MCP-enabled agent with dynamic tool loading:

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.chat import ChatBot

# Agent with MCP tool support
agent = SpoonReactMCP(
    llm=ChatBot(),
    system_prompt="You are an expert research assistant.",
    max_steps=15
)

# When using the CLI (main.py / spoon-cli), agents and MCP tools are defined in the CLI's `config.json` file.
# When using the SDK directly (as in this example), configure tools and agents in Python code instead of reading config.json.
```

## Creating Custom Agents

### Basic Custom Agent

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools import ToolManager
from spoon_ai.chat import ChatBot

class WeatherTool(BaseTool):
    name: str = "get_weather"
    description: str = "Get current weather for a location"
    parameters: dict = {
        "type": "object",
        "properties": {
            "location": {"type": "string", "description": "City name"}
        },
        "required": ["location"]
    }

    async def execute(self, location: str) -> str:
        # Implement weather API call
        return f"Weather in {location}: Sunny, 72°F"

class WeatherAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot()

        # Set system prompt
        self.system_prompt = """
        You are a weather assistant. Use the get_weather tool to provide
        accurate weather information for any location the user asks about.
        """

        # Set up tools
        weather_tool = WeatherTool()
        self.available_tools = ToolManager([weather_tool])
```

### Advanced Agent with Multiple Tools

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools import ToolManager
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.chat import ChatBot
import os

class ResearchAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-4.1")

        # Set up MCP tools
        tools = []

        # Web search tool (requires TAVILY_API_KEY)
        if os.getenv("TAVILY_API_KEY"):
            search_tool = MCPTool(
                name="web_search",
                description="Search the web for current information",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "tavily-mcp"],
                    "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
                    "transport": "stdio"
                }
            )
            tools.append(search_tool)

        self.available_tools = ToolManager(tools)
        self.system_prompt = """
        You are a research assistant with access to web search tools.

        When asked to research a topic:
        1. Search for current information using available tools
        2. Analyze and synthesize findings
        3. Provide well-structured summaries
        4. Always cite your sources

        Be thorough and accurate in your research.
        """
```

## Agent Configuration

### System Prompts

System prompts define your agent's personality and behavior:

```python
# Task-specific prompt
system_prompt = """
You are a crypto trading assistant. Your role is to:
- Analyze market data and trends
- Provide trading insights and recommendations
- Help users understand market conditions
- Always include risk warnings in trading advice
"""

# Personality-driven prompt
system_prompt = """
You are a friendly and enthusiastic coding mentor. You:
- Explain concepts clearly with examples
- Encourage learning and experimentation
- Provide constructive feedback
- Use emojis and positive language
"""
```

### Agent Parameters

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

class ConfigurableAgent(SpoonReactAI):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM with specific parameters
        self.llm = ChatBot(
            model_name="gpt-4.1",
            temperature=0.7,        # Creativity level
            max_tokens=4096,        # Response length limit
            timeout=60              # Request timeout
        )

        # Execution settings
        self.max_steps = 20         # Maximum reasoning steps
        self.verbose = True         # Show reasoning steps

        # Agent behavior settings
        self.system_prompt = "You are a configurable AI assistant."
```

## Agent Lifecycle

### 1. Initialization

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

# Create agent instance
agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-4.1"),
    system_prompt="You are a helpful assistant.",
    max_steps=15
)
```

### 2. Execution

```python
# Single interaction
result = await agent.run("What's the weather in New York?")
print(result)

# Multiple interactions
questions = [
    "What is SpoonOS?",
    "How do I create an agent?",
    "What tools are available?"
]

for question in questions:
    response = await agent.run(question)
    print(f"Q: {question}")
    print(f"A: {response}
")
```

### 3. Configuration Management

```python
# Access agent configuration
print(f"Max steps: {agent.max_steps}")
print(f"System prompt: {agent.system_prompt}")

# Modify configuration
agent.max_steps = 20
agent.system_prompt = "You are an expert assistant."

# Check available tools
if hasattr(agent, 'available_tools'):
    tools = agent.available_tools.list_tools()
    print(f"Available tools: {tools}")
```

## Best Practices

### 1. Clear System Prompts

```python
# Good: Specific and actionable
system_prompt = """
You are a code review assistant. For each code submission:
1. Check for syntax errors and bugs
2. Suggest improvements for readability
3. Identify security vulnerabilities
4. Recommend best practices
5. Provide specific, actionable feedback
"""

# Avoid: Vague or generic
system_prompt = "You are a helpful assistant."
```

### 2. Appropriate Tool Selection

```python
# Match tools to agent purpose
class DataAnalysisAgent(ToolCallAgent):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Data-focused tools
        tools = [
            PandasTool(),      # Data manipulation
            PlotlyTool(),      # Visualization
            StatsTool(),       # Statistical analysis
            DatabaseTool(),    # Data access
        ]

        self.available_tools = ToolManager(tools)
```

### 3. Error Handling

```python
class RobustAgent(ToolCallAgent):
    async def run(self, user_input: str) -> str:
        try:
            return await super().run(user_input)
        except Exception as e:
            # Log error and provide graceful fallback
            self.logger.error(f"Agent execution failed: {e}")
            return "I encountered an error. Please try rephrasing your request."
```

### 4. Performance Optimization

```python
# Use appropriate model for task complexity
simple_agent = ToolCallAgent(
    llm=ChatBot(model_name="gpt-3.5-turbo")  # Faster, cheaper
)

complex_agent = ToolCallAgent(
    llm=ChatBot(model_name="gpt-4.1")        # More capable
)

# Set reasonable limits
agent.max_steps = 10        # Prevent infinite loops
agent.timeout = 60.0        # Prevent hanging
```

## Testing Agents

### Unit Testing

```python
import pytest
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_weather_agent():
    # Mock the LLM
    mock_llm = AsyncMock()
    mock_llm.chat.return_value = "The weather in Paris is sunny."

    # Create agent with mock
    agent = WeatherAgent(llm=mock_llm)

    # Test execution
    result = await agent.run("What's the weather in Paris?")
    assert "sunny" in result.lower()
```

### Integration Testing

```python
@pytest.mark.asyncio
async def test_agent_with_real_llm():
    # Test with actual LLM (requires API key)
    agent = WeatherAgent(llm=ChatBot())

    result = await agent.run("What's the weather in London?")
    assert len(result) > 0
    assert "weather" in result.lower()
```

## Monitoring and Debugging

### Logging

```python
import logging

# Enable agent logging
logging.basicConfig(level=logging.DEBUG)

# Agent will log reasoning steps
agent = MyAgent(llm=ChatBot(), verbose=True)
```

### Performance Metrics

```python
import time

start_time = time.time()
result = await agent.run("Complex query here")
execution_time = time.time() - start_time

print(f"Agent executed in {execution_time:.2f} seconds")
print(f"Used {agent.step_count} reasoning steps")
```

## Advanced Features

### MCP Tool Integration

#### Single MCP Tool Integration

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.tools import ToolManager
from spoon_ai.chat import ChatBot
import os

class MCPEnabledAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure LLM
        self.llm = ChatBot(model_name="gpt-4.1")

        # Configure stdio MCP tool
        search_tool = MCPTool(
            name="tavily_search",
            description="Advanced web search capabilities",
            mcp_config={
                "command": "npx",
                "args": ["-y", "tavily-mcp"],
                "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
                "timeout": 30,
                "retry_attempts": 3
            }
        )

        # Configure HTTP MCP tool
        context7_tool = MCPTool(
            name="context7_docs",
            description="Access Context7 documentation and libraries",
            mcp_config={
                "url": "https://mcp.context7.com/mcp",
                "timeout": 30,
                "retry_attempts": 2,
                "headers": {
                    "User-Agent": "SpoonOS-Agent/1.0"
                }
            }
        )

        # Create tool manager
        self.available_tools = ToolManager([search_tool, context7_tool])

        self.system_prompt = """
        You are a research assistant with access to multiple MCP tools:
        - tavily_search: Web search functionality
        - context7_docs: Access Context7 documentation and library information

        When using tools:
        1. Choose the appropriate tool for the task
        2. Clearly define search objectives and keywords
        3. Analyze the reliability of search results
        4. Provide comprehensive analysis reports
        """

# Usage example
async def use_single_mcp():
    agent = MCPEnabledAgent()
    result = await agent.run("Search for the latest developments in SpoonOS framework and review related documentation")
    return result
```

#### Comprehensive MCP Tool Integration

```python
from spoon_ai.agents import SpoonReactMCP
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.tools import ToolManager

class ComprehensiveMCPAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # Configure multiple MCP tools
        mcp_tools = []

        # Web search tool
        if os.getenv("TAVILY_API_KEY"):
            mcp_tools.append(MCPTool(
                name="web_search",
                description="Web search and information gathering",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "tavily-mcp"],
                    "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
                }
            ))

        # GitHub tools
        if os.getenv("GITHUB_TOKEN"):
            mcp_tools.append(MCPTool(
                name="github_management",
                description="GitHub repository and code management",
                mcp_config={
                    "command": "npx",
                    "args": ["-y", "@modelcontextprotocol/server-github"],
                    "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")},
                    "transport": "stdio"
                }
            ))

        # Filesystem tools
        mcp_tools.append(MCPTool(
            name="file_operations",
            description="File and directory operations",
            mcp_config={
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-filesystem"],
                "transport": "stdio"
            }
        ))

        # Database tools
        mcp_tools.append(MCPTool(
            name="database_operations",
            description="SQLite database operations",
            mcp_config={
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-sqlite"],
                "env": {"DATABASE_PATH": "./agent_data.db"},
                "transport": "stdio"
            }
        ))

        # HTTP MCP tool - Context7 documentation service
        mcp_tools.append(MCPTool(
            name="context7_docs",
            description="Context7 documentation and library information access",
            mcp_config={
                "url": "https://mcp.context7.com/mcp",
                "transport": "http",
                "timeout": 30,
                "retry_attempts": 2,
                "headers": {
                    "User-Agent": "SpoonOS-ComprehensiveAgent/1.0"
                }
            }
        ))

        # SSE MCP tool - Firecrawl web scraping service
        if os.getenv("FIRECRAWL_API_KEY"):
            mcp_tools.append(MCPTool(
                name="firecrawl_scraper",
                description="Advanced web scraping and content extraction service",
                mcp_config={
                    "url": f"https://mcp.firecrawl.dev/{os.getenv('FIRECRAWL_API_KEY')}/sse",
                    "transport": "sse",
                    "timeout": 60,
                    "retry_attempts": 3,
                    "reconnect_interval": 5,
                    "max_reconnect_attempts": 10,
                    "headers": {
                        "Accept": "text/event-stream",
                        "User-Agent": "SpoonOS-ComprehensiveAgent/1.0",
                        "Cache-Control": "no-cache"
                    }
                }
            ))

        # Create tool manager
        self.available_tools = ToolManager(mcp_tools)

        self.system_prompt = """
        You are a comprehensive AI assistant with the following MCP tools:

        1. web_search: Search for latest information and resources
        2. github_management: Manage GitHub repositories and code
        3. file_operations: Handle files and directories
        4. database_operations: Operate SQLite databases
        5. context7_docs: Access Context7 documentation and library information
        6. firecrawl_scraper: Advanced web scraping and content extraction service

        Workflow:
        1. Analyze user requirements and determine which tools to use
        2. Use multiple tools in logical sequence to complete tasks
        3. Integrate results from all tools to provide comprehensive solutions

        Always ensure operation safety and accuracy.
        """

# Usage example
async def use_comprehensive_mcp():
    agent = ComprehensiveMCPAgent(llm=ChatBot())

    task = """
    Please help me complete a project analysis task:
    1. Search for the latest trends in 'AI agent frameworks'
    2. Review Context7 related documentation and library information
    3. Create project directory structure locally
    4. Create database to store analysis results
    5. If possible, review related GitHub projects
    """

    result = await agent.run(task)
    return result
```

### Advanced Parameter Configuration

#### Detailed Agent Parameter Configuration

```python
class AdvancedConfigAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        # Advanced LLM configuration
        llm_config = {
            "model_name": "gpt-4.1",
            "temperature": 0.3,          # Creativity control
            "max_tokens": 4096,          # Maximum response length
            "top_p": 0.9,               # Nucleus sampling parameter
            "frequency_penalty": 0.1,    # Frequency penalty
            "presence_penalty": 0.1,     # Presence penalty
            "timeout": 60,              # Request timeout
            "retry_attempts": 3,         # Retry attempts
            "stream": False             # Whether to stream responses
        }

        super().__init__(
            llm=ChatBot(**llm_config),
            **kwargs
        )

        # Agent execution parameters
        self.max_iterations = 20        # Maximum reasoning steps
        self.max_execution_time = 300   # Maximum execution time (seconds)
        self.enable_memory = True       # Enable conversation memory
        self.memory_window = 10         # Memory window size
        self.enable_reflection = True   # Enable reflection mechanism
        self.reflection_threshold = 5   # Reflection trigger threshold

        # Tool execution parameters
        self.tool_timeout = 30          # Tool execution timeout
        self.tool_retry_attempts = 2    # Tool retry attempts
        self.parallel_tool_execution = False  # Parallel tool execution
        self.tool_error_handling = "graceful"  # Tool error handling strategy

        # Security and limitation parameters
        self.enable_safety_checks = True      # Enable security checks
        self.max_tool_calls_per_iteration = 3 # Maximum tool calls per iteration
        self.restricted_operations = [        # Restricted operations
            "file_delete",
            "system_command",
            "network_access"
        ]

        self.system_prompt = """
        You are an advanced AI agent with the following configuration:

        Execution parameters:
        - Maximum reasoning steps: {max_iterations}
        - Memory enabled: {enable_memory}
        - Reflection enabled: {enable_reflection}

        Security settings:
        - Security checks: Enabled
        - Operation restrictions: Configured
        - Error handling: Graceful mode

        Please complete tasks efficiently within these parameter constraints.
        """.format(
            max_iterations=self.max_iterations,
            enable_memory=self.enable_memory,
            enable_reflection=self.enable_reflection
        )

# Dynamic parameter adjustment
class DynamicConfigAgent(SpoonReactMCP):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.performance_metrics = {
            "success_rate": 0.0,
            "avg_execution_time": 0.0,
            "tool_usage_efficiency": 0.0
        }

    async def run(self, user_input: str) -> str:
        """Execution with dynamic parameter adjustment"""
        # Adjust parameters based on historical performance
        self._adjust_parameters()

        # Execute task
        start_time = time.time()
        try:
            result = await super().run(user_input)
            success = True
        except Exception as e:
            result = f"Execution failed: {str(e)}"
            success = False

        # Update performance metrics
        execution_time = time.time() - start_time
        self._update_metrics(success, execution_time)

        return result

    def _adjust_parameters(self):
        """Adjust parameters based on performance metrics"""
        if self.performance_metrics["success_rate"] < 0.8:
            # Low success rate, increase retry attempts and timeout
            self.max_iterations = min(self.max_iterations + 2, 25)
            self.tool_timeout = min(self.tool_timeout + 10, 60)

        if self.performance_metrics["avg_execution_time"] > 120:
            # Execution time too long, reduce maximum steps
            self.max_iterations = max(self.max_iterations - 1, 10)

        if self.performance_metrics["tool_usage_efficiency"] < 0.6:
            # Low tool usage efficiency, enable parallel execution
            self.parallel_tool_execution = True

    def _update_metrics(self, success: bool, execution_time: float):
        """Update performance metrics"""
        # Simplified metric update logic
        alpha = 0.1  # Learning rate

        self.performance_metrics["success_rate"] = (
            (1 - alpha) * self.performance_metrics["success_rate"] +
            alpha * (1.0 if success else 0.0)
        )

        self.performance_metrics["avg_execution_time"] = (
            (1 - alpha) * self.performance_metrics["avg_execution_time"] +
            alpha * execution_time
        )
```

### Advanced Configuration File Settings

```json
{
  "agents": {
    "advanced_mcp_agent": {
      "class": "SpoonReactMCP",
      "description": "Advanced MCP agent configuration example",
      "config": {
        "max_steps": 20,
        "temperature": 0.3,
        "max_execution_time": 300,
        "enable_memory": true,
        "memory_window": 10,
        "enable_reflection": true,
        "reflection_threshold": 5,
        "safety_checks": true,
        "parallel_tool_execution": false,
        "tool_timeout": 30,
        "tool_retry_attempts": 2,
        "max_tool_calls_per_iteration": 3,
        "system_prompt": "You are an advanced AI agent..."
      },
      "llm_config": {
        "model_name": "gpt-4.1",
        "temperature": 0.3,
        "max_tokens": 4096,
        "top_p": 0.9,
        "frequency_penalty": 0.1,
        "presence_penalty": 0.1,
        "timeout": 60,
        "retry_attempts": 3
      },
      "tools": [
        {
          "name": "tavily_search",
          "type": "mcp",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 30,
            "retry_attempts": 3,
            "cache_duration": 300,
            "rate_limit": 100
          },
          "mcp_server": {
            "command": "npx",
            "args": ["-y", "tavily-mcp"],
            "env": {"TAVILY_API_KEY": "your-tavily-key"},
            "transport": "stdio",
            "health_check_interval": 60,
            "auto_restart": true
          }
        },
        {
          "name": "github_tools",
          "type": "mcp",
          "enabled": true,
          "priority": "medium",
          "config": {
            "timeout": 45,
            "retry_attempts": 2,
            "default_branch": "main",
            "max_file_size": "10MB"
          },
          "mcp_server": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {"GITHUB_TOKEN": "your-github-token"},
            "transport": "stdio"
          }
        },
        {
          "name": "context7_docs",
          "type": "mcp",
          "enabled": true,
          "priority": "medium",
          "config": {
            "timeout": 30,
            "retry_attempts": 2,
            "rate_limit": 50,
            "cache_duration": 600
          },
          "mcp_server": {
            "url": "https://mcp.context7.com/mcp",
            "transport": "http",
            "headers": {
              "User-Agent": "SpoonOS-Agent/1.0"
            }
          }
        },
        {
          "name": "firecrawl_scraper",
          "type": "mcp",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 60,
            "retry_attempts": 3,
            "reconnect_interval": 5,
            "max_reconnect_attempts": 10,
            "rate_limit": 30,
            "cache_duration": 300
          },
          "mcp_server": {
            "url": "https://mcp.firecrawl.dev/{FIRECRAWL_API_KEY}/sse",
            "transport": "sse",
            "headers": {
              "Accept": "text/event-stream",
              "User-Agent": "SpoonOS-Agent/1.0",
              "Cache-Control": "no-cache"
            }
          }
        },
        {
          "name": "crypto_powerdata_cex",
          "type": "builtin",
          "enabled": true,
          "priority": "high",
          "config": {
            "timeout": 30,
            "max_retries": 3,
            "cache_duration": 300,
            "rate_limit": 200,
            "default_currency": "USD",
            "precision": 8
          },
          "env": {
            "OKX_API_KEY": "your_okx_api_key",
            "OKX_SECRET_KEY": "your_okx_secret_key",
            "OKX_API_PASSPHRASE": "your_okx_passphrase"
          }
        }
      ],
      "monitoring": {
        "enable_metrics": true,
        "log_level": "INFO",
        "performance_tracking": true,
        "error_reporting": true,
        "health_checks": true
      },
      "security": {
        "restricted_operations": [
          "file_delete",
          "system_command",
          "network_access"
        ],
        "api_rate_limits": {
          "requests_per_minute": 100,
          "tokens_per_hour": 50000
        },
        "data_privacy": {
          "log_user_inputs": false,
          "encrypt_sensitive_data": true,
          "data_retention_days": 30
        }
      }
    }
  }
}
```

## Next Steps

### 📚 **Agent Implementation Examples**

#### 🎯 [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Advanced agent patterns demonstrated:**
- Long-lived agent architecture with persistent memory
- Intelligent routing and decision-making workflows
- Advanced state management and context preservation
- Production-ready error handling and monitoring

#### 🔍 [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**MCP integration patterns:**
- Dynamic tool discovery and loading
- Multi-tool orchestration and coordination
- Real-time web search integration
- Advanced error handling for distributed systems

#### 📊 [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Domain-specific agent development:**
- Cryptocurrency market analysis workflows
- Real-time data processing and technical indicators
- LLM-driven investment recommendations
- Financial data validation and error handling

### 🛠️ **Development Resources**

- **[Tools System](./tools.md)** - Complete tool integration guide
- **[Custom Tools](../how-to-guides/add-custom-tools.md)** - Build specialized tools
- **[MCP Protocol](./mcp-protocol.md)** - Dynamic tool loading and execution
- **[Graph System](./graph-system.md)** - Advanced workflow orchestration

### 📖 **Additional Documentation**

- **[API Reference](../api-reference/spoon_ai/agents/base/)** - Complete agent API documentation
- **[Performance Optimization](../troubleshooting/performance.md)** - Agent performance tuning
- **[Troubleshooting](../troubleshooting/common-issues.md)** - Common issues and solutions

Ready to build more sophisticated agents? Check out the [Tools](./tools.md) documentation! 🛠️

---

FILE: docs/core-concepts/agents.md

# Agents

Agents are the core intelligence layer of SpoonOS—**autonomous AI systems that reason, plan, and act** to accomplish complex tasks. An agent combines an LLM's reasoning capabilities with tools, memory, and structured execution patterns to go beyond simple question-answering.

## Why Agents?

A raw LLM can answer questions, but it can't:

- **Take actions** — Call APIs, query databases, execute transactions
- **Remember context** — Maintain state across conversations and sessions
- **Reason iteratively** — Break down complex tasks into steps, observe results, and adapt
- **Handle failures** — Retry, fallback, or ask for help when something goes wrong

SpoonOS agents solve these problems with two execution models:

```mermaid
graph LR
    subgraph ReAct["ReAct Agent"]
        A[Think] --> B[Act]
        B --> C[Observe]
        C --> A
    end

    subgraph Graph["Graph Agent"]
        D[Node A] --> E{Route}
        E --> F[Node B]
        E --> G[Node C]
        F --> H[Node D]
        G --> H
    end
```

| Model | Best For | How It Works |
|-------|----------|--------------|
| **ReAct** | Simple tasks, single-step tool calls, Q&A | Think → Act → Observe loop until done |
| **Graph** | Complex workflows, parallel tasks, conditional logic | Stateful graph with nodes, edges, and routing |

## What Can You Build?

| Use Case | Agent Type | Example |
|----------|------------|---------|
| **Chatbot with tools** | ReAct | Answer questions by searching the web or querying APIs |
| **Trading bot** | Graph | Analyze market → decide action → execute trade → monitor |
| **Research assistant** | Graph | Gather sources → summarize → synthesize → format report |
| **Customer support** | ReAct | Handle tickets by querying knowledge base and escalating |
| **Portfolio tracker** | Graph | Fetch prices → calculate metrics → generate alerts |

## SpoonOS vs Other Frameworks

| Feature | SpoonOS | LangChain | AutoGPT |
|---------|---------|-----------|---------|
| **Execution** | ReAct + Graph workflows | ReAct variants | Autonomous loop |
| **Tools** | `BaseTool` + MCP protocol | `Tool` class | Plugins |
| **Memory** | Built-in short-term + Mem0 | External modules | File-based |
| **Providers** | Unified multi-provider with fallback | Per-model adapters | Single provider |
| **Web3/Crypto** | Native CEX, DEX, on-chain toolkits | Third-party only | Limited |

---

## Quick Start

```bash
pip install spoon-ai
export OPENAI_API_KEY="your-key"
```

```python
import asyncio
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot

agent = SpoonReactAI(llm=ChatBot(model_name="gpt-4.1", llm_provider="openai"))

async def main():
    response = await agent.run("What is the capital of France?")
    print(response)

asyncio.run(main())
```

---

## Agent Types

### ReAct Agents

ReAct (Reasoning + Acting) agents follow a thought → action → observation loop. The agent thinks about what to do, executes a tool or generates a response, observes the result, and repeats until the task is complete.

```python
import os
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools import ToolManager
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.mcp_tool import MCPTool

class PercentageTool(BaseTool):
    name: str = "calculate_percentage"
    description: str = "Calculate a percentage of a numeric value"
    parameters: dict = {
        "type": "object",
        "properties": {
            "value": {"type": "number", "description": "Base value"},
            "percent": {"type": "number", "description": "Percentage to apply (e.g., 10 for 10%)"},
        },
        "required": ["value", "percent"],
    }

    async def execute(self, value: float, percent: float) -> str:
        return str(value * percent / 100)

# MCP web search tool (requires TAVILY_API_KEY)
tavily_search = MCPTool(
    name="tavily-search",
    description="Search the web for current information",
    mcp_config={
        "command": "npx",
        "args": ["-y", "tavily-mcp"],
        "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")},
    },
)

# Agent with real tools
agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-4.1", llm_provider="openai"),
    tools=ToolManager([tavily_search, PercentageTool()]),
    max_iterations=10  # Limit reasoning loops
)

response = await agent.run("Search Bitcoin price and calculate 10% of it")
```

**Best for:** Single-step tasks, API calls, Q&A, simple automation.

### Graph Agents

Graph agents execute structured workflows defined as state graphs, supporting conditional branching, parallel execution, and complex multi-step pipelines.

```python
from spoon_ai.agents import GraphAgent
from spoon_ai.graph import StateGraph

# Build workflow (see Graph System docs for StateGraph details)
graph = StateGraph(MyState)
graph.add_node("analyze", analyze_fn)
graph.add_node("execute", execute_fn)
graph.add_edge("__start__", "analyze")
graph.add_conditional_edge("analyze", router_fn)

# Agent with memory persistence
agent = GraphAgent(
    graph=graph.compile(),
    memory_path="./agent_memory",
    session_id="user_123"
)

result = await agent.run("Analyze market and execute trades")
```

**Best for:** Multi-step workflows, conditional logic, parallel tasks, human-in-the-loop.

---

## Agent Architecture

### Core Components

1. **LLM Provider** - The language model powering the agent
2. **Tool Manager** - Manages available tools and execution
3. **Memory System** - Stores conversation history and context
4. **Prompt System** - Handles system prompts and instructions

### Agent Lifecycle

```mermaid
graph TD
    A[Initialize Agent] --> B[Load Tools]
    B --> C[Receive Input]
    C --> D[Reason About Task]
    D --> E[Select Action]
    E --> F[Execute Tool]
    F --> G[Observe Result]
    G --> H{Task Complete?}
    H -->|No| D
    H -->|Yes| I[Return Response]
```

## Creating Custom Agents

### Basic Agent Setup

```python
from spoon_ai.agents.base import BaseAgent
from spoon_ai.tools import ToolManager

class CustomAgent(BaseAgent):
    def __init__(self, llm, tools=None):
        super().__init__(llm)
        self.tool_manager = ToolManager(tools or [])

    async def run(self, message: str) -> str:
        # Custom agent logic here
        return await self.process_message(message)
```

### Agent Configuration

```python
import os
from spoon_ai.agents.spoon_react_mcp import SpoonReactMCP
from spoon_ai.tools import ToolManager
from spoon_ai.tools.mcp_tool import MCPTool
from spoon_ai.chat import ChatBot
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool

# Install extra dependency: pip install spoon-toolkits

class SpoonMacroAnalysisAgent(SpoonReactMCP):
    name: str = "SpoonMacroAnalysisAgent"
    system_prompt: str = (
        "You are a crypto market analyst. Use tavily-search for news and "
        "crypto_power_data_cex for market data, then deliver a concise macro view."
    )

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.available_tools = ToolManager([])

    async def initialize(self):
        tavily_key = os.getenv("TAVILY_API_KEY")
        if not tavily_key:
            raise ValueError("Set TAVILY_API_KEY before running the agent.")

        tavily_tool = MCPTool(
            name="tavily-search",
            description="Web search via Tavily",
            mcp_config={
                "command": "npx",
                "args": ["--yes", "tavily-mcp"],
                "env": {"TAVILY_API_KEY": tavily_key},
            },
        )

        crypto_tool = CryptoPowerDataCEXTool()
        self.available_tools = ToolManager([tavily_tool, crypto_tool])

agent = SpoonMacroAnalysisAgent(llm=ChatBot(llm_provider="openai"))
```

## Best Practices

### Tool Selection
- Choose tools that match your use case
- Avoid tool overload - too many tools can confuse the agent
- Test tool combinations thoroughly

### Prompt Engineering
- Provide clear, specific instructions
- Include examples of desired behavior
- Set appropriate constraints and guidelines

### Error Handling

- Leverage framework's automatic retry mechanisms
- Use built-in fallback strategies
- Rely on framework's structured error handling

### Framework Error Handling

SpoonOS agents benefit from built-in error resilience:

```python
# Framework handles errors automatically
# Requires: pip install spoon-toolkits
from spoon_ai.tools import ToolManager
from spoon_toolkits.crypto.crypto_powerdata.tools import CryptoPowerDataCEXTool

agent = SpoonReactAI(
    llm=ChatBot(model_name="gpt-4.1", llm_provider="openai"),
    tools=ToolManager([CryptoPowerDataCEXTool()])  # real tool with retries/failures
)

# Automatic handling includes:
# - LLM provider failures with fallback
# - Tool execution errors with retry
# - Network issues with graceful degradation
response = await agent.run("Get Bitcoin price and analyze trends")
```

## Performance Considerations

### Memory Usage
- ReAct agents: Lower memory footprint
- Graph agents: Higher memory for complex workflows

### Execution Speed
- Simple tasks: ReAct agents are faster
- Complex workflows: Graph agents are more efficient

### Scalability
- ReAct: Better for high-frequency, simple tasks
- Graph: Better for complex, long-running processes

## Next Steps

### 📚 **Agent Implementation Examples**

#### 🎯 [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**What it demonstrates:**
- Complete Graph agent implementation with intelligent routing
- Long-lived agent architecture with persistent memory
- Advanced state management and context preservation
- Production-ready error handling and recovery

**Key features:**
- Dynamic query routing based on user intent (general_qa → short_term_trend → macro_trend → deep_research)
- True parallel execution across multiple data sources
- Memory persistence and conversation context
- Real-time performance monitoring and metrics

**Best for learning:**
- Graph agent architecture patterns
- Long-running process management
- Advanced memory and state handling
- Production deployment considerations

#### 🔍 [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- MCP-enabled agent with dynamic tool discovery
- Web search integration with cryptocurrency analysis
- Multi-tool orchestration and data synthesis
- Real-world agent deployment patterns

**Key features:**
- Tavily MCP server integration for web search
- Crypto PowerData tools for market analysis
- Unified analysis combining multiple data sources
- Dynamic tool loading and validation

**Best for learning:**
- MCP protocol implementation
- Multi-tool agent architecture
- Real-time data integration patterns
- Error handling in distributed systems

#### 📊 [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**What it demonstrates:**
- Specialized cryptocurrency analysis agent
- LLM-driven decision making throughout the workflow
- Real-time market data processing and analysis
- Investment recommendation generation

**Key features:**
- Real Binance API integration (no simulated data)
- Technical indicator calculation (RSI, MACD, EMA, Bollinger Bands)
- Multi-timeframe analysis and correlation
- Risk assessment and market sentiment analysis

**Best for learning:**
- Domain-specific agent development
- Financial data processing patterns
- LLM-driven workflow automation
- Real API integration in agents

### 🛠️ **Development Guides**

- **[Tools System](./tools.md)** - Complete guide to available tools and integrations
- **[LLM Providers](./llm-providers.md)** - Configure and optimize language models
- **[Build Your First Agent](../how-to-guides/build-first-agent.md)** - Step-by-step agent development tutorial

### 📖 **Advanced Topics**

- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Dynamic tool discovery and execution
- **[API Reference](../api-reference/spoon_ai/agents/base/)** - Complete agent API documentation

---

FILE: docs/core-concepts/graph-system.md

# Graph System

The SpoonOS Graph System is a library for building **stateful, multi-step AI agent workflows**. It models applications as directed graphs where **nodes** represent actions (calling an LLM, executing a tool, processing data) and **edges** define how control flows between them—including conditional branching, parallel fan-out, and cycles for iterative reasoning.

## Why Graph System?

Traditional LLM applications are often simple chains: prompt → response → done. But real-world AI agents need more:

- **State persistence** — Remember context across multiple steps and interactions
- **Conditional logic** — Take different paths based on LLM outputs or external data
- **Parallel execution** — Run multiple tasks simultaneously and combine results
- **Human-in-the-loop** — Pause for user input, approval, or correction
- **Error recovery** — Handle failures gracefully without losing progress

The Graph System makes these patterns first-class citizens, not afterthoughts.

## Key Concepts

```mermaid
graph LR
    A[User Input] --> B[Node: Analyze]
    B -->|route| C{Router}
    C -->|intent=search| D[Node: Search]
    C -->|intent=calculate| E[Node: Calculate]
    C -->|intent=chat| F[Node: Respond]
    D --> G[Node: Summarize]
    E --> G
    F --> H[END]
    G --> H
```

| Concept | Description |
|---------|-------------|
| **State** | A typed dictionary (`TypedDict`) shared across all nodes. Each node reads state, performs work, and returns updates to merge back. |
| **Node** | An async function that receives state and returns a partial update. Nodes are the "actions" in your workflow. |
| **Edge** | A connection between nodes. Can be static (always go A→B), conditional (go A→B or A→C based on state), or LLM-driven. |
| **Checkpoint** | An automatic snapshot of state before each node. Enables recovery, debugging, and human-in-the-loop interrupts. |

## What Can You Build?

| Use Case | How Graph System Helps |
|----------|----------------------|
| **Autonomous Agents** | Multi-step reasoning with tool calls, observation loops, and adaptive planning |
| **RAG Pipelines** | Retrieve → Grade → Regenerate cycles with conditional routing based on relevance |
| **Multi-Agent Systems** | Multiple specialized agents collaborating via shared state and handoffs |
| **Approval Workflows** | Pause execution for human review, then resume from checkpoint |
| **Parallel Analysis** | Fan-out to multiple data sources, join results with configurable strategies |

## Graph System vs LangGraph

SpoonOS Graph System is inspired by [LangGraph](https://github.com/langchain-ai/langgraph) and shares similar concepts. Key differences:

| Feature | SpoonOS Graph | LangGraph |
|---------|---------------|-----------|
| **Parallel Groups** | Native `add_parallel_group()` with quorum joins, timeouts, circuit breakers | Manual asyncio or branching |
| **Routing Stack** | Priority-based: explicit → rules → intelligent → LLM → fallback | Conditional edges only |
| **Declarative Definition** | `GraphTemplate` / `NodeSpec` / `EdgeSpec` for serializable, composable graphs | Imperative builder only |
| **Resource Control** | Built-in rate limiting, max concurrency, circuit breakers | External implementation |
| **Web3/Crypto** | Native integration with SpoonOS toolkits (CEX, DEX, on-chain) | Via third-party tools |

Choose SpoonOS Graph when you need production-grade parallel execution, multi-layer routing, or crypto/Web3 integrations.

---

## Quick Start

```bash
pip install spoon-ai
```

```python
import asyncio
from typing import TypedDict
from spoon_ai.graph import StateGraph, END

class MyState(TypedDict):
    query: str
    result: str

async def process(state: MyState) -> dict:
    return {"result": f"Processed: {state['query']}"}

graph = StateGraph(MyState)
graph.add_node("process", process)
graph.add_edge("__start__", "process")
graph.add_edge("process", END)

app = graph.compile()

async def main():
    result = await app.invoke({"query": "Hello", "result": ""})
    print(result["result"])  # Processed: Hello

asyncio.run(main())
```

---

## Architecture

```mermaid
flowchart TB
    subgraph Execution["CompiledGraph Execution"]
        direction TB
        S((START)) --> N1[Node: analyze_intent]
        N1 -->|"intent=price"| N2[Node: fetch_price]
        N1 -->|"intent=trade"| N3[Node: execute_trade]
        N1 -->|"intent=general"| N4[Node: general_response]
        N2 --> N5[Node: summarize]
        N3 --> N5
        N4 --> N5
        N5 --> E((END))
    end

    subgraph State["Shared State (TypedDict)"]
        ST["user_query: str<br/>intent: str<br/>result: str<br/>..."]
    end

    Execution -.->|"reads/writes"| State
```

The graph system consists of three main components:

| Component | Responsibility |
|-----------|---------------|
| **`StateGraph`** | Builder class for defining workflow topology—nodes, edges, routing rules, parallel groups |
| **`CompiledGraph`** | Executable runtime that manages state transitions, checkpointing, and metrics collection |
| **`GraphAgent`** | Optional wrapper that integrates graph execution with SpoonOS agent lifecycle and memory |

```mermaid
flowchart LR
    subgraph Build["Build Phase"]
        SG[StateGraph]
        SG -->|"add_node()"| N[Nodes]
        SG -->|"add_edge()"| E[Edges]
        SG -->|"add_parallel_group()"| P[Parallel Groups]
    end

    subgraph Compile["Compile Phase"]
        SG -->|"compile()"| CG[CompiledGraph]
    end

    subgraph Execute["Execute Phase"]
        CG -->|"invoke(state)"| R[Final State]
        CG -->|"stream(state)"| S[State Updates]
    end
```

---

## Core Concepts

### Nodes

A **node** is an async function that receives the current workflow state and returns a dictionary of updates to merge back into state.

```python
from typing import TypedDict, Dict, Any

class AnalysisState(TypedDict):
    user_query: str
    symbol: str
    price_data: Dict[str, Any]
    analysis: str

async def fetch_price_node(state: AnalysisState) -> dict:
    """
    Node that fetches price data for a trading symbol.

    Receives: Full state dictionary
    Returns: Dictionary of fields to update (merged into state)
    """
    symbol = state.get("symbol", "BTC")

    # Actual API call to data source
    from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerData
    client = CryptoPowerData()
    ohlcv = await client.get_ohlcv(symbol=f"{symbol}USDT", interval="1h", limit=24)

    return {
        "price_data": {
            "symbol": symbol,
            "current_price": ohlcv[-1]["close"],
            "high_24h": max(c["high"] for c in ohlcv),
            "low_24h": min(c["low"] for c in ohlcv),
            "volume_24h": sum(c["volume"] for c in ohlcv),
        }
    }
```

**Node contract:**

- **Input**: Receives immutable view of current state
- **Output**: Returns `dict` of fields to update (partial update, not full replacement)
- **Async**: Should be `async def` (sync functions are auto-wrapped but less efficient)
- **Idempotent**: Should produce same result given same state (for retry safety)

### Edges

**Edges** define transitions between nodes. Three types are supported:

```python
from spoon_ai.graph import StateGraph, END

graph = StateGraph(AnalysisState)

# 1. Static edge: Always transitions to target
graph.add_edge("fetch_price", "analyze")
graph.add_edge("analyze", END)

# 2. Conditional edge: Routes based on state inspection
def route_by_intent(state: AnalysisState) -> str:
    """Return key from path_map based on detected intent."""
    intent = state.get("intent", "unknown")
    if intent == "price_query":
        return "price"
    elif intent == "technical_analysis":
        return "technical"
    return "general"

graph.add_conditional_edges(
    source="classify_intent",
    condition=route_by_intent,
    path_map={
        "price": "fetch_price",
        "technical": "compute_indicators",
        "general": "general_response",
    }
)

# 3. Routing rule: Pattern-based with priorities
graph.add_routing_rule(
    source_node="entry",
    condition=lambda state, query: "bitcoin" in query.lower(),
    target_node="btc_specialist",
    priority=10  # Higher priority = checked first
)
```

### State

**State** is a shared `TypedDict` that flows through the graph. Each node reads from state and returns updates to merge.

```python
from typing import TypedDict, List, Dict, Any, Optional, Annotated

class CryptoAnalysisState(TypedDict):
    # Input fields
    user_query: str
    user_id: str

    # Intermediate fields
    intent: str
    symbol: str
    timeframes: List[str]

    # Data fields
    price_data: Dict[str, Any]
    technical_indicators: Dict[str, float]
    news_sentiment: Dict[str, Any]

    # Output fields
    analysis: str
    recommendations: List[str]
    confidence: float

    # System fields
    messages: Annotated[List[dict], "Conversation history - appended via reducer"]
    execution_log: List[str]
```

**State merge behavior:**

| Field Type | Merge Strategy |
|------------|---------------|
| `dict` | Deep merge (nested dicts merged recursively) |
| `list` | Append (capped at 100 items to prevent unbounded growth) |
| `messages` field | Uses `add_messages` reducer (append with deduplication) |
| Other types | Replace |

### Checkpointing

The graph system automatically checkpoints state before each node execution, enabling:

- **Recovery**: Resume from last successful node after failure
- **Debugging**: Inspect state at any point in execution history
- **Human-in-the-loop**: Pause execution, collect input, resume

```python
from spoon_ai.graph import InMemoryCheckpointer, StateGraph

# Configure checkpointer
checkpointer = InMemoryCheckpointer(max_checkpoints_per_thread=100)
graph = StateGraph(AnalysisState, checkpointer=checkpointer)

# After compilation, access state history
compiled = graph.compile()
result = await compiled.invoke(
    {"user_query": "Analyze BTC"},
    config={"configurable": {"thread_id": "session_123"}}
)

# Retrieve checkpoint history
config = {"configurable": {"thread_id": "session_123"}}
history = list(graph.get_state_history(config))

for checkpoint in history:
    print(f"Node: {checkpoint.metadata.get('node')}")
    print(f"Iteration: {checkpoint.metadata.get('iteration')}")
    print(f"State keys: {list(checkpoint.values.keys())}")
```

---

## Building Graphs

### Imperative API

For simple workflows, use the imperative builder methods directly:

```python
from spoon_ai.graph import StateGraph, END

graph = StateGraph(AnalysisState)

# Add nodes
graph.add_node("classify", classify_intent_node)
graph.add_node("fetch_price", fetch_price_node)
graph.add_node("analyze", analyze_node)
graph.add_node("respond", generate_response_node)

# Add edges
graph.add_edge("classify", "fetch_price")
graph.add_edge("fetch_price", "analyze")
graph.add_edge("analyze", "respond")
graph.add_edge("respond", END)

# Set entry point
graph.set_entry_point("classify")

# Compile and execute
compiled = graph.compile()
result = await compiled.invoke({"user_query": "What is BTC price?"})
```

### Declarative API

For larger workflows, use `GraphTemplate` for better maintainability:

```python
from spoon_ai.graph import StateGraph, END
from spoon_ai.graph.builder import (
    DeclarativeGraphBuilder,
    GraphTemplate,
    NodeSpec,
    EdgeSpec,
    ParallelGroupSpec,
)
from spoon_ai.graph.config import GraphConfig, ParallelGroupConfig

# Define node specifications
nodes = [
    NodeSpec("classify", classify_intent_node),
    NodeSpec("fetch_price", fetch_price_node, parallel_group="data_fetch"),
    NodeSpec("fetch_news", fetch_news_node, parallel_group="data_fetch"),
    NodeSpec("fetch_sentiment", fetch_sentiment_node, parallel_group="data_fetch"),
    NodeSpec("analyze", analyze_node),
    NodeSpec("respond", generate_response_node),
]

# Define edge specifications
edges = [
    EdgeSpec("classify", "fetch_price"),  # Entry to parallel group
    EdgeSpec("fetch_price", "analyze"),   # All parallel nodes -> analyze
    EdgeSpec("fetch_news", "analyze"),
    EdgeSpec("fetch_sentiment", "analyze"),
    EdgeSpec("analyze", "respond"),
    EdgeSpec("respond", END),
]

# Define parallel groups
parallel_groups = [
    ParallelGroupSpec(
        name="data_fetch",
        nodes=["fetch_price", "fetch_news", "fetch_sentiment"],
        config=ParallelGroupConfig(
            join_strategy="all",
            timeout=30.0,
            error_strategy="collect_errors",
        )
    )
]

# Create template
template = GraphTemplate(
    entry_point="classify",
    nodes=nodes,
    edges=edges,
    parallel_groups=parallel_groups,
    config=GraphConfig(max_iterations=50),
)

# Build graph
builder = DeclarativeGraphBuilder(AnalysisState)
graph = builder.build(template)
```

---

## Routing Strategies

The graph system evaluates routing in priority order:

```mermaid
flowchart TD
    A[Current Node Complete] --> B{Explicit Edge?}
    B -->|Yes| C[Follow Edge]
    B -->|No| D{Routing Rule Match?}
    D -->|Yes| E[Apply Rule]
    D -->|No| F{Intelligent Router?}
    F -->|Yes| G[Call Router Function]
    F -->|No| H{LLM Router Enabled?}
    H -->|Yes| I[LLM Decides Next Node]
    H -->|No| J{Default Target Set?}
    J -->|Yes| K[Use Default]
    J -->|No| L[END]
```

### Conditional Edges

Route based on state inspection:

```python
def route_by_confidence(state: AnalysisState) -> str:
    """Route based on analysis confidence level."""
    confidence = state.get("confidence", 0.0)
    if confidence >= 0.8:
        return "high_confidence"
    elif confidence >= 0.5:
        return "medium_confidence"
    return "low_confidence"

graph.add_conditional_edges(
    "analyze",
    route_by_confidence,
    {
        "high_confidence": "generate_recommendation",
        "medium_confidence": "request_clarification",
        "low_confidence": "escalate_to_human",
    }
)
```

### LLM-Powered Routing

Enable natural language routing when patterns are complex:

```python
from spoon_ai.graph.config import GraphConfig, RouterConfig

config = GraphConfig(
    router=RouterConfig(
        allow_llm=True,
        llm_timeout=8.0,
        default_target="fallback_handler",
        allowed_targets=["price_handler", "trade_handler", "analysis_handler"],
    )
)

graph = StateGraph(AnalysisState)
graph.config = config

# Or enable after graph creation
graph.enable_llm_routing(config={
    "model": "gpt-4",
    "temperature": 0.1,
    "max_tokens": 50,
})
```

---

## Parallel Execution

Execute multiple nodes concurrently with configurable join and error strategies:

```python
from spoon_ai.graph.config import ParallelGroupConfig, ParallelRetryPolicy

# Define parallel data collection
graph.add_parallel_group(
    "market_data_collection",
    nodes=["fetch_binance", "fetch_coinbase", "fetch_kraken"],
    config=ParallelGroupConfig(
        # Join strategy
        join_strategy="quorum",  # "all", "any", "quorum"
        quorum=0.66,             # 66% must complete (2 of 3)

        # Timing
        timeout=15.0,            # Max wait time in seconds

        # Error handling
        error_strategy="collect_errors",  # "fail_fast", "collect_errors", "ignore_errors"

        # Retry policy for individual nodes
        retry_policy=ParallelRetryPolicy(
            max_retries=2,
            backoff_initial=0.5,
            backoff_multiplier=2.0,
            backoff_max=5.0,
        ),

        # Resource controls
        max_in_flight=10,                # Max concurrent tasks
        circuit_breaker_threshold=5,     # Disable group after N failures
        circuit_breaker_cooldown=30.0,   # Re-enable after cooldown
    )
)
```

**Join Strategies:**

| Strategy | Behavior | Use Case |
|----------|----------|----------|
| `"all"` | Wait for all nodes | Need complete data from all sources |
| `"any"` | Return on first success | Redundant sources, want fastest |
| `"quorum"` | Wait for majority | Fault-tolerant consensus |

**Error Strategies:**

| Strategy | Behavior | Use Case |
|----------|----------|----------|
| `"fail_fast"` | Cancel all, raise exception | Critical path, must succeed |
| `"collect_errors"` | Continue, store errors in `__errors__` | Best-effort, report issues |
| `"ignore_errors"` | Continue, discard failures | Non-critical enrichment |

---

## Human-in-the-Loop

Interrupt execution to collect user input:

```python
from spoon_ai.graph import interrupt, Command

async def confirm_trade_node(state: AnalysisState) -> dict:
    """Node that requires user confirmation before proceeding."""
    trade_details = state.get("trade_details", {})

    if not state.get("user_confirmed"):
        # Interrupt execution
        interrupt({
            "type": "confirmation_required",
            "question": f"Execute {trade_details['action']} {trade_details['amount']} {trade_details['symbol']}?",
            "trade_details": trade_details,
        })

    # This code runs after resume with confirmation
    return {"trade_executed": True, "execution_time": "2024-01-15T10:30:00Z"}

# Handling the interrupt
compiled = graph.compile()

# Initial execution - will interrupt
result = await compiled.invoke(
    {"user_query": "Buy 0.1 BTC"},
    config={"configurable": {"thread_id": "trade_session"}}
)

if "__interrupt__" in result:
    interrupt_info = result["__interrupt__"][0]
    print(f"Question: {interrupt_info['value']['question']}")

    # Get user confirmation (from UI, API, etc.)
    user_confirmed = await get_user_confirmation()

    # Resume execution with confirmation
    result = await compiled.invoke(
        Command(resume={"user_confirmed": user_confirmed}),
        config={"configurable": {"thread_id": "trade_session"}}
    )
```

---

## GraphAgent Integration

`GraphAgent` wraps graph execution with SpoonOS agent lifecycle and persistent memory:

```python
from spoon_ai.graph import StateGraph, GraphAgent, Memory

# Build graph
graph = build_analysis_graph()

# Create agent with memory
agent = GraphAgent(
    name="crypto_analyzer",
    graph=graph,
    memory_path="./agent_memory",
    session_id="user_123_session",
    preserve_state=True,  # Preserve state between runs
)

# Execute
result = await agent.run("Analyze BTC price trends")
print(result)

# Access execution metadata
metadata = agent.get_execution_metadata()
print(f"Successful: {metadata.get('execution_successful')}")

# Memory operations
agent.set_memory_metadata("last_analysis_time", "2024-01-15T10:30:00Z")
stats = agent.get_memory_statistics()
print(f"Total messages: {stats['total_messages']}")

# Search memory
matches = agent.search_memory("bitcoin", limit=5)

# Switch sessions
agent.load_session("user_456_session")
```

---

## Monitoring and Debugging

### Execution Metrics

```python
# Enable monitoring
graph.enable_monitoring([
    "execution_time",
    "success_rate",
    "routing_performance",
])

compiled = graph.compile()
result = await compiled.invoke(initial_state)

# Retrieve metrics
metrics = compiled.get_execution_metrics()
print(f"""
Execution Summary:
  Total executions: {metrics['total_executions']}
  Success rate: {metrics['success_rate']:.1%}
  Avg execution time: {metrics['avg_execution_time']:.3f}s

Per-Node Statistics:
""")
for node, stats in metrics['node_stats'].items():
    print(f"  {node}:")
    print(f"    Calls: {stats['count']}")
    print(f"    Avg time: {stats['avg_time']:.3f}s")
    print(f"    Error rate: {stats['error_rate']:.1%}")
```

### Execution History

```python
# Get detailed execution history
history = compiled.get_execution_history()

for step in history:
    print(f"""
Step: {step['node']}
  Iteration: {step['iteration']}
  Success: {step['success']}
  Execution time: {step['execution_time']:.3f}s
  Timestamp: {step['timestamp']}
""")
```

---

## Configuration Reference

### GraphConfig

```python
from spoon_ai.graph.config import GraphConfig, RouterConfig

config = GraphConfig(
    # Execution limits
    max_iterations=100,              # Maximum node transitions per invoke()

    # Router configuration
    router=RouterConfig(
        allow_llm=False,             # Enable LLM-based routing
        allowed_targets=None,        # Restrict valid routing targets (None = all)
        default_target=None,         # Fallback target when no route matches
        llm_timeout=8.0,             # Timeout for LLM router calls
        enable_fallback_to_default=True,  # Use default_target on routing failure
    ),

    # Validation
    state_validators=[],             # List of (state) -> None functions

    # Pre-configured parallel groups
    parallel_groups={},              # name -> ParallelGroupConfig
)
```

### ParallelGroupConfig

```python
from spoon_ai.graph.config import ParallelGroupConfig, ParallelRetryPolicy

config = ParallelGroupConfig(
    # Join behavior
    join_strategy="all",             # "all", "any", "quorum"
    quorum=None,                     # For quorum: 0.0-1.0 (ratio) or int (count)
    join_condition=None,             # Optional: async (state, completed_nodes) -> bool

    # Timing
    timeout=None,                    # Max wait time in seconds (None = unlimited)

    # Error handling
    error_strategy="fail_fast",      # "fail_fast", "collect_errors", "ignore_errors"
    retry_policy=ParallelRetryPolicy(
        max_retries=0,               # Retries per node
        backoff_initial=0.5,         # Initial backoff delay
        backoff_multiplier=2.0,      # Backoff multiplier
        backoff_max=10.0,            # Maximum backoff delay
    ),

    # Resource controls
    max_in_flight=None,              # Max concurrent tasks (None = unlimited)
    rate_limit_per_second=None,      # Rate limit (None = unlimited)

    # Circuit breaker
    circuit_breaker_threshold=None,  # Disable after N failures
    circuit_breaker_cooldown=30.0,   # Re-enable after cooldown seconds
)
```

---

## Best Practices

### Node Design

1. **Single responsibility**: Each node should do one thing. Split complex logic across multiple nodes.

2. **Idempotency**: Nodes should produce the same result given the same state (enables safe retries).

3. **Minimal state updates**: Return only changed fields. The system handles merging.

```python
# Good: Returns only updated fields
async def good_node(state):
    return {"result": "computed value"}

# Avoid: Returns entire state copy
async def avoid_node(state):
    new_state = state.copy()
    new_state["result"] = "computed value"
    return new_state
```

### State Management

1. **Type your state**: Use `TypedDict` for IDE support and documentation.

2. **Bound list growth**: Use reducers or explicit trimming to prevent unbounded lists.

3. **Avoid large objects**: State is checkpointed frequently. Keep it JSON-serializable and reasonably sized.

### Error Handling

1. **Use appropriate error strategies**: `fail_fast` for critical paths, `collect_errors` for best-effort.

2. **Add validation**: Use `state_validators` to catch invalid state early.

3. **Log in nodes**: Include context in error messages for debugging.

```python
async def robust_node(state):
    try:
        result = await external_api_call(state["symbol"])
        return {"data": result}
    except ExternalAPIError as e:
        logger.error(f"API call failed for {state['symbol']}: {e}")
        return {"error": str(e), "data": None}
```

---

## Examples

- **[Intent Graph Demo](../examples/intent-graph-demo.md)** — Intelligent routing with parallel execution
  ([source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py))

- **[Graph Crypto Analysis](../examples/graph-crypto-analysis.md)** — Real-time market analysis pipeline
  ([source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py))

## Related Documentation

- **[Agents](./agents.md)** — When to use `GraphAgent` vs `ReActAgent`
- **[Tools Integration](./tools.md)** — Adding tools to graph nodes
- **[MCP Protocol](./mcp-protocol.md)** — Dynamic tool discovery in workflows

---

FILE: docs/core-concepts/llm-providers.md

# LLM Providers

SpoonOS provides a **unified interface** to multiple LLM providers. Write your code once, then switch between OpenAI, Anthropic, Google, DeepSeek, or OpenRouter by changing a single parameter—no code rewrites, no API differences to handle.

## Why Multi-Provider?

Relying on a single LLM provider is risky:

- **Outages** — OpenAI goes down, your app goes down
- **Rate limits** — Hit the ceiling, requests fail
- **Cost** — Different models have different pricing
- **Capabilities** — Some models excel at code, others at analysis

SpoonOS solves this with:

```mermaid
graph LR
    A[Your Agent] --> B[ChatBot]
    B --> C{Provider Router}
    C -->|primary| D[OpenAI GPT-4]
    C -->|fallback 1| E[Anthropic Claude]
    C -->|fallback 2| F[Google Gemini]
    D -->|rate limited| C
    E -->|success| A
```

## Provider Comparison

| Provider | Best For | Context | Strengths |
|----------|----------|---------|-----------|
| **OpenAI** | General purpose, code | 128K | Fastest iteration, best tool calling |
| **Anthropic** | Long documents, analysis | 200K | Prompt caching, safety features |
| **Google** | Multimodal, cost-sensitive | 1M | Longest context, fast inference |
| **DeepSeek** | Complex reasoning, code | 64K | Best cost/performance for code |
| **OpenRouter** | Experimentation | Varies | 100+ models, automatic routing |

## Key Features

| Feature | What It Does |
|---------|--------------|
| **Unified API** | Same `ChatBot` class for all providers |
| **Auto-fallback** | Chain providers: GPT-4 → Claude → Gemini |
| **Streaming** | Real-time responses across all providers |
| **Tool calling** | Consistent function calling interface |
| **Token tracking** | Automatic counting and cost monitoring |

---

## Quick Start

```bash
pip install spoon-ai
export OPENAI_API_KEY="your-key"
```

```python
import asyncio
from spoon_ai.chat import ChatBot

# Same interface for all providers—just change model_name and llm_provider
llm = ChatBot(model_name="gpt-4.1", llm_provider="openai")

async def main():
    response = await llm.chat("Explain quantum computing in one sentence")
    print(response)

asyncio.run(main())
```

---

## Supported Providers

### OpenAI

- **Models**: GPT-4.1 (default), GPT-4o, GPT-4o-mini, o1-preview, o1-mini
- **Features**: Function calling, streaming, embeddings, reasoning models
- **Best for**: General-purpose tasks, reasoning, code generation

```python
from spoon_ai.chat import ChatBot

# OpenAI configuration with default model
llm = ChatBot(
    model_name="gpt-4.1",  # Framework default
    llm_provider="openai",
    temperature=0.7
)
```

### Anthropic (Claude)

- **Models**: Claude-Sonnet-4-20250514 (default), Claude-3.5 Sonnet, Claude-3.5 Haiku
- **Features**: Large context windows, prompt caching, safety features
- **Best for**: Long documents, analysis, safety-critical applications

```python
# Anthropic configuration with default model
llm = ChatBot(
    model_name="claude-sonnet-4-20250514",  # Framework default
    llm_provider="anthropic",
    temperature=0.1
)
```

### Google (Gemini)

- **Models**: Gemini-2.5-Pro (default), Gemini-2.0-Flash, Gemini-1.5-Pro
- **Features**: Multimodal capabilities, fast inference, large context
- **Best for**: Multimodal tasks, cost-effective solutions, long context

```python
# Google configuration with default model
llm = ChatBot(
    model_name="gemini-2.5-pro",  # Framework default
    llm_provider="gemini",
    temperature=0.1
)
```

### DeepSeek

- **Models**: DeepSeek-Reasoner (default), DeepSeek-V3, DeepSeek-Chat
- **Features**: Advanced reasoning, code-specialized models, cost-effective
- **Best for**: Complex reasoning, code generation, technical tasks

```python
# DeepSeek configuration with default model
llm = ChatBot(
    model_name="deepseek-reasoner",  # Framework default
    llm_provider="deepseek",
    temperature=0.2
)
```

### OpenRouter

- **Models**: Access to multiple providers through one API
- **Features**: Model routing, cost optimization
- **Best for**: Experimentation, cost optimization

```python
# OpenRouter configuration
llm = ChatBot(
    model_name="anthropic/claude-3-opus",
    llm_provider="openrouter",
    temperature=0.7
)
```

## Unified LLM Manager

The LLM Manager provides provider-agnostic access with automatic fallback:

```python
from spoon_ai.llm.manager import LLMManager

# Initialize with multiple providers
llm_manager = LLMManager(
    primary_provider="openai",
    fallback_providers=["anthropic", "gemini"],
    model_preferences={
        "openai": "gpt-4.1",
        "anthropic": "claude-sonnet-4-20250514",
        "gemini": "gemini-2.5-pro",
        "deepseek": "deepseek-reasoner"
    }
)

# Use with automatic fallback
response = await llm_manager.generate("Explain quantum computing")
```

## Configuration

### Environment Variables

```bash
# Provider API Keys
OPENAI_API_KEY=sk-your_openai_key_here
ANTHROPIC_API_KEY=sk-ant-your_anthropic_key_here
GEMINI_API_KEY=your_gemini_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
OPENROUTER_API_KEY=sk-or-your_openrouter_key_here

# Default Settings
DEFAULT_LLM_PROVIDER=openai
DEFAULT_MODEL=gpt-4.1
DEFAULT_TEMPERATURE=0.3
```

### Runtime Configuration

```json
{
  "llm": {
    "provider": "openai",
    "model": "gpt-4.1",
    "temperature": 0.3,
    "max_tokens": 32768,
    "fallback_providers": ["anthropic", "deepseek", "gemini"]
  }
}
```

## Advanced Features

### Prompt Caching (Anthropic)

```python
from spoon_ai.llm.cache import PromptCache

# Enable prompt caching for repeated system prompts
llm = ChatBot(
    model_name="claude-sonnet-4-20250514",
    llm_provider="anthropic",
    enable_caching=True
)
```

### Streaming Responses

```python
# Stream responses for real-time interaction
async for chunk in llm.stream("Write a long story about AI"):
    print(chunk, end="", flush=True)
```

### Function Calling

```python
# Define functions for the model to call
functions = [
    {
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string"}
            }
        }
    }
]

response = await llm.generate(
    "What's the weather in New York?",
    functions=functions
)
```

## Model Selection Guide

### Task-Based Recommendations

#### Code Generation

- Primary: DeepSeek-Reasoner, GPT-4.1
- Alternative: Claude-Sonnet-4

#### Analysis & Reasoning

- Primary: DeepSeek-Reasoner, GPT-4.1, Claude-Sonnet-4
- Alternative: Gemini-2.5-Pro

#### Cost-Sensitive Tasks

- Primary: DeepSeek-Reasoner, Gemini-2.5-Pro
- Alternative: GPT-4.1

#### Long Context Tasks

- Primary: Gemini-2.5-Pro (250K tokens), Claude-Sonnet-4 (200K tokens)
- Alternative: DeepSeek-Reasoner (65K tokens)

### Performance Comparison

| Provider                  | Speed     | Cost     | Context | Quality              |
| ------------------------- | --------- | -------- | ------- | -------------------- |
| OpenAI GPT-4.1            | Fast      | Medium   | 128K    | Excellent            |
| Anthropic Claude-Sonnet-4 | Medium    | Medium   | 200K    | Excellent            |
| Google Gemini-2.5-Pro     | Very Fast | Low      | 250K    | Very Good            |
| DeepSeek-Reasoner         | Fast      | Very Low | 65K     | Superior (Reasoning) |
| OpenAI o1-preview         | Slow      | High     | 128K    | Superior (Reasoning) |

## Error Handling & Fallbacks

### Automatic Fallback

The framework provides built-in error handling with automatic fallback between providers:

```python
from spoon_ai.llm.manager import LLMManager

# Configure fallback chain - errors are handled automatically
llm_manager = LLMManager(
    primary_provider="openai",
    fallback_providers=["anthropic", "google"],
    retry_attempts=3,
    timeout=30
)

# Automatic fallback on provider failures
response = await llm_manager.generate("Hello world")
```

### Error Types & Recovery

The framework uses structured error types for clean error handling:

```python
from spoon_ai.llm.errors import RateLimitError, AuthenticationError, ModelNotFoundError

# Simple error handling with specific error types
response = await llm.generate("Hello world")

# Framework handles common errors automatically:
# - Rate limits: automatic retry with backoff
# - Network issues: automatic retry with fallback
# - Authentication: clear error messages
# - Model availability: fallback to alternative models
```

### Graceful Degradation

```python
# Framework provides graceful degradation patterns
llm_manager = LLMManager(
    primary_provider="openai",
    fallback_providers=["deepseek", "gemini"],  # Cost-effective fallbacks
    enable_graceful_degradation=True
)

# If primary fails, automatically uses fallback
# No manual error handling required
response = await llm_manager.generate("Complex reasoning task")
```

## Monitoring & Metrics

### Usage Tracking

```python
from spoon_ai.llm.monitoring import LLMMonitor

# Track usage and costs automatically
monitor = LLMMonitor()
response = await llm.generate("Hello", monitor=monitor)

# Get metrics
metrics = monitor.get_metrics()
print(f"Tokens used: {metrics.total_tokens}")
print(f"Cost: ${metrics.total_cost}")
```

### Performance Monitoring

```python
# Monitor response times and success rates
monitor.log_request(
    provider="openai",
    model="gpt-4",
    tokens=150,
    latency=1.2,
    success=True
)
```

## Best Practices

### Provider Selection

- **Test multiple providers** for your specific use case
- **Consider cost vs. quality** trade-offs
- **Use fallbacks** for production reliability

### Configuration Management

- **Store API keys securely** in environment variables
- **Use configuration files** for easy switching
- **Monitor usage and costs** regularly

### Performance Optimization

- **Cache responses** when appropriate
- **Use streaming** for long responses
- **Batch requests** when possible

### Error Handling Philosophy

The SpoonOS framework follows a "fail-fast, recover-gracefully" approach:

- **Automatic Recovery**: Common errors (rate limits, network issues) are handled automatically
- **Structured Errors**: Use specific error types instead of generic exceptions
- **Fallback Chains**: Configure multiple providers for automatic failover
- **Minimal Try-Catch**: Let the framework handle errors; only catch when you need custom logic

```python
# Preferred: Let framework handle errors
response = await llm_manager.generate("Hello world")

# Only use explicit error handling for custom business logic
if response.provider != "openai":
    logger.info(f"Fell back to {response.provider}")
```

## Next Steps

- [Agents](./agents.md) - Learn how agents use LLMs
- [MCP Protocol](./mcp-protocol.md) - Dynamic tool integration
- [Configuration Guide](../getting-started/configuration.md) - Detailed setup instructions

---

FILE: docs/core-concepts/long-term memory.md

# Long-Term Memory

Long-term memory lets your agent **remember across sessions**. Unlike short-term memory (which resets each conversation), long-term memory persists indefinitely—enabling personalized experiences, learning from past interactions, and building knowledge over time.

## Why Long-Term Memory?

Without long-term memory, every conversation starts from zero:

```text
Session 1: User: "I prefer dark mode"    Agent: "Got it!"
Session 2: User: "Change my settings"    Agent: "What settings?" ← forgot everything
```

With long-term memory:

```text
Session 1: User: "I prefer dark mode"    Agent: "Got it!" → saves to memory
Session 2: User: "Change my settings"    Agent: "I'll enable dark mode for you" ← remembers
```

## How It Works

SpoonOS integrates with [Mem0](https://mem0.ai), a managed memory service that handles storage, indexing, and semantic search:

```mermaid
graph LR
    A[Agent] -->|"store"| B[SpoonMem0]
    B -->|"embed & index"| C[Mem0 Cloud]
    A -->|"search"| B
    B -->|"semantic query"| C
    C -->|"relevant memories"| B
    B -->|"results"| A
```

| Feature | How It Helps |
|---------|--------------|
| **Semantic search** | Find memories by meaning: "user preferences" finds "I like dark mode" |
| **Auto-scoping** | Memories are isolated per user/agent automatically |
| **Graceful fallback** | If Mem0 is down, operations return empty (no crashes) |

## What Can You Store?

| Memory Type | Example |
|-------------|---------|
| **Preferences** | "User prefers concise responses" |
| **Facts** | "User's portfolio includes BTC and ETH" |
| **Context** | "User is a day trader focused on meme coins" |
| **History** | "User asked about Solana DeFi protocols last week" |

---

## Quick Start

```bash
pip install spoon-ai mem0ai
export MEM0_API_KEY="your-mem0-key"
```

```python
from spoon_ai.memory.mem0_client import SpoonMem0

mem0 = SpoonMem0({"user_id": "user_123"})

# Store and search
mem0.add_text("User prefers dark mode")
results = mem0.search("UI preferences")
print(results)
```

---

**Core class:** `spoon_ai.memory.mem0_client.SpoonMem0`

### Initialization

```python
from spoon_ai.memory.mem0_client import SpoonMem0

mem0 = SpoonMem0({
    "api_key": "YOUR_MEM0_API_KEY",   # or MEM0_API_KEY env var
    "user_id": "user_123",            # scope all operations to this user
    "collection": "my_namespace",     # optional namespace isolation
    "metadata": {"project": "demo"},  # auto-attached to writes
    "filters": {"project": "demo"},   # auto-applied to queries
    "async_mode": False,              # sync writes (default)
})

if not mem0.is_ready():
    print("Mem0 service unavailable")
```

### Add Memory

Store conversation history or individual text:

```python
# Add conversation messages
mem0.add_memory([
    {"role": "user", "content": "I love Solana meme coins"},
    {"role": "assistant", "content": "Got it, focusing on Solana"},
], user_id="user_123")

# Add single text (shorthand)
mem0.add_text("User prefers low gas fees")
```

Async variant: `await mem0.aadd_memory(messages, user_id=...)`

### Search memory
```python
results = mem0.search_memory(
    "Solana meme coins high risk",
    user_id="user_123",
    limit=5,
)
for r in results:  # results is a list of strings extracted from Mem0 responses
    print("-", r)
```

Async variant: `await mem0.asearch_memory(query, user_id=...)`

### Get all memory
```python
all_memories = mem0.get_all_memory(user_id="user_123", limit=20)  # returns [] if client is not ready or call fails
```

## Demo: Intelligent Web3 Portfolio Assistant
Path: `examples/mem0_agent_demo.py`

Key idea: The agent (ChatBot) is configured with Mem0 so it can recall user preferences after restart.

```python
from spoon_ai.chat import ChatBot

USER_ID = "crypto_whale_001"
SYSTEM_PROMPT = "...portfolio assistant..."

mem0_config = {
    "user_id": USER_ID,
    "metadata": {"project": "web3-portfolio-assistant"},
    "async_mode": False,  # sync writes so next query sees the data
}

# Create an LLM with long-term memory enabled
llm = ChatBot(
    llm_provider="openrouter",
    model_name="openai/gpt-5.1",
    enable_long_term_memory=True,
    mem0_config=mem0_config,
)
```

Flow:
1) **Session 1** – capture preferences: user says they are a high-risk Solana meme trader; model replies; Mem0 stores the interaction.
2) **Session 2** – reload a fresh ChatBot with the same `mem0_config`; the agent recalls past preferences (via Mem0 search) before answering.
3) **Session 3** – user pivots to safe Arbitrum yield; new info is stored; subsequent queries reflect updated preferences.

Run the demo:
```bash
python examples/mem0_agent_demo.py
```

## Notes & Best Practices
- Always set `MEM0_API_KEY` or pass `api_key` in `mem0_config`.
- Use a stable `user_id` (or `agent_id`) so memories stay scoped; include `collection`/`filters` if you want stricter isolation. The wrapper injects `user_id` into filters and metadata if missing.
- Keep `async_mode=False` during demos/tests to avoid read-after-write delays; the wrapper always uses `mem0_config.get("async_mode", False)` for adds (no per-call override).
- Handle absence gracefully: `SpoonMem0.is_ready()` lets you disable LTM if Mem0 isn’t installed or configured; helpers will otherwise return empty results when the client is unavailable.

---

FILE: docs/core-concepts/mcp-protocol.md

# MCP Protocol

The **Model Context Protocol (MCP)** is an open standard for connecting AI agents to external tools and data sources. Instead of hardcoding tool integrations, agents discover tools dynamically at runtime—enabling modular, federated ecosystems where tools can be shared across different AI applications.

## Why MCP?

Traditional tool integration is brittle:

```text
❌ Old way: Agent ↔ Hardcoded Tool A ↔ Hardcoded Tool B ↔ Hardcoded Tool C
✅ MCP way: Agent ↔ MCP Client ↔ Any MCP Server (tools discovered at runtime)
```

With MCP, your agent can:

- **Discover tools dynamically** — No code changes when tools are added or updated
- **Connect to any MCP server** — Use tools from Cursor, Claude Desktop, or custom servers
- **Share tools across apps** — One MCP server can serve multiple agents
- **Hot-reload** — Update tool definitions without redeploying

## How It Works

```mermaid
sequenceDiagram
    participant Agent
    participant MCP Client
    participant MCP Server
    participant External API

    Agent->>MCP Client: Connect to server
    MCP Client->>MCP Server: list_tools()
    MCP Server-->>MCP Client: [tool schemas]
    Agent->>MCP Client: call_tool("search", {query: "..."})
    MCP Client->>MCP Server: Execute tool
    MCP Server->>External API: API call
    External API-->>MCP Server: Response
    MCP Server-->>MCP Client: Result
    MCP Client-->>Agent: Tool output
```

| Concept | Description |
|---------|-------------|
| **MCP Server** | Exposes tools via a standard protocol. Can run as subprocess (stdio), HTTP/SSE, or WebSocket. |
| **MCP Client** | Connects to servers, discovers tools, and executes them on behalf of agents. |
| **Tool Schema** | JSON-schema definition of tool name, description, and parameters—fetched at runtime. |
| **Resources** | Optional: MCP also supports resource URIs for documents, databases, and other data. |

## MCP vs Other Approaches

| Aspect | MCP | OpenAI Plugins | Hardcoded Tools |
|--------|-----|----------------|-----------------|
| **Discovery** | Runtime `list_tools()` | Manifest file at URL | Compile-time |
| **Transport** | stdio, SSE, WebSocket | HTTPS only | In-process |
| **Ecosystem** | Cursor, Claude, SpoonOS, etc. | ChatGPT only | Single app |
| **Updates** | Hot-reload, no redeploy | Redeploy plugin | Redeploy app |

---

## Quick Start

```bash
pip install spoon-ai
```

```python
import asyncio
from spoon_ai.mcp import MCPClient

async def main():
    async with MCPClient.from_config({
        "command": "npx",
        "args": ["-y", "@anthropic/mcp-server-filesystem", "/tmp"]
    }) as client:
        tools = await client.list_tools()
        result = await client.call_tool("read_file", {"path": "/tmp/test.txt"})
        print(result)

asyncio.run(main())
```

---

## Architecture

```mermaid
graph TD
    A[Agent] --> B[MCP Client]
    B --> C[MCP Server]
    C --> D[Tool 1]
    C --> E[Tool 2]
    C --> F[Tool N]

    B --> G[Tool Discovery]
    B --> H[Tool Execution]
    B --> I[Resource Access]
```

### MCP Components

1. **MCP Server** - Hosts tools and resources
2. **MCP Client** - Connects agents to servers
3. **Tools** - Executable functions with defined schemas
4. **Resources** - Data sources and content

## Setting Up MCP

### Basic MCP Server

```python
import asyncio
from spoon_ai.tools.mcp_tools_collection import MCPToolsCollection

mcp_tools = MCPToolsCollection()

async def main():
    # Runs a FastMCP SSE server. Change the port as needed.
    await mcp_tools.run(port=8765)

asyncio.run(main())
```

### MCP Client Configuration

```python
import asyncio
from spoon_ai.agents.mcp_client_mixin import MCPClientMixin

class MCPEnabledClient(MCPClientMixin):
    def __init__(self, transport: str):
        super().__init__(transport)

client = MCPEnabledClient("ws://localhost:8765")

async def list_tools():
    async with client.get_session() as session:
        return await session.list_tools()

tools = asyncio.run(list_tools())
for tool in tools:
    print(f"{tool.name}: {tool.description}")
```

## Tool Discovery

### Automatic Discovery

```python
async def discover_tools():
    async with client.get_session() as session:
        return await session.list_tools()

tools = asyncio.run(discover_tools())
for tool in tools:
    print(tool.name)
```

### Tool Registration

```python
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.mcp_tools_collection import mcp_tools

class WeatherTool(BaseTool):
    name: str = "get_weather"
    description: str = "Get current weather for a location"
    parameters: dict = {
        "type": "object",
        "properties": {
            "location": {"type": "string", "description": "City name"}
        },
        "required": ["location"]
    }

    async def execute(self, location: str) -> dict:
        # Weather API call implementation
        return {"location": location, "temperature": 22, "condition": "sunny"}

# Register tool with the running MCP server
asyncio.run(mcp_tools.add_tool(WeatherTool()))
```

## Tool Execution

### Direct Execution

```python
# Execute tool via MCP client
result = asyncio.run(client.call_mcp_tool("get_weather", location="New York"))
print(result)
```

### Agent-Driven Execution

```python
from spoon_ai.agents.spoon_react_mcp import SpoonReactMCP

# Agent that can consume MCP tools discovered via its transport
agent = SpoonReactMCP()
response = await agent.run("What's the weather like in San Francisco?")
print(response)
```

## MCP Configuration

### Server Configuration

```json
{
  "mcp": {
    "servers": [
      {
        "name": "local_tools",
        "url": "http://localhost:8000",
        "timeout": 30
      },
      {
        "name": "external_api",
        "url": "https://api.example.com/mcp",
        "auth": {
          "type": "bearer",
          "token": "your_token_here"
        }
      }
    ]
  }
}
```

### Client Configuration

```python
from spoon_ai.agents.mcp_client_mixin import MCPClientMixin


class ConfiguredMCPClient(MCPClientMixin):
    def __init__(self, transport: str, *, timeout: int = 30):
        super().__init__(transport)
        self.timeout = timeout


# Configure MCP client using FastMCP transport (SSE/WS)
client = ConfiguredMCPClient("ws://localhost:8765", timeout=30)
```

## Security Considerations

### Authentication

> **Note:** `AuthenticatedMCPServer` is a conceptual example and not shipped in `spoon_ai`. Implement authentication using your FastMCP server framework (e.g., middleware or request hooks).

```python
# Server-side authentication
class AuthenticatedMCPServer:
    def __init__(self, api_key: str):
        self.api_key = api_key

    def authenticate(self, request_key: str) -> bool:
        return request_key == self.api_key
```

### Tool Permissions

```python
# Define tool permissions
class RestrictedTool(BaseTool):
    required_permissions = ["read_data", "write_files"]

    async def execute(self, **kwargs):
        # Check permissions before execution
        if not self.check_permissions():
            raise PermissionError("Insufficient permissions")

        return await self.perform_action(**kwargs)
```

### Input Validation

```python
# Validate tool inputs
class SecureTool(BaseTool):
    async def execute(self, user_input: str) -> str:
        # Sanitize input
        clean_input = self.sanitize_input(user_input)

        # Validate against schema
        if not self.validate_input(clean_input):
            raise ValueError("Invalid input")

        return await self.process(clean_input)
```

## Performance Optimization

### Connection Pooling

> **Note:** `MCPConnectionPool` is not provided by `spoon_ai`. The `MCPClientMixin` already reuses sessions per task; wrap it or your FastMCP client in your own pooling logic if you need cross-server pooling.

```python
# Reuse sessions via MCPClientMixin (simplest pooling strategy)
from spoon_ai.agents.mcp_client_mixin import MCPClientMixin

class PooledMCPClient(MCPClientMixin):
    def __init__(self, transport: str):
        super().__init__(transport)

client = PooledMCPClient("ws://localhost:8765")

async def use_pool():
    async with client.get_session() as session:
        return await session.list_tools()
```

### Caching

> **Note:** `MCPCache` is not included in `spoon_ai`. Use a simple in-memory cache or a library like `functools.lru_cache` for discovery results.

```python
# Minimal in-memory cache for tool discovery
tool_cache: dict[str, list] = {}

async def get_tools_cached():
    if "tools" not in tool_cache:
        tool_cache["tools"] = await mcp_tools.discover_tools()
    return tool_cache["tools"]
```

### Async Operations

```python
# Execute multiple tools concurrently
import asyncio

async def parallel_execution():
    tasks = [
        mcp_tools.execute_tool("tool1", {"param": "value1"}),
        mcp_tools.execute_tool("tool2", {"param": "value2"}),
        mcp_tools.execute_tool("tool3", {"param": "value3"})
    ]

    results = await asyncio.gather(*tasks)
    return results
```

## Common Use Cases

### API Integration

```python
# Integrate external APIs through MCP
class APITool(BaseTool):
    name = "api_call"

    async def execute(self, endpoint: str, method: str = "GET") -> dict:
        async with aiohttp.ClientSession() as session:
            async with session.request(method, endpoint) as response:
                return await response.json()
```

### Database Access

```python
# Database operations through MCP
class DatabaseTool(BaseTool):
    name = "query_database"

    async def execute(self, query: str) -> list:
        # Execute database query
        return await self.db.execute(query)
```

### File Operations

```python
# File system operations
class FileTool(BaseTool):
    name = "read_file"

    async def execute(self, filepath: str) -> str:
        with open(filepath, 'r') as f:
            return f.read()
```

## Best Practices

### Tool Design

- **Clear naming** - Use descriptive tool names
- **Comprehensive schemas** - Define complete parameter schemas
- **Error handling** - Leverage framework's automatic error handling
- **Documentation** - Provide clear descriptions and examples

### Performance

- **Connection reuse** - Reuse MCP connections when possible
- **Caching** - Cache discovery results and frequently used data
- **Timeouts** - Set appropriate timeouts for tool execution

### Security

- **Input validation** - Always validate tool inputs
- **Authentication** - Implement proper authentication mechanisms
- **Permissions** - Use least-privilege access principles

### Error Handling Philosophy

The SpoonOS framework follows a "fail-fast, recover-gracefully" approach for MCP operations:

- **Automatic Recovery**: Connection failures, timeouts, and server errors are handled automatically
- **Graceful Degradation**: When tools are unavailable, the system provides meaningful fallbacks
- **Minimal Manual Handling**: Let the framework handle errors; only intervene for custom business logic

```python
# Preferred: Let framework handle MCP errors
result = await mcp_tools.execute_tool("weather_tool", {"location": "NYC"})

# Framework automatically handles:
# - Server connection issues
# - Tool discovery failures
# - Execution timeouts
# - Parameter validation errors
```

## Troubleshooting

### Common Issues

#### Connection Errors

The framework automatically handles connection failures with built-in retry mechanisms:

```python
# Framework handles connection failures automatically
await mcp_client.connect()  # Automatic retry with exponential backoff
```

#### Tool Discovery Failures

```python
# Framework provides graceful handling of discovery issues
tools = await mcp_tools.discover_tools()
# Automatic fallback to cached tools if server unavailable
```

#### Execution Timeouts

```python
# Framework manages timeouts automatically
result = await mcp_tools.execute_tool("slow_tool", {})
# Automatic timeout handling with configurable limits
```

## Next Steps

### 📚 **MCP Implementation Examples**

#### 🔍 [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- Complete MCP server integration and tool discovery
- Real-world MCP implementation with Tavily web search
- Dynamic tool loading and orchestration
- Production-ready MCP error handling and recovery

**Key features:**
- Tavily MCP server integration via `npx tavily-mcp`
- Automatic tool discovery and validation
- Seamless integration with existing SpoonOS architecture
- Advanced error handling for MCP server failures

**Learning outcomes:**
- How to initialize and manage MCP servers
- Dynamic tool discovery patterns
- MCP server error handling and recovery
- Integration of MCP tools with LLM workflows

### 🛠️ **Development Resources**

- **[Tools System](./tools.md)** - Learn about the complete tool ecosystem
- **[Custom Tool Development](../how-to-guides/add-custom-tools.md)** - Build MCP-compatible tools
- **[MCP Tool Reference](../api-reference/spoon_ai/tools/)** - MCP-specific tool documentation

### 📖 **Additional Resources**

- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Agent-MCP integration patterns
- **[API Reference](../api-reference/index)** - Complete SpoonOS API documentation
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**What it demonstrates:**
- Complete MCP server integration and tool discovery
- Real-world MCP implementation with Tavily web search
- Dynamic tool loading and orchestration
- Production-ready MCP error handling and recovery

**Key features:**
- Tavily MCP server integration via `npx tavily-mcp`
- Automatic tool discovery and validation
- Seamless integration with existing SpoonOS architecture
- Advanced error handling for MCP server failures

**Learning outcomes:**
- How to initialize and manage MCP servers
- Dynamic tool discovery patterns
- MCP server error handling and recovery
- Integration of MCP tools with LLM workflows

### 🛠️ **Development Resources**

- **[Tools System](./tools.md)** - Learn about the complete tool ecosystem
- **[Custom Tool Development](../how-to-guides/add-custom-tools.md)** - Build MCP-compatible tools
- **[MCP Tool Reference](../api-reference/spoon_ai/tools/)** - MCP-specific tool documentation

### 📖 **Additional Resources**

- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Agent-MCP integration patterns
- **[API Reference](../api-reference/index)** - Complete SpoonOS API documentation

---

FILE: docs/core-concepts/Short-term memory.md

# Short-Term Memory

Short-term memory keeps track of the **current conversation**. It's what lets your agent remember "My name is Alice" three messages ago, and what prevents context windows from overflowing in long conversations.

## The Problem

LLMs have limited context windows. As conversations grow:

```text
Turn 1:  User: "My name is Alice"          ← 10 tokens
Turn 50: User: "What's my name again?"     ← 50,000 tokens total
         Agent: "I don't know" ← context overflow, lost earlier messages
```

Naive solutions have tradeoffs:

| Approach | Problem |
|----------|---------|
| **Keep everything** | Exceeds context window, costs explode |
| **Drop old messages** | Loses important context ("My name is Alice") |
| **Fixed sliding window** | Arbitrary cutoff, may drop critical info |

## SpoonOS Solution

`ShortTermMemoryManager` intelligently manages context:

```mermaid
graph LR
    A[New Message] --> B{Token Budget?}
    B -->|Under limit| C[Add to history]
    B -->|Over limit| D[Trim Strategy]
    D --> E[Summarize old messages]
    E --> F[Keep summary + recent]
    F --> C
```

| Feature | What It Does |
|---------|--------------|
| **Token-aware** | Tracks actual token count, not message count |
| **Smart trimming** | Multiple strategies: oldest first, from start, from end |
| **Summarization** | Condenses old messages into a summary when needed |
| **Built-in** | `ChatBot` handles this automatically—no extra code |

## When To Use

- **Chatbots** with multi-turn conversations
- **Agents** that need to remember earlier context
- **Long sessions** that would exceed context limits
- **Cost optimization** to reduce token usage

---

## Quick Start

```bash
pip install spoon-ai
export OPENAI_API_KEY="your-key"
```

```python
import asyncio
from spoon_ai.chat import ChatBot

# ChatBot includes built-in short-term memory with auto-trimming
llm = ChatBot(model_name="gpt-4.1", llm_provider="openai")

async def main():
    await llm.chat("My name is Alice")
    await llm.chat("What's the capital of France?")
    response = await llm.chat("What's my name?")  # Remembers "Alice"
    print(response)

asyncio.run(main())
```

---

## ShortTermMemoryManager

For fine-grained control beyond `ChatBot`'s automatic handling, use `ShortTermMemoryManager` directly:

```python
import asyncio
from spoon_ai.memory.short_term_manager import ShortTermMemoryManager, TrimStrategy
from spoon_ai.schema import Message

manager = ShortTermMemoryManager()
history = [
    Message(id="u1", role="user", content="Hello!"),
    Message(id="a1", role="assistant", content="Hi there — how can I help?"),
    Message(id="u2", role="user", content="What's DeFi?"),
]


#  Trim the message list by token budget
trimmed = asyncio.run(
    manager.trim_messages(
        messages=history,
        max_tokens=48,
        strategy=TrimStrategy.FROM_END,
        keep_system=True,
    )
)

#  Summarize history before model call
llm_ready, removals, summary = asyncio.run(
    manager.summarize_messages(
        messages=history,
        max_tokens_before_summary=48,
        messages_to_keep=2,
        summary_model="anthropic/claude-3.5-sonnet",
        llm_manager=chatbot.llm_manager,
        llm_provider=chatbot.llm_provider,
        existing_summary=chatbot.latest_summary() or "",
    )
)
```

 `llm_ready` — condensed history you can pass to the LLM
 `removals` — list of RemoveMessage directives:apply removals to your persisted history using spoon_ai.graph.reducers.add_messages.

Note: both `summarize_messages()` and `ChatBot.ask()` invoke your configured LLM. Ensure `chatbot.llm_manager`/`chatbot.llm_provider` (and any required API keys or env vars) are set so these examples can run end‑to‑end.

```python
from spoon_ai.chat import ChatBot
from spoon_ai.graph.reducers import add_messages

chatbot = ChatBot(enable_short_term_memory=True)
history = [
    Message(id="u1", role="user", content="Hello!"),
    Message(id="a1", role="assistant", content="Hi there — how can I help?"),
    Message(id="u2", role="user", content="What's DeFi?"),
]
# Remove the latest assistant message
assistant_ids = [msg.id for msg in history if msg.role == "assistant"]
remove_last = chatbot.remove_message(assistant_ids[-1])

# Or clear the entire history
remove_all = chatbot.remove_all_messages()

# Apply directives to persisted history
updated_history = add_messages(history, [remove_last])
cleared_history = add_messages(history, [remove_all])
```

`add_messages()` merges the removal directives into the existing history,
deleting targeted entries (or the entire transcript).
This mirrors how the short-term memory manager emits `RemoveMessage` items
when summarization trims older turns.

---

## Inspecting Thread State and Checkpoints

every time the graph runs, you can retrieve the latest snapshot (messages plus metadata), iterate the full checkpoint history, or read a `CheckpointTuple` for an external consumer. This makes it easy to debug memory behaviour, replay from any checkpoint, or sync state to persistent storage. The example below shows how to fetch the most recent summary, list all checkpoints, and view the tuple-style payload.

```python
config = {"configurable": {"thread_id": "memory_demo_thread"}}

snapshot = graph.get_state(config)
print("Latest checkpoint:", snapshot.metadata.get("checkpoint_id"))

for snap in graph.get_state_history(config):
    print("History id:", snap.metadata.get("checkpoint_id"))

checkpoint_tuple = graph.checkpointer.get_checkpoint_tuple(config)
print("Checkpoint tuple:", checkpoint_tuple)

for entry in graph.checkpointer.iter_checkpoint_history(config):
    print("Tuple history entry:", entry)
```

If you are using a compiled graph (`CompiledGraph`), call `graph.graph.get_state(config)` and `graph.graph.get_state_history(config)` instead; the snippet above assumes `graph` is a `StateGraph`.

---

FILE: docs/core-concepts/tools.md

# Tools

Tools are **callable capabilities** that let agents interact with the outside world—APIs, databases, blockchains, file systems, and any other external service. Without tools, an LLM can only generate text. With tools, it can take action.

## Why Tools?

An LLM doesn't know today's Bitcoin price, can't send emails, and has no way to query your database. Tools bridge this gap:

```mermaid
graph LR
    A[Agent] -->|"call tool"| B[Tool: get_price]
    B -->|"API call"| C[Binance API]
    C -->|"$67,432"| B
    B -->|"return"| A
    A -->|"Bitcoin is $67,432"| D[User]
```

SpoonOS tools are:

- **Typed** — JSON-schema parameters prevent LLM hallucination of invalid inputs
- **Validated** — Runtime checks ensure data integrity before execution
- **Async** — Non-blocking I/O for high-performance agent loops
- **Composable** — Bundle tools into toolkits, share via MCP protocol

## Tool Anatomy

Every SpoonOS tool has three parts:

| Part | Purpose | Example |
|------|---------|---------|
| **name** | Unique identifier the LLM uses to call the tool | `"get_crypto_price"` |
| **description** | Natural language explanation of what the tool does | `"Get real-time price for a cryptocurrency"` |
| **parameters** | JSON-schema defining expected inputs | `{"symbol": {"type": "string"}}` |

The LLM reads the description to decide *when* to use a tool and the parameters to know *how* to call it.

## What Can You Build?

| Tool Type | Examples |
|-----------|----------|
| **Data retrieval** | Web search, database queries, API calls |
| **Crypto/Web3** | CEX trading, DEX swaps, on-chain reads, wallet operations |
| **File operations** | Read/write files, parse documents, generate reports |
| **Communication** | Send emails, post to Slack, create tickets |
| **Computation** | Run code, execute SQL, perform calculations |

## SpoonOS vs Other Tool Systems

| Feature | SpoonOS | LangChain | OpenAI Functions |
|---------|---------|-----------|------------------|
| **Definition** | `BaseTool` class | `@tool` decorator | JSON in API call |
| **Validation** | JSON-schema + runtime | Optional Pydantic | Server-side only |
| **Remote tools** | MCP protocol (stdio/SSE/WS) | API wrappers | N/A |
| **Discovery** | `ToolManager` + semantic search | `load_tools()` | Manual |
| **Crypto native** | Built-in CEX/DEX/on-chain | Third-party | N/A |

---

## Quick Start

```bash
pip install spoon-ai
```

```python
import asyncio
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools import ToolManager

# Define a tool with JSON-schema parameters
class GreetTool(BaseTool):
    name = "greet"
    description = "Greet someone by name"
    parameters = {
        "type": "object",
        "properties": {"name": {"type": "string"}},
        "required": ["name"]
    }

    async def execute(self, name: str) -> str:
        return f"Hello, {name}!"

# Register and execute
manager = ToolManager([GreetTool()])

async def main():
    result = await manager.execute(name="greet", tool_input={"name": "World"})
    print(result)  # Hello, World!

asyncio.run(main())
```

---

## Tool Types

### Local Tools (`BaseTool`)

All tools inherit from `BaseTool` with three required attributes and one method:

```python
from spoon_ai.tools.base import BaseTool

class MyTool(BaseTool):
    name = "my_tool"                    # Unique identifier
    description = "What this tool does" # LLM reads this to decide when to use it
    parameters = {                      # JSON-schema for input validation
        "type": "object",
        "properties": {
            "arg1": {"type": "string", "description": "First argument"},
            "arg2": {"type": "integer", "default": 10}
        },
        "required": ["arg1"]
    }

    async def execute(self, arg1: str, arg2: int = 10) -> str:
        return f"Result: {arg1}, {arg2}"
```

The `__call__` method forwards to `execute()`, so `await tool(arg1="value")` works.

### ToolManager

Orchestrates tool registration, lookup, and execution:

```python
from spoon_ai.tools import ToolManager

manager = ToolManager([MyTool(), AnotherTool()])

# Execute by name
result = await manager.execute(name="my_tool", tool_input={"arg1": "hello"})

# Get tool specs for LLM function calling
specs = manager.to_params()  # List of OpenAI-compatible tool definitions
```

**Key methods:**

- `add_tool(tool)` / `add_tools([...])` — Register tools
- `remove_tool(name)` — Unregister by name
- `get_tool(name)` — Retrieve tool instance
- `to_params()` — Export OpenAI-compatible tool definitions
- `index_tools()` / `query_tools(query)` — Semantic search (requires Pinecone + OpenAI)

### Crypto toolkit (optional)
If `spoon-toolkits` is installed, you can load its crypto tools:
```python
from spoon_ai.tools.crypto_tools import get_crypto_tools, create_crypto_tool_manager

tools = get_crypto_tools()              # returns instantiated toolkit tools
manager = create_crypto_tool_manager()  # ToolManager with all crypto tools
```
Environment variables for these tools depend on the specific provider (e.g., `OKX_API_KEY`, `BITQUERY_API_KEY`, `RPC_URL`, etc.).

### MCP client tools (`MCPTool`)
`MCPTool` lets an agent call tools hosted on an MCP server.
```python
from spoon_ai.tools.mcp_tool import MCPTool

mcp_tool = MCPTool(
    mcp_config={
        "url": "http://localhost:8765",      # or ws://..., or command/args for stdio
        "transport": "sse",                  # optional: "sse" (default) | "http"
        "timeout": 30,
        "max_retries": 3,
    }
)
# The tool’s schema/description is fetched dynamically from the MCP server.
```
`MCPTool.execute(...)` will fetch the server’s tool list, align the name/parameters, and perform retries and health checks.

### MCP server (`MCPToolsCollection`)
You can expose local or toolkit tools as an MCP server:
```python
from spoon_ai.tools.mcp_tools_collection import MCPToolsCollection
import asyncio

mcp_tools = MCPToolsCollection()  # wraps spoon-toolkits tools if installed
asyncio.run(mcp_tools.run(port=8765))  # SSE server by default
```
This uses `fastmcp` under the hood and auto-registers each tool as an MCP `FunctionTool`.

## Configuration
- **Core**: none required for basic tools.
- **Embedding index (optional)**: `OPENAI_API_KEY`, `PINECONE_API_KEY`.
- **Crypto/toolkit tools**: provider-specific keys (e.g., `OKX_API_KEY`, `BITQUERY_API_KEY`, `RPC_URL`, `GOPLUSLABS_API_KEY`).
- **MCP**: set transport target via `mcp_config` (`url` or `command` + `args`/`env`).

## Best Practices
- Keep tools single-purpose with clear `parameters` JSON schema.
- Validate inputs inside `execute`; raise rich errors for better agent feedback.
- Prefer async I/O in `execute` to avoid blocking the event loop.
- Reuse `ToolManager` for name-based dispatch and tool metadata generation.
- When using toolkit or MCP tools, fail gracefully if optional dependencies or servers are missing.

## See Also
- API reference: `../api-reference/tools/base-tool.md`
- MCP protocol details: `./mcp-protocol.md`
- Custom tool guide: `../how-to-guides/add-custom-tools.md`

---

FILE: docs/core-concepts/x402-payments.md

# x402 Payments

x402 enables **agents to pay for things autonomously**. When an agent hits a paywall (HTTP 402), it automatically signs a crypto payment, retries the request, and continues—no human intervention required. This creates a native monetization layer for AI services.

## Why x402?

Traditional payments don't work for autonomous agents:

| Problem | With Traditional Payments | With x402 |
|---------|---------------------------|-----------|
| Agent hits paywall | ❌ Wait for human to enter credit card | ✅ Auto-sign and retry |
| Micropayments ($0.001) | ❌ Fees exceed payment | ✅ Low-cost on L2s |
| Settlement | ❌ 1-3 days | ✅ Instant |
| Verification | ❌ Trust Stripe API | ✅ Cryptographic proof |

## How It Works

```mermaid
sequenceDiagram
    participant Agent
    participant API Server
    participant x402 Facilitator
    participant Blockchain

    Agent->>API Server: GET /premium-data
    API Server-->>Agent: 402 Payment Required (price, recipient)
    Agent->>Agent: Sign EIP-712 payment
    Agent->>API Server: GET /premium-data + X-PAYMENT header
    API Server->>x402 Facilitator: Verify signature
    x402 Facilitator->>Blockchain: Execute transfer
    Blockchain-->>x402 Facilitator: Confirmed
    x402 Facilitator-->>API Server: Valid
    API Server-->>Agent: 200 OK + data
```

| Step | What Happens |
|------|--------------|
| **1. Request** | Agent calls a paid API endpoint |
| **2. 402 Response** | Server returns payment requirements (amount, token, recipient) |
| **3. Sign** | Agent signs an EIP-712 typed-data payload (no gas yet) |
| **4. Retry** | Agent sends request again with signed payment header |
| **5. Verify & Execute** | Facilitator verifies signature and executes transfer on-chain |
| **6. Success** | Server returns the requested data |

## x402 vs Alternatives

| Aspect | x402 | Stripe | Lightning |
|--------|------|--------|-----------|
| **Settlement** | Instant | 1-3 days | Instant |
| **Agent autonomy** | Auto-sign | Needs webhook | Manual channel |
| **Micropayments** | ✅ L2 fees | ❌ High fees | ✅ |
| **Verification** | Cryptographic | API call | Node verification |

---

## Quick Start

```bash
pip install spoon-ai
export PRIVATE_KEY="your-wallet-private-key"
```

```python
from spoon_ai.agents import SpoonReactAI
from spoon_ai.payments import x402_paywalled_request

# Agent auto-detects 402 and signs payments
agent = SpoonReactAI(tools=[x402_paywalled_request])
result = await agent.run("Fetch https://api.example.com/premium")
```

---

## Components

| Piece | Role inside SpoonOS |
| --- | --- |
| **x402 facilitator** | Public service (`https://x402.org/facilitator` by default) that verifies and settles signed payment payloads. |
| **Paywall server** | Your FastAPI router (`spoon_ai.payments.app`) that refuses unpaid requests with a 402 payload and forwards valid calls to agents. |
| **SpoonReact agent** | Issues HTTP probes, signs payments via tools, and stores payment receipts in memory. |
| **Signer** | Either the `PRIVATE_KEY` loaded in-process or a Turnkey identity configured via `TURNKEY_*` variables. |

## Configuration surfaces

Most deployments only need a `.env` entry and (optionally) config overrides:

```bash
X402_RECEIVER_ADDRESS=0xwallet-that-receives-fees
X402_FACILITATOR_URL=https://x402.org/facilitator
X402_DEFAULT_ASSET=
X402_DEFAULT_NETWORK=
X402_DEFAULT_SCHEME=exact
X402_DEFAULT_AMOUNT_USDC=
X402_PAYWALL_APP_NAME=SpoonOS Agent Services
X402_PAYWALL_APP_LOGO=https://your-domain.example/logo.png
X402_DEMO_URL=https://www.x402.org/protected
```

Key points:

- The system always prefers the local `PRIVATE_KEY`. If that variable is empty and Turnkey credentials (`TURNKEY_*`) exist, SpoonOS transparently switches to hosted signing.
- In CLI workflows (spoon-cli or the legacy `main.py` CLI), the `x402` block in the CLI `config.json` mirrors these defaults (branding, description, timeout, etc.). Update that file when you need per-environment variance. The core SDK still reads values from environment variables.
- Setting `X402_DEFAULT_ASSET` ensures all typed-data domains reference the real USDC contract so signatures pass facilitator validation.

## Runtime lifecycle

```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant Paywall as Paywall Router (/x402)
    participant Facilitator

    User->>Agent: Task / query
    Agent->>Paywall: http_probe (unauthenticated)
    alt Paywall open
        Paywall-->>Agent: HTTP 200 (no payment)
        Agent-->>User: Return content / summary
    else Paywalled (HTTP 402)
        Paywall-->>Agent: 402 + requirements
        Agent->>Agent: Merge requirements + config overrides
        Agent->>Agent: Select signer (PRIVATE_KEY or Turnkey)
        Agent->>Agent: Build typed-data payload
        Agent->>Agent: Sign -> X-PAYMENT header
        Agent->>Paywall: Retry with X-PAYMENT
        Paywall->>Facilitator: verify_payment (optional settle)
        Facilitator-->>Paywall: Valid? + receipt
        alt Invalid
            Paywall-->>Agent: 402 / error
        else Valid
            Paywall-->>Agent: 200 + X-PAYMENT-RESPONSE
            Agent->>Agent: Log receipt / update memory
            Agent-->>User: Protected content + summary
        end
    end
```

If the paid retry fails (for example `verify_payment` rejects the header or the facilitator reports an error), the paywall server immediately returns another `402` or error payload and the agent decides whether to run `x402_paywalled_request` again with corrected parameters. A successful verification moves straight into settlement and target agent execution, so there is no additional retry cycle once the `X-PAYMENT` header is accepted.

## Operational checklist

1. Use [https://faucet.circle.com/](https://faucet.circle.com/) to mint 0.01 USDC for the public demo.
2. Keep `X402_RECEIVER_ADDRESS` aligned with the wallet that ultimately receives settlements.
3. Monitor facilitator responses. Any `invalid_exact_evm_payload_signature` errors typically mean the `asset`, `chainId`, or nonce encoding no longer matches the paywall challenge.
4. Use `X402PaymentService.decode_payment_response(header)` to archive payment receipts in logs or analytics pipelines.

---

FILE: docs/examples/graph-crypto-analysis.md

---
sidebar_position: 2
---

# Graph Crypto Analysis

This example implements a complete cryptocurrency research and analysis pipeline using the declarative graph building system, demonstrating end-to-end LLM-driven decision making for market analysis and investment recommendations.

#### 📊 **Workflow Diagram**

```mermaid
graph TD
    A[Start] --> B[Fetch Binance Market Data]
    B --> C[Select Top 10 Pairs by Volume]
    C --> D[Prepare Token List]

    D --> E[Parallel Token Analysis]
    E --> F1[Token 1: Technical + News Analysis]
    E --> F2[Token 2: Technical + News Analysis]
    E --> F3[Token 3: Technical + News Analysis]
    E --> F4[Token 4: Technical + News Analysis]
    E --> F5[Token 5: Technical + News Analysis]
    E --> F6[Token 6: Technical + News Analysis]
    E --> F7[Token 7: Technical + News Analysis]
    E --> F8[Token 8: Technical + News Analysis]
    E --> F9[Token 9: Technical + News Analysis]
    E --> F10[Token 10: Technical + News Analysis]

    F1 --> G[Aggregate All Results]
    F2 --> G
    F3 --> G
    F4 --> G
    F5 --> G
    F6 --> G
    F7 --> G
    F8 --> G
    F9 --> G
    F10 --> G

    G --> H[LLM Final Aggregation]
    H --> I[Generate Market Report]
    I --> J[END]

    style A fill:#e1f5fe
    style J fill:#c8e6c9
    style E fill:#fff3e0
    style G fill:#fce4ec

    subgraph "Technical Analysis"
        F1
        F2
        F3
        F4
        F5
        F6
        F7
        F8
        F9
        F10
    end
```

#### 🎯 **Core Features**

**Intelligent Market Analysis:**
- LLM-driven token selection based on real-time market conditions
- Multi-timeframe analysis (1h, 4h) for comprehensive market view
- Dynamic decision flow guided by LLM analysis at each step

**Advanced Technical Analysis:**
- Real-time indicator calculation (RSI, MACD, EMA) using PowerData toolkit
- Market sentiment analysis and momentum evaluation
- Risk assessment and volatility metrics for each token

**LLM-Powered Synthesis:**
- Intelligent summarization of complex market data
- Data-driven investment recommendations with reasoning
- Short-term and macro-level market outlook generation

#### 🚀 **Key Capabilities**

- **Declarative Graph Building** - `GraphTemplate`, `NodeSpec`, `EdgeSpec` for modular workflows
- **High-Level API Integration** - `HighLevelGraphAPI` for automatic parameter inference
- **Complete Workflow** - End-to-end from data ingestion to final recommendations
- **Real API Integration** - Live Binance and cryptocurrency data via PowerData toolkit
- **LLM Decision Making** - Every major decision guided by LLM analysis
- **Advanced State Management** - Complex analysis state throughout the process
- **Error Recovery** - Robust error handling and fallback mechanisms

#### 📋 **Prerequisites**

```bash
# Required environment variables
export OPENAI_API_KEY="your-openai-api-key"          # Primary LLM
export ANTHROPIC_API_KEY="your-anthropic-api-key"   # Alternative LLM
export TAVILY_API_KEY="your-tavily-api-key"       # Search engine
```

#### 🏃 **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the declarative crypto analysis
python graph_crypto_analysis.py
```

#### 🔍 **What to Observe**

**Architecture:**
- How `GraphTemplate` and `NodeSpec` simplify workflow construction
- `HighLevelGraphAPI` automatically inferring parameters from queries
- Modular node implementations with better separation of concerns

**Data Flow:**
- Real market data fetching from Binance API and PowerData toolkit
- LLM analysis of raw data for intelligent decision making
- Step-by-step process from data collection to final recommendations

**Technical Analysis:**
- Real-time indicator calculation using PowerData toolkit
- Correlation of different data sources
- Market sentiment analysis and quantification

**LLM Decision Process:**
- Token evaluation and selection for analysis
- Synthesis combining technical and fundamental analysis
- Investment recommendations with detailed reasoning

#### 📊 **Sample Output**

```
🔍 MARKET ANALYSIS REPORT
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

📈 SELECTED TOKENS FOR ANALYSIS: BTC, ETH, SOL, ADA

📊 TECHNICAL ANALYSIS:
• BTC/USDT: Bullish momentum, RSI: 68, MACD positive crossover
• ETH/USDT: Consolidation phase, approaching key resistance
• SOL/USDT: Strong uptrend, breaking previous highs
• ADA/USDT: Recovery phase, positive volume momentum

🎯 INVESTMENT RECOMMENDATIONS:
• SHORT-TERM: Consider BTC and SOL for momentum plays
• MEDIUM-TERM: Hold ETH through current consolidation
• RISK ASSESSMENT: Moderate volatility expected in next 24-48 hours

💡 MARKET OUTLOOK:
The current market shows strong bullish momentum with BTC leading...
```

#### 📁 **Source Code**

- **Main Example**: [graph_crypto_analysis.py](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)
- **Supporting Modules**:
  - `spoon_ai/graph/builder.py` - Declarative templates and high-level API
  - `spoon_ai/tools/crypto_tools.py` - PowerData integration helpers
  - `spoon_ai/graph/` - Core engine and monitoring utilities
  - [Tool System Docs](../core-concepts/tools.md)

#### 🎓 **Learning Outcomes**

- Using declarative graph building (`GraphTemplate`, `NodeSpec`, `EdgeSpec`)
- Leveraging `HighLevelGraphAPI` for automatic parameter inference
- Implementing modular, maintainable node functions
- Building complete end-to-end analysis systems with LLM integration
- Advanced cryptocurrency market analysis techniques
- Real-time data processing and technical indicator calculation
- LLM-driven decision making in complex workflows
- Error handling and data validation in financial applications

#### 💡 **Best Practices**

- Declarative architecture for improved modularity
- High-level API usage for automatic parameter inference
- Data validation and comprehensive error handling
- Performance optimization and efficient data processing
- Security considerations for API keys and financial data
- Modular architecture with clean separation of concerns

---

FILE: docs/examples/intent-graph-demo.md

---
sidebar_position: 1
---


# Intent Graph Demo

This example demonstrates an intelligent StateGraph workflow with advanced query routing, parallel execution, and memory management using the modern declarative graph building system.

#### 📊 **Workflow Diagram**

```mermaid
graph TD
    A[User Query] --> B[Bootstrap Session]
    B --> C[Load Memory]
    C --> D[Plan Analysis]
    D --> E{LLM Intent Analysis}
    E -->|general_qa| F[General Q&A]
    E -->|short_term_trend| G[Extract Symbol]
    E -->|macro_trend| H[Extract Symbol]
    E -->|deep_research| I[Deep Research Search]

    G --> J[Short-term Data Collection]
    H --> K[Macro Data Collection]
    I --> L[Research Sources]

    J --> M[Short-term Summary]
    K --> N[Macro Summary]
    L --> O[Research Report]

    M --> P[Review Trade]
    N --> P
    O --> P
    F --> P

    P --> Q[Update Memory]
    Q --> R[Finalize Response]
    R --> S[END]

    style A fill:#e1f5fe
    style S fill:#c8e6c9
    style E fill:#fff3e0
    style P fill:#fce4ec
```

#### 🎯 **Core Features**

**Intelligent Query Routing:**
- LLM-powered intent classification into: `general_qa`, `short_term_trend`, `macro_trend`, or `deep_research`
- Dynamic routing based on detected intent and conversation history
- Context-aware decision making with market context

**Parallel Data Processing:**
- Concurrent data fetching across multiple timeframes (15m, 30m, 1h, 4h, daily, weekly)
- Real-time cryptocurrency data integration
- Performance optimization through parallel execution

**Advanced Memory Management:**
- Persistent conversation context across sessions
- Automatic storage of learned patterns and market insights
- State preservation for analysis results and routing decisions

#### 🚀 **Key Capabilities**

- **Declarative Graph Building** - `GraphTemplate`, `NodeSpec`, `EdgeSpec` for modular workflows
- **High-Level API Integration** - `HighLevelGraphAPI` for automatic parameter inference
- **LLM Integration** - Advanced prompt engineering and response processing
- **Tool Orchestration** - Multi-source data integration (PowerData, Tavily, EVM swap)
- **Error Handling** - Robust recovery with duplicate log prevention
- **Performance Monitoring** - Built-in metrics and execution tracking

#### 📋 **Prerequisites**

```bash
# Required environment variables
export OPENAI_API_KEY="your-openai-api-key"
export TAVILY_API_KEY="your-tavily-api-key"       # Search engine
```

#### 🏃 **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the declarative intent graph demo
python intent_graph_demo.py
```

#### 🔍 **What to Observe**

**Architecture:**
- How `GraphTemplate` and `NodeSpec` simplify workflow construction
- `HighLevelGraphAPI` automatically inferring parameters from queries
- Modular node implementations with better separation of concerns

**Execution Flow:**
- Intelligent routing to appropriate analysis paths based on query intent
- Parallel data fetching across multiple timeframes
- Memory loading and updates throughout the process

**Performance:**
- Execution times for different routing paths
- Parallel vs sequential processing performance
- Memory usage optimization and duplicate log prevention

**Advanced Behaviors:**
- LLM-powered routing decisions based on intent analysis
- Real-time data integration from multiple sources
- Context maintenance across complex workflows

#### 📁 **Source Code**

- **Main Example**: [intent_graph_demo.py](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)
- **Supporting Modules**:
  - `spoon_ai/graph/` - Core graph system and declarative builders
  - `spoon_ai/graph/builder.py` - High-level API and parameter inference
  - [Graph System Docs](../core-concepts/graph-system.md)

#### 🎓 **Learning Outcomes**

- Using declarative graph building (`GraphTemplate`, `NodeSpec`, `EdgeSpec`)
- Leveraging `HighLevelGraphAPI` for automatic parameter inference
- Implementing modular, maintainable node functions
- Advanced LLM integration and prompt engineering
- Parallel processing for performance optimization
- Memory management in long-running processes
- Error handling and recovery strategies

#### 💡 **Best Practices**

- Declarative architecture for improved modularity
- High-level API usage for automatic parameter inference
- Scalable design for easy extension
- Resource-efficient implementation
- Maintainable, well-documented code

---

FILE: docs/examples/mcp-spoon-search-agent.md

---
sidebar_position: 3
---

# MCP Spoon Search Agent

This example demonstrates how to build an MCP (Model Context Protocol) enabled agent that seamlessly integrates web search capabilities with cryptocurrency analysis tools, creating a powerful research and analysis assistant.

#### 🎯 **Core Functionality**

**Intelligent Web Search Integration:**
- **Tavily MCP integration** - Advanced web search capabilities through the Model Context Protocol
- **Real-time information retrieval** - Access to current news, articles, and market data from across the web
- **Context-aware search** - Searches are guided by user intent and current market context

**Cryptocurrency Analysis Tools:**
- **Crypto PowerData integration** - Professional-grade cryptocurrency market data and analysis
- **Multi-exchange support** - Access to data from major exchanges (Binance, Coinbase, etc.)
- **Technical indicators** - Real-time calculation of RSI, MACD, EMA, and other key indicators

**Unified Analysis System:**
- **Cross-referenced insights** - Combines web search results with technical analysis
- **Macro market analysis** - Provides comprehensive market outlook by correlating multiple data sources
- **Intelligent synthesis** - LLM-powered synthesis of diverse information sources into coherent analysis

#### 🚀 **Key Features Demonstrated**

- **MCP Protocol Implementation** - Complete MCP server integration and tool discovery
- **Multi-tool Orchestration** - Seamless coordination between search and analysis tools
- **Real-time Data Processing** - Live data integration from multiple APIs
- **Advanced Error Handling** - Robust error recovery and fallback mechanisms
- **Modular Architecture** - Clean separation between MCP tools and analysis logic

#### 📋 **Prerequisites**

```bash
# Required environment variables
export TAVILY_API_KEY="your-tavily-api-key"        # Web search API
export OPENAI_API_KEY="your-openai-api-key"        # LLM responses
export ANTHROPIC_API_KEY="your-anthropic-api-key"  # Alternative LLM

# System requirements
npm install -g tavily-mcp  # Install Tavily MCP server
npx --version              # Ensure npx is available
```

#### 🏃 **Quick Start**

```bash
# Navigate to examples directory
cd spoon-core/example

# Install dependencies
pip install -r requirements.txt

# Run the MCP search agent
python spoon_search_agent.py
```

#### 🔍 **What to Observe**

**MCP Tool Discovery:**
- Watch how the system automatically discovers and connects to MCP servers
- Observe the dynamic tool loading process
- See how tools are validated and initialized

**Search-Analysis Integration:**
- Monitor how web search results are combined with market data
- Observe the correlation between news sentiment and technical indicators
- Track how the system synthesizes diverse information sources

**Real-time Processing:**
- See live data fetching from both web sources and crypto exchanges
- Watch the real-time analysis and recommendation generation
- Observe how the system handles API rate limits and errors

#### 📊 **Analysis Output Example**

```
🔍 COMPREHENSIVE MARKET ANALYSIS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

📰 LATEST MARKET NEWS:
• Federal Reserve signals potential rate cut in Q4 2024
• Bitcoin ETF inflows reach record $2.1B this week
• Ethereum staking rewards hit 7.2% APY
• Major tech companies announce crypto payment integration

📊 TECHNICAL ANALYSIS:
• BTC/USDT: Breaking above $45K resistance, volume spike detected
• ETH/USDT: Testing $2,800 support level, RSI showing oversold conditions
• Market-wide momentum: Bullish divergence across major altcoins

🎯 INVESTMENT INSIGHTS:
• SHORT-TERM: Bullish momentum favors BTC accumulation
• MEDIUM-TERM: ETH showing strong fundamental support
• RISK FACTORS: Monitor Federal Reserve policy decisions

💡 MARKET SENTIMENT:
Overall market sentiment is cautiously optimistic with strong institutional...
```

#### 📁 **Source Code & Documentation**

- **GitHub Link**: [MCP Spoon Search Agent](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)
- **Related Files**:
  - `spoon-core/examples/mcp/spoon_search_agent.py` - Core MCP implementation
  - `spoon-core/spoon_ai/tools/mcp_tools.py` - MCP tool integration
  - `docs/core-concepts/mcp-protocol.md` - MCP protocol documentation

#### 🎓 **Learning Objectives**

This example teaches you:
- How to integrate MCP (Model Context Protocol) servers into your agents
- Advanced multi-tool orchestration and data synthesis techniques
- Real-time web search integration with LLM-powered analysis
- Error handling and recovery in distributed tool systems
- Building research assistants that combine multiple data sources

#### 💡 **Best Practices Demonstrated**

- **MCP Server Management** - Proper initialization and error handling for MCP servers
- **Tool Discovery** - Dynamic tool loading and validation
- **Data Correlation** - Effective synthesis of diverse information sources
- **API Rate Limiting** - Intelligent handling of API limitations and quotas
- **Fallback Mechanisms** - Robust error recovery when tools or APIs are unavailable

---

FILE: docs/examples/x402-react-agent.md

# x402 Agent

This walkthrough mirrors `core/examples/x402_agent_demo.py`, which shows a SpoonReact agent autonomously paying the official x402 paywall before summarising the protected content.

## Prerequisites

1. Install SpoonOS core dependencies (`uv pip install -r requirements.txt`).
2. Configure `core/.env` with:
   - `OPENAI_API_KEY`
   - `PRIVATE_KEY` (0x-prefixed; must hold ≥0.01 USDC). If omitted, set the Turnkey variables documented in the API reference.
   - `X402_RECEIVER_ADDRESS` (usually matches the private key address).
   - Optional: `X402_FACILITATOR_URL`, `X402_DEFAULT_NETWORK`, `X402_DEMO_URL`.
3. Acquire USDC Testnet Token (0.01 is enough) via [https://faucet.circle.com/](https://faucet.circle.com/).

## Run the demo

```bash
uv run python examples/x402_agent_demo.py
```

What happens:

1. The script prints signer details and the target resource (`https://www.x402.org/protected` by default).
2. A `SpoonReactAI` instance performs a ReAct loop:
   - Calls `http_probe` (no payment) to capture the 402 challenge.
   - Calls `x402_paywalled_request` to sign and submit a 0.01 USDC payment.
   - Retrieves the protected payload (a SoundCloud embed) after settlement.
3. The console logs tool traces, the signed `X-PAYMENT` header, and the decoded settlement receipt (transaction hash, payer, network).

## Troubleshooting

| Symptom | Likely cause | Fix |
| --- | --- | --- |
| `invalid_exact_evm_payload_signature` | Asset/network mismatch or stale nonce. | Ensure you copied the paywall's `asset` and `pay_to` fields, and confirm `PRIVATE_KEY` funds exist. |
| `Configuration error: API key is required for provider ...` | Missing `OPENAI_API_KEY`. | Export a valid LLM key before running the demo. |
| `x402 configuration error: Turnkey signing identity missing` | `X402_USE_TURNKEY` enabled but no `TURNKEY_SIGN_WITH`. | Provide the required Turnkey identifiers or disable Turnkey by setting `X402_USE_TURNKEY=0`. |

## Next steps

- Automate the payment header generation inside your own agents by reusing the `x402_paywalled_request` tool documented in the API reference.
- Expose your agents via the paywall router (`python -m spoon_ai.payments.app`) so external callers must submit verified x402 payments before invoking them.

---

FILE: docs/getting-started/configuration.md

# Configuration

SpoonOS is **env-first**. The core Python SDK only reads environment variables (including values from a `.env` file). The `spoon-cli` workflow is the only place `config.json` is read; the CLI loads that file and exports the values into the environment before starting agents.

## Configuration Priority

At runtime (latest wins):

1. Built-in defaults in the SDK  
2. Environment variables (`.env` or shell)  
3. Values exported by `spoon-cli` from `config.json` (CLI only)

## Environment Variables

Create a `.env` file in your project root:

```bash
# LLM Provider API Keys (set at least one)
GEMINI_API_KEY=your_gemini_key_here        # recommended for Quick Start
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
OPENROUTER_API_KEY=your_openrouter_key_here

# Optional: Default LLM Settings
DEFAULT_LLM_PROVIDER=gemini                # or openai / anthropic / deepseek / openrouter
DEFAULT_MODEL=gemini-2.5-pro
GEMINI_MAX_TOKENS=20000                    # recommended context limit for Gemini

# Web3 Configuration (only needed for on-chain tools)
WEB3_PROVIDER_URL=https://mainnet.infura.io/v3/your_project_id
PRIVATE_KEY=your_private_key_here
```

## CLI Configuration File (optional)

If you use `spoon-cli`, manage CLI-specific settings in `config.json`. The CLI exports that file into environment variables automatically; the SDK does **not** read it directly. See `docs/cli/configuration.md` for the full schema and commands.

## API Key Setup

### OpenAI
1. Visit [OpenAI API Keys](https://platform.openai.com/api-keys)
2. Create a new API key
3. Add to your `.env` file

### Anthropic
1. Visit [Anthropic Console](https://console.anthropic.com/)
2. Generate an API key
3. Add to your `.env` file

### Google (Gemini)
1. Visit [Google AI Studio](https://aistudio.google.com/app/apikey)
2. Create an API key
3. Add to your `.env` file

## Verification

Test your configuration:

```bash
python -c "from spoon_ai.utils.config_manager import ConfigManager; print('✅ Configuration loaded successfully')"
```

The framework automatically validates your configuration and provides helpful error messages if any issues are detected.

## Next Steps

- [Quick Start](./quick-start.md) - Build your first agent
- [Core Concepts](../core-concepts/agents.md) - Learn about agents

---

FILE: docs/getting-started/installation.md

# Installation

## Prerequisites

- Python 3.12 or higher
- Git
- Virtual environment (recommended)

## Quick Installation

### Fast path (uv — recommended)

`uv` gives faster, reproducible installs and works as a drop-in replacement for `pip`:

```bash
uv venv .venv
source .venv/bin/activate            # macOS/Linux
# .\\.venv\\Scripts\\Activate.ps1    # Windows (PowerShell)

# Install published packages
uv pip install spoon-ai-sdk          # core framework
uv pip install spoon-toolkits        # optional: extended blockchain & data toolkits
```

You can substitute `uv pip` with `pip` if you prefer the standard installer.

### Option A: Install from PyPI (recommended)

You can use the published PyPI packages without cloning the repository:

1. Create and activate a virtual environment

```bash
# macOS/Linux
python3 -m venv spoon-env
source spoon-env/bin/activate

# Windows (PowerShell)
python -m venv spoon-env
.\spoon-env\Scripts\Activate.ps1
```

2. Install the core SDK (and optionally the toolkits package)

```bash
pip install spoon-ai-sdk        # core framework
pip install spoon-toolkits      # optional: extended blockchain & data toolkits
```

### Option B: Use a local repository checkout

If you are working inside this monorepo (for example you already opened it in your IDE), you can install directly from the local folders without needing to `git clone` again.

1. Create Virtual Environment

```bash
# macOS/Linux
python3 -m venv spoon-env
source spoon-env/bin/activate

# Windows (PowerShell)
python -m venv spoon-env
.\spoon-env\Scripts\Activate.ps1
```

> 💡 On newer Apple Silicon Macs the `python` shim may not point to Python 3.
> Use `python3` for all commands unless you have explicitly configured `python`
> to target Python 3.12 or later.

2. Install core package in editable mode

```bash
git clone https://github.com/XSpoonAi/spoon-core.git
cd spoon-core
uv pip install -e .    # or `pip install -e .` if you don't use uv
```

3. (Optional) Install Toolkits Package from local repo

If you want to use the extended blockchain and data tools from `spoon_toolkits`, install the **spoon-toolkits** package from the `spoon-toolkits` folder:

```bash
git clone https://github.com/XSpoonAi/spoon-toolkits.git
cd spoon-toolkits
pip install -e .
```

## Framework Validation

The SpoonOS framework includes built-in validation that automatically:

- Checks API key configuration
- Validates provider connectivity
- Ensures proper dependency installation
- Provides clear error messages if issues are found

## Next Steps

- [Configuration](./configuration.md) - Set up API keys and configuration
- [Quick Start](./quick-start.md) - Build your first agent

---

FILE: docs/getting-started/quick-start.md

# Quick Start

Get up and running with SpoonOS framework in under 5 minutes.

## Prerequisites

- [Installation](./installation.md) completed
- [Configuration](./configuration.md) set up with at least one provider API key (for example `OPENAI_API_KEY`)

## Your First Agent

### 1. Create a Simple Agent

Create a new Python file `my_first_agent.py`:

```python
import asyncio
from spoon_ai.agents.toolcall import ToolCallAgent
from spoon_ai.chat import ChatBot
from spoon_ai.tools import ToolManager
from spoon_ai.tools.base import BaseTool

# Define a custom tool
class GreetingTool(BaseTool):
    name: str = "greeting"
    description: str = "Generate personalized greetings"
    parameters: dict = {
        "type": "object",
        "properties": {
            "name": {"type": "string", "description": "Person's name"}
        },
        "required": ["name"]
    }

    async def execute(self, name: str) -> str:
        return f"Hello {name}! Welcome to SpoonOS! 🚀"

# Create your agent
class MyFirstAgent(ToolCallAgent):
    name: str = "my_first_agent"
    description: str = "A friendly assistant with greeting capabilities"

    system_prompt: str = """
    You are a helpful AI assistant built with SpoonOS framework.
    You can greet users and help with various tasks.
    """

    available_tools: ToolManager = ToolManager([GreetingTool()])

async def main():
    # Initialize agent with LLM
    agent = MyFirstAgent(
        llm=ChatBot(
            llm_provider="openai",         # or "anthropic", "gemini", "deepseek", "openrouter"
            model_name="gpt-5.1"   # Framework default for OpenAI
        )
    )

    # Run the agent - framework handles all error cases automatically
    response = await agent.run("Please greet me, my name is Alice")
    return response

if __name__ == "__main__":
    result = asyncio.run(main())
    # Agent response will be returned directly
```

### 2. Run Your Agent

```bash
python my_first_agent.py
```

The agent will respond with a personalized greeting and offer to help with various tasks.

### 3. Add Web3 Capabilities

Enhance your agent with blockchain tools:

```python
from spoon_ai.tools.crypto_tools import get_crypto_tools

class Web3Agent(ToolCallAgent):
    name: str = "web3_agent"
    description: str = "AI agent with Web3 and crypto capabilities"

    system_prompt: str = """
    You are a Web3-native AI assistant with access to blockchain data.
    You can help with crypto prices, DeFi operations, and blockchain analysis.
    """

    available_tools: ToolManager = ToolManager([
        GreetingTool(),
        # Loads all crypto/Web3 tools from spoon-toolkits (requires `pip install -e spoon-toolkits`)
        *get_crypto_tools()
    ])

# Usage
async def web3_demo():
    agent = Web3Agent(
        llm=ChatBot(
            llm_provider="anthropic",
            model_name="claude-sonnet-4-20250514"  # Framework default
        )
    )

    # Framework automatically handles crypto data fetching and error cases
    response = await agent.run("What's the current price of Bitcoin?")
    return response
```

### 4. Framework Features Overview

The SpoonOS framework provides:

- **Multiple LLM Providers**: OpenAI (`openai`), Anthropic (`anthropic`), Google Gemini (`gemini`), DeepSeek (`deepseek`), OpenRouter (`openrouter`)
- **Built-in Tools**: Crypto, DeFi, social media, data analysis
- **Agent Types**: ReAct, ToolCall, Graph-based agents
- **MCP Integration**: Dynamic tool discovery and execution

### Framework Simplicity

SpoonOS eliminates common development complexity:

```python
# Simple agent creation - no error handling needed
agent = ToolCallAgent(
    llm=ChatBot(llm_provider="openai", model_name="gpt-4.1"),
    available_tools=ToolManager([CryptoTool(), Web3Tool()])
)


response = await agent.run("Analyze Bitcoin trends and suggest trades")
```

## Framework Development Patterns

### Agent Composition

```python
# Combine multiple agents for complex workflows
from spoon_ai.agents.graph import GraphAgent

class MultiAgentSystem(GraphAgent):
    def __init__(self):
        super().__init__()
        self.add_agent("researcher", ResearchAgent())
        self.add_agent("analyst", AnalysisAgent())
        self.add_agent("trader", TradingAgent())
```

### Custom Tool Development

```python
# Create domain-specific tools
class BlockchainAnalysisTool(BaseTool):
    name: str = "blockchain_analysis"
    description: str = "Analyze blockchain transactions and patterns"

    async def execute(self, address: str, chain: str = "ethereum") -> str:
        # Your custom blockchain analysis logic
        return f"Analysis results for {address} on {chain}"
```

### MCP Integration

```python
# Use Model Context Protocol for dynamic tools
from spoon_ai.tools.mcp_tools_collection import MCPToolsCollection

agent = ToolCallAgent(
    llm=ChatBot(llm_provider="anthropic", model_name="claude-sonnet-4-20250514"),
    available_tools=ToolManager([
        MCPToolsCollection()  # Automatically discovers MCP tools
    ])
)
```

## Next Steps

Now that you understand the framework basics:

- [Core Concepts](../core-concepts/agents.md) - Deep dive into agent architecture
- [Built-in Tools](../core-concepts/tools.md) - Explore Web3 and crypto tools
- [How-To Guides](../how-to-guides/build-first-agent.md) - Advanced agent patterns

---

FILE: docs/how-to-guides/add-custom-tools.md

# Adding Custom Tools to SpoonOS

This guide shows you how to create and integrate custom tools into the SpoonOS framework. Tools extend agent capabilities by providing specific functionality like API integrations, data processing, or blockchain interactions.

## Tool Architecture Overview

SpoonOS uses a modular tool system where:

- **BaseTool**: Abstract base class defining the tool interface
- **ToolManager**: Manages tool collections and execution
- **MCP Integration**: Exposes tools via Model Context Protocol
- **Dynamic Loading**: Tools can be added at runtime

## Creating a Basic Tool

### Step 1: Define Your Tool Class

Create a new tool by inheriting from `BaseTool`:

```python
from spoon_ai.tools.base import BaseTool, ToolResult
from typing import Any, Dict

class MyCustomTool(BaseTool):
    name: str = "my_custom_tool"
    description: str = "A custom tool that processes data"
    parameters: dict = {
        "type": "object",
        "properties": {
            "input_data": {
                "type": "string",
                "description": "The data to process"
            },
            "options": {
                "type": "object",
                "description": "Optional processing parameters",
                "properties": {
                    "format": {"type": "string", "default": "json"}
                }
            }
        },
        "required": ["input_data"]
    }

    async def execute(self, input_data: str, options: Dict[str, Any] = None) -> ToolResult:
        """Execute the tool logic - framework handles errors automatically"""
        # Your tool logic here
        processed_data = self.process_data(input_data, options or {})

        return ToolResult(
            output=processed_data,
            system=f"Successfully processed {len(input_data)} characters"
        )

    def process_data(self, data: str, options: Dict[str, Any]) -> str:
        """Your custom processing logic"""
        # Example: simple data transformation
        format_type = options.get("format", "json")
        if format_type == "uppercase":
            return data.upper()
        return f'{{"processed": "{data}"}}'
```

### Step 2: Tool Parameters Schema

The `parameters` field defines the JSON schema for tool inputs:

```python
parameters: dict = {
    "type": "object",
    "properties": {
        "required_param": {
            "type": "string",
            "description": "A required parameter"
        },
        "optional_param": {
            "type": "integer",
            "description": "An optional parameter",
            "default": 42
        },
        "enum_param": {
            "type": "string",
            "enum": ["option1", "option2", "option3"],
            "description": "Choose from predefined options"
        }
    },
    "required": ["required_param"]
}
```

## Advanced Tool Examples

### API Integration Tool

```python
import aiohttp
from spoon_ai.tools.base import BaseTool, ToolResult

class APITool(BaseTool):
    name: str = "api_fetcher"
    description: str = "Fetches data from external APIs"
    parameters: dict = {
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "API endpoint URL"},
            "method": {"type": "string", "enum": ["GET", "POST"], "default": "GET"},
            "headers": {"type": "object", "description": "HTTP headers"}
        },
        "required": ["url"]
    }

    async def execute(self, url: str, method: str = "GET", headers: dict = None) -> ToolResult:
        # Framework provides automatic error handling and retry logic
        async with aiohttp.ClientSession() as session:
            async with session.request(method, url, headers=headers) as response:
                data = await response.json()
                return ToolResult(
                    output=data,
                    system=f"API call successful: {response.status}"
                )
```

### Blockchain Tool Example

```python
from web3 import Web3
from spoon_ai.tools.base import BaseTool, ToolResult

class BlockchainTool(BaseTool):
    name: str = "get_eth_balance"
    description: str = "Gets Ethereum balance for an address"
    parameters: dict = {
        "type": "object",
        "properties": {
            "address": {
                "type": "string",
                "description": "Ethereum address to check"
            },
            "network": {
                "type": "string",
                "enum": ["mainnet", "goerli", "sepolia"],
                "default": "mainnet"
            }
        },
        "required": ["address"]
    }

    def __init__(self):
        super().__init__()
        self.w3 = Web3(Web3.HTTPProvider("https://eth-mainnet.alchemyapi.io/v2/YOUR_KEY"))

    async def execute(self, address: str, network: str = "mainnet") -> ToolResult:
        # Framework handles validation and error cases automatically
        if not self.w3.is_address(address):
            return ToolResult(error="Invalid Ethereum address")

        balance_wei = self.w3.eth.get_balance(address)
        balance_eth = self.w3.from_wei(balance_wei, 'ether')

        return ToolResult(
            output={
                "address": address,
                "balance_eth": str(balance_eth),
                "balance_wei": str(balance_wei),
                "network": network
            }
        )
```

## Integrating Tools

### Method 1: Add to Tool Manager

```python
from spoon_ai.tools.tool_manager import ToolManager
from your_module import MyCustomTool

# Create tool manager with existing tools
tool_manager = ToolManager([])

# Add your custom tool
custom_tool = MyCustomTool()
tool_manager.add_tool(custom_tool)

# Or add multiple tools at once
tool_manager.add_tools(
    MyCustomTool(),
    APITool(),
    BlockchainTool()
)
```

### Method 2: Create Tool Collection

```python
# tools/my_tools.py
from typing import List
from spoon_ai.tools.base import BaseTool
from .my_custom_tool import MyCustomTool
from .api_tool import APITool

def get_my_tools() -> List[BaseTool]:
    """Return collection of custom tools"""
    return [
        MyCustomTool(),
        APITool(),
        # Add more tools here
    ]

def create_my_tool_manager() -> ToolManager:
    """Create tool manager with custom tools"""
    from spoon_ai.tools.tool_manager import ToolManager
    return ToolManager(get_my_tools())
```

### Method 3: MCP Integration

```python
# mcp_server.py
from fastmcp import FastMCP
from spoon_ai.tools.tool_manager import ToolManager
from your_tools import get_my_tools

mcp = FastMCP("My Custom Tools")

# Add tools to MCP server
tools = get_my_tools()
tool_manager = ToolManager(tools)

for tool in tools:
    mcp.add_tool(
        tool.execute,
        name=tool.name,
        description=tool.description
    )

if __name__ == "__main__":
    import asyncio
    asyncio.run(mcp.run_async(transport="sse", port=8766))
```

## Tool Configuration

### Environment Variables

```python
import os
from spoon_ai.tools.base import BaseTool, ToolResult

class ConfigurableTool(BaseTool):
    name: str = "configurable_tool"
    description: str = "Tool that uses environment configuration"

    def __init__(self):
        super().__init__()
        self.api_key = os.getenv("MY_API_KEY")
        self.base_url = os.getenv("MY_API_URL", "https://api.example.com")

        if not self.api_key:
            raise ValueError("MY_API_KEY environment variable required")

    async def execute(self, query: str) -> ToolResult:
        # Use self.api_key and self.base_url
        pass
```

### Configuration Class

```python
from pydantic import BaseModel
from typing import Optional

class ToolConfig(BaseModel):
    api_key: str
    base_url: str = "https://api.example.com"
    timeout: int = 30
    retries: int = 3

class ConfigurableTool(BaseTool):
    def __init__(self, config: ToolConfig):
        super().__init__()
        self.config = config

    async def execute(self, **kwargs) -> ToolResult:
        # Use self.config.api_key, etc.
        pass
```

## Error Handling Best Practices

### Framework Error Handling

```python
async def execute(self, **kwargs) -> ToolResult:
    # Framework provides automatic input validation and error handling
    if not kwargs.get("required_param"):
        return ToolResult(error="Missing required parameter")

    # Execute tool logic - framework handles network errors, timeouts, etc.
    result = await self.do_work(**kwargs)

    return ToolResult(
        output=result,
        system="Operation completed successfully"
    )
```

### Framework Monitoring

```python
from spoon_ai.tools.base import BaseTool, ToolResult

class MonitoredTool(BaseTool):
    async def execute(self, **kwargs) -> ToolResult:
        # Framework provides automatic logging and monitoring
        result = await self.do_work(**kwargs)

        # Framework tracks:
        # - Execution time and performance metrics
        # - Success/failure rates
        # - Parameter usage patterns
        # - Error frequencies and types
        return ToolResult(output=result)
```

## Testing Your Tools

### Unit Testing

```python
import pytest
from your_tools import MyCustomTool

@pytest.mark.asyncio
async def test_my_custom_tool():
    tool = MyCustomTool()

    # Test successful execution
    result = await tool.execute(input_data="test data")
    assert result.output is not None
    assert result.error is None

    # Test error handling
    result = await tool.execute(input_data="")
    assert result.error is not None

@pytest.mark.asyncio
async def test_tool_parameters():
    tool = MyCustomTool()

    # Test with optional parameters
    result = await tool.execute(
        input_data="test",
        options={"format": "uppercase"}
    )
    assert "TEST" in result.output
```

### Integration Testing

```python
from spoon_ai.tools.tool_manager import ToolManager
from your_tools import MyCustomTool

@pytest.mark.asyncio
async def test_tool_manager_integration():
    tool_manager = ToolManager([MyCustomTool()])

    # Test tool execution through manager
    result = await tool_manager.execute(
        name="my_custom_tool",
        tool_input={"input_data": "test"}
    )

    assert result.output is not None
```

## Tool Discovery and Documentation

### Auto-generating Tool Docs

```python
def generate_tool_docs(tools: List[BaseTool]) -> str:
    """Generate markdown documentation for tools"""
    docs = "# Available Tools

"

    for tool in tools:
        docs += f"## {tool.name}

"
        docs += f"{tool.description}

"
        docs += "### Parameters

"

        for param, config in tool.parameters.get("properties", {}).items():
            required = param in tool.parameters.get("required", [])
            docs += f"- **{param}** ({'required' if required else 'optional'}): {config.get('description', '')}
"

        docs += "
"

    return docs
```

### Tool Registry

```python
class ToolRegistry:
    """Central registry for tool discovery"""

    def __init__(self):
        self._tools = {}

    def register(self, tool_class: type):
        """Register a tool class"""
        tool = tool_class()
        self._tools[tool.name] = tool_class
        return tool_class

    def get_tool(self, name: str) -> BaseTool:
        """Get tool instance by name"""
        if name not in self._tools:
            raise ValueError(f"Tool {name} not found")
        return self._tools[name]()

    def list_tools(self) -> List[str]:
        """List all registered tool names"""
        return list(self._tools.keys())

# Usage
registry = ToolRegistry()

@registry.register
class MyTool(BaseTool):
    # Tool implementation
    pass
```

## Best Practices

### 1. Tool Naming

- Use descriptive, action-oriented names
- Follow snake_case convention
- Avoid generic names like "tool" or "helper"

### 2. Parameter Design

- Provide clear descriptions for all parameters
- Use appropriate data types and validation
- Set sensible defaults for optional parameters

### 3. Error Messages

- Be specific about what went wrong
- Include suggestions for fixing issues
- Don't expose sensitive information in errors

### 4. Performance

- Use async/await for I/O operations
- Leverage framework's built-in timeout handling
- Cache results when appropriate

### 5. Security

- Validate all inputs thoroughly
- Use environment variables for secrets
- Rely on framework's rate limiting features

## Next Steps

### 📚 **Custom Tool Examples**

#### 🔍 [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**Custom tool integration demonstrated:**
- MCP server integration with custom search tools
- Web search capabilities using Tavily MCP
- Custom error handling for external API calls
- Real-world custom tool deployment patterns

**Key learning points:**
- How to wrap external APIs as custom tools
- MCP server integration patterns
- Error handling for unreliable external services
- Tool validation and testing strategies

#### 📊 [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Financial tool development:**
- Custom cryptocurrency data processing tools
- Real-time technical indicator calculations
- Multi-source data aggregation and validation
- Financial data error handling and recovery

**Key learning points:**
- Domain-specific tool development patterns
- Financial data validation techniques
- Multi-API integration strategies
- Performance optimization for data-intensive tools

#### 🎯 [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Advanced tool orchestration:**
- Custom routing and decision-making tools
- Memory management and context preservation tools
- Parallel processing coordination tools
- Performance monitoring and metrics tools

**Key learning points:**
- Complex tool interaction patterns
- State management in custom tools
- Performance optimization techniques
- Error recovery in multi-tool workflows

### 🛠️ **Development Resources**

- **[Core Concepts: Tools](../core-concepts/tools.md)** - Complete tool system understanding
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Advanced integration patterns
- **[Tool API Reference](../api-reference/spoon_ai/tools/)** - Complete development documentation

### 📖 **Additional Resources**

- **[Built-in Tools Reference](../api-reference/spoon_ai/tools/)** - Explore existing tool implementations
- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Tool-agent integration patterns

## Troubleshooting

### Common Issues

**Tool not found in manager:**

- Ensure tool is properly added to ToolManager
- Check tool name matches exactly
- Verify tool class inherits from BaseTool

**Parameter validation errors:**

- Check JSON schema syntax in parameters
- Ensure required parameters are marked correctly
- Validate parameter types match schema

**Execution failures:**

- Leverage framework's automatic error handling
- Check for missing dependencies or API keys
- Use framework's built-in debugging features
**Key learning points:**
- Domain-specific tool development patterns
- Financial data validation techniques
- Multi-API integration strategies
- Performance optimization for data-intensive tools

#### 🎯 [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Advanced tool orchestration:**
- Custom routing and decision-making tools
- Memory management and context preservation tools
- Parallel processing coordination tools
- Performance monitoring and metrics tools

**Key learning points:**
- Complex tool interaction patterns
- State management in custom tools
- Performance optimization techniques
- Error recovery in multi-tool workflows

### 🛠️ **Development Resources**

- **[Core Concepts: Tools](../core-concepts/tools.md)** - Complete tool system understanding
- **[MCP Protocol](../core-concepts/mcp-protocol.md)** - Advanced integration patterns
- **[Tool API Reference](../api-reference/spoon_ai/tools/)** - Complete development documentation

### 📖 **Additional Resources**

- **[Built-in Tools Reference](../api-reference/spoon_ai/tools/)** - Explore existing tool implementations
- **[Graph System](../core-concepts/graph-system.md)** - Advanced workflow orchestration
- **[Agent Architecture](../core-concepts/agents.md)** - Tool-agent integration patterns

## Troubleshooting

### Common Issues

**Tool not found in manager:**

- Ensure tool is properly added to ToolManager
- Check tool name matches exactly
- Verify tool class inherits from BaseTool

**Parameter validation errors:**

- Check JSON schema syntax in parameters
- Ensure required parameters are marked correctly
- Validate parameter types match schema

**Execution failures:**

- Leverage framework's automatic error handling
- Check for missing dependencies or API keys
- Use framework's built-in debugging features

---

FILE: docs/how-to-guides/build-first-agent.md

# Build Your First Agent

Learn how to create a custom AI agent from scratch using SpoonOS.

## Prerequisites

- SpoonOS installed and configured
- API keys set up for your chosen LLM provider
- Basic Python knowledge

### Fast environment setup (uv)

If you haven’t installed the SDK yet, `uv` is the quickest way:

```bash
uv venv .venv
source .venv/bin/activate            # macOS/Linux
# .\\.venv\\Scripts\\Activate.ps1    # Windows (PowerShell)
uv pip install spoon-ai-sdk          # core SDK
uv pip install spoon-toolkits        # optional Web3/crypto tools
```

## Step 1: Basic Agent Setup

### Create Agent File

Create a new file `my_first_agent.py` (works with Gemini, OpenAI, or any configured provider):

```python
import os
from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools.crypto_tools import get_crypto_tools

# Create your first agent
def create_agent():
    # Configure LLM
    llm = ChatBot(
        # Pick up provider/model from env to support Gemini out of the box.
        # Example: set DEFAULT_LLM_PROVIDER=gemini and GEMINI_API_KEY=***
        llm_provider=os.getenv("LLM_PROVIDER") or os.getenv("DEFAULT_LLM_PROVIDER") or "gemini",
        model_name=os.getenv("LLM_MODEL") or "gemini-2.5-pro",
        temperature=0.3
    )

    # Create agent with tools
    agent = SpoonReactAI(
        llm=llm,
        tools=[*get_crypto_tools()]  # requires `pip install -e toolkit`
    )

    return agent

# Test the agent
async def main():
    agent = create_agent()

    # Framework handles all errors automatically
    response = await agent.run("Hello! What can you help me with?")
    response = await agent.run("What's the current price of Bitcoin?")

    return response

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### Run Your Agent

```bash
python my_first_agent.py
```

Your agent will respond with helpful information and current Bitcoin price data.

## Step 2: Add Custom Functionality

### Create Custom Tool

```python
from spoon_ai.tools.base import BaseTool
from typing import Dict, Any

class GreetingTool(BaseTool):
    name: str = "greeting_tool"
    description: str = "Generate personalized greetings"
    parameters: dict = {
        "type": "object",
        "properties": {
            "name": {"type": "string", "description": "Person's name"},
            "style": {"type": "string", "description": "Greeting style (formal/casual)"}
        },
        "required": ["name"]
    }

    async def execute(self, name: str, style: str = "casual") -> str:
        if style == "formal":
            return f"Good day, {name}. It's a pleasure to meet you."
        else:
            return f"Hey {name}! Nice to meet you! 👋"
```

### Enhanced Agent with Custom Tool

```python
def create_enhanced_agent():
    import os
    llm = ChatBot(
        llm_provider=os.getenv("LLM_PROVIDER") or os.getenv("DEFAULT_LLM_PROVIDER") or "gemini",
        model_name=os.getenv("LLM_MODEL") or "gemini-2.5-pro",
        temperature=0.3,
        enable_short_term_memory=True,
        short_term_memory_config={
            "max_tokens": 8000,
            "strategy": "summarize",
            "messages_to_keep": 6,
        },
    )

    # Add multiple tools
    agent = SpoonReactAI(
        llm=llm,
        tools=[
            *get_crypto_tools(),
            GreetingTool()
        ]
    )

    return agent

# Run enhanced agent (same entry style as Step 1)
async def main_enhanced():
    agent = create_enhanced_agent()

    # Framework automatically handles tool selection and execution
    response = await agent.run("Give me a formal greeting for John")
    response = await agent.run("Greet Alice casually and then tell her the Bitcoin price")

    return response

if __name__ == "__main__":
    import asyncio
    asyncio.run(main_enhanced())

### Optional: Stream responses

If you want token-by-token output (works with any supported provider):

```python
async def stream_demo():
    llm = ChatBot(model_name="gpt-4.1", llm_provider="openai")
    messages = [{"role": "user", "content": "Stream a 3-step plan to learn SpoonOS"}]
    async for chunk in llm.astream(messages=messages):
        print(chunk.delta or "", end="", flush=True)
```

## Step 3: Add Memory and Context

> Tip: Short-term memory trimming/summarization is already enabled in `create_enhanced_agent` via `enable_short_term_memory=True`. Use a higher `max_tokens` or switch `strategy` to `"trim"` if you prefer dropping history instead of summarizing.

### Agent with Memory

```python
class MemoryAgent:
    def __init__(self):
        self.agent = create_enhanced_agent()
        self.conversation_history = []

    async def chat(self, message: str) -> str:
        # Add context from previous conversations
        context = self.build_context()
        full_message = f"{context}

User: {message}"

        # Get response
        response = await self.agent.run(full_message)

        # Store in memory
        self.conversation_history.append({
            "user": message,
            "agent": response,
            "timestamp": time.time()
        })

        return response

    def build_context(self) -> str:
        if not self.conversation_history:
            return "This is the start of our conversation."

        # Include last 3 exchanges for context
        recent = self.conversation_history[-3:]
        context_parts = []

        for exchange in recent:
            context_parts.append(f"User: {exchange['user']}")
            context_parts.append(f"Agent: {exchange['agent']}")

        return "Previous conversation:" + "".join(context_parts)

# Test memory functionality
async def test_memory_agent():
    agent = MemoryAgent()

    # Framework maintains conversation context automatically
    response1 = await agent.chat("My name is Sarah")
    response2 = await agent.chat("What's my name?")  # Agent remembers Sarah

    return response1, response2
```

## Step 4: Framework Error Handling

### Built-in Robustness

SpoonOS provides automatic error handling and robustness features:

```python
class SimpleAgent:
    def __init__(self):
        # Framework handles all error cases automatically
        self.agent = create_enhanced_agent()

    async def run(self, message: str) -> str:
        # Framework provides:
        # - Automatic retry with exponential backoff
        # - Provider fallback (OpenAI -> Anthropic -> Google)
        # - Tool error recovery with graceful degradation
        # - Timeout handling with configurable limits
        return await self.agent.run(message)

# Simple usage - no error handling needed
async def test_agent():
    agent = SimpleAgent()

    # Framework handles all error scenarios automatically
    response = await agent.run("Hello!")
    response = await agent.run("Perform a complex analysis")

    return response
```

## Step 5: Configuration and Deployment

### Configurable Agent

```python
import json
from pathlib import Path

class ConfigurableAgent:
    def __init__(self, config_path: str = "agent_config.json"):
        self.config = self.load_config(config_path)
        self.agent = self.create_agent_from_config()

    def load_config(self, config_path: str) -> dict:
        config_file = Path(config_path)
        if config_file.exists():
            with open(config_file, 'r') as f:
                return json.load(f)
        else:
            # Default configuration
            default_config = {
                "llm": {
                    "provider": "gemini",
                    "model": "gemini-2.5-pro",
                    "temperature": 0.3
                },
                "tools": ["crypto_tools", "greeting_tool"],
                "memory": {
                    "enabled": True,
                    "max_history": 10
                }
            }
            # Save default config
            with open(config_file, 'w') as f:
                json.dump(default_config, f, indent=2)
            return default_config

    def create_agent_from_config(self):
        # Create LLM from config
        llm_config = self.config["llm"]
        llm = ChatBot(
            model_name=llm_config["model"],
            llm_provider=llm_config["provider"],
            temperature=llm_config["temperature"]
        )

        # Create tools from config
        tools = []
        if "crypto_tools" in self.config["tools"]:
            tools.extend(get_crypto_tools())
        if "greeting_tool" in self.config["tools"]:
            tools.append(GreetingTool())

        return SpoonReactAI(llm=llm, tools=tools)

    async def run(self, message: str) -> str:
        return await self.agent.run(message)

# Example configuration file (agent_config.json)
example_config = {
    "llm": {
        "provider": "anthropic",
        "model": "claude-sonnet-4-20250514",
        "temperature": 0.1
    },
    "tools": ["crypto_tools", "greeting_tool"],
    "memory": {
        "enabled": True,
        "max_history": 5
    }
}
```

## Step 6: Testing Your Agent

### Unit Tests

```python
import pytest
from unittest.mock import AsyncMock, patch

class TestMyAgent:
    @pytest.fixture
    async def agent(self):
        return create_enhanced_agent()

    @pytest.mark.asyncio
    async def test_basic_greeting(self, agent):
        with patch.object(agent, 'run', new_callable=AsyncMock) as mock_run:
            mock_run.return_value = "Hello! How can I help you?"

            response = await agent.run("Hello")
            assert "Hello" in response
            mock_run.assert_called_once_with("Hello")

    @pytest.mark.asyncio
    async def test_crypto_tool_integration(self, agent):
        # Test that crypto tools are available
        tool_names = [tool.name for tool in agent.tools]
        assert "get_price" in tool_names or any("crypto" in name.lower() for name in tool_names)

# Run tests
# pytest test_my_agent.py -v
```

### Integration Tests

```python
async def integration_test():
    """Test complete agent workflow"""
    agent = create_enhanced_agent()

    # Framework provides built-in validation and testing
    response1 = await agent.run("Hello")
    response2 = await agent.run("What's the Bitcoin price?")
    response3 = await agent.run("Give me a casual greeting for Alice")

    # Framework automatically validates responses and tool execution
    return all([response1, response2, response3])

# Run integration tests
if __name__ == "__main__":
    result = asyncio.run(integration_test())
```

## Complete Example

Here's the complete, production-ready agent:

```python
import asyncio
import json
import logging
import time
from pathlib import Path
from typing import List, Dict, Any

from spoon_ai.agents import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools.crypto_tools import get_crypto_tools
from spoon_ai.tools.base import BaseTool

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GreetingTool(BaseTool):
    name: str = "greeting_tool"
    description: str = "Generate personalized greetings"
    parameters: dict = {
        "type": "object",
        "properties": {
            "name": {"type": "string", "description": "Person's name"},
            "style": {"type": "string", "description": "Greeting style (formal/casual)"}
        },
        "required": ["name"]
    }

    async def execute(self, name: str, style: str = "casual") -> str:
        if style == "formal":
            return f"Good day, {name}. It's a pleasure to meet you."
        else:
            return f"Hey {name}! Nice to meet you! 👋"

class ProductionAgent:
    def __init__(self, config_path: str = "agent_config.json"):
        self.config = self.load_config(config_path)
        self.agent = self.create_agent()
        self.conversation_history = []
        self.logger = logging.getLogger(self.__class__.__name__)

    def load_config(self, config_path: str) -> dict:
        config_file = Path(config_path)
        if config_file.exists():
            with open(config_file, 'r') as f:
                return json.load(f)

        # Default configuration
        default_config = {
            "llm": {
                "provider": "openai",
                "model": "gpt-4.1",
                "temperature": 0.3
            },
            "tools": ["crypto_tools", "greeting_tool"],
            "memory": {"enabled": True, "max_history": 10},
            "retry_attempts": 3,
            "timeout": 30
        }

        with open(config_file, 'w') as f:
            json.dump(default_config, f, indent=2)

        return default_config

    def create_agent(self):
        llm_config = self.config["llm"]
        llm = ChatBot(
            model_name=llm_config["model"],
            llm_provider=llm_config["provider"],
            temperature=llm_config["temperature"]
        )

        tools = []
        if "crypto_tools" in self.config["tools"]:
            tools.extend(get_crypto_tools())
        if "greeting_tool" in self.config["tools"]:
            tools.append(GreetingTool())

        return SpoonReactAI(llm=llm, tools=tools)

    async def chat(self, message: str) -> str:
        # Framework handles timeouts and errors automatically
        if self.config["memory"]["enabled"]:
            context = self.build_context()
            full_message = f"{context}\n\nUser: {message}"
        else:
            full_message = message

        # Framework provides automatic timeout and error handling
        response = await self.agent.run(full_message)

        # Store in memory
        if self.config["memory"]["enabled"]:
            self.store_conversation(message, response)

        return response

    def build_context(self) -> str:
        if not self.conversation_history:
            return "This is the start of our conversation."

        max_history = self.config["memory"]["max_history"]
        recent = self.conversation_history[-max_history:]

        context_parts = ["Previous conversation:"]
        for exchange in recent:
            context_parts.append(f"User: {exchange['user']}")
            context_parts.append(f"Agent: {exchange['agent']}")

        return "
".join(context_parts)

    def store_conversation(self, user_message: str, agent_response: str):
        self.conversation_history.append({
            "user": user_message,
            "agent": agent_response,
            "timestamp": time.time()
        })

        # Limit history size
        max_history = self.config["memory"]["max_history"]
        if len(self.conversation_history) > max_history:
            self.conversation_history = self.conversation_history[-max_history:]

# Usage example
async def main():
    agent = ProductionAgent()

    # Simple chat interface - framework handles all complexity
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit', 'bye']:
            break

        response = await agent.chat(user_input)
        # Response is automatically formatted and error-free

if __name__ == "__main__":
    asyncio.run(main())
```

## Next Steps

Now that you've built your first agent, explore these advanced topics:

- [Add Custom Tools](./add-custom-tools.md) - Create specialized tools

---

FILE: docs/how-to-guides/vibe-coding.md

# Vibe Coding / IDE AI Guide

Give Cursor, Codex, Claude Code, and similar assistants the right context to generate accurate XSpoonAi code. Pick **one** of the four methods below; they are independent, ordered from most guided to most manual.

## Method 1: Use MCP / online retrieval (no local clone)

- Configure MCP connectors (e.g., deepwiki/context7) to fetch files from GitHub or the web.
- Target upstream repos directly: `XSpoonAi/spoon-core` and `XSpoonAi/toolkit` (source + examples) plus this Cookbook site for docs.
- Instruct the assistant to: list relevant files, fetch the exact file contents it will mimic, then cite paths/lines. No need to mirror or clone locally.

## Method 2: Supply the bundled docs file (`cookbook/llm.txt`)

- The repo ships an auto-generated bundle of every Markdown doc at `cookbook/llm.txt` (CI keeps it fresh).
- Share that single file with your LLM to give it the full Cookbook context for Vibe Coding.
- GitHub copy (fallback): [`llm.txt`](https://github.com/XSpoonAi/xspoonai.github.io/blob/main/llm.txt).

## Method 3: Point to installed package paths

- If you installed editable/local packages, expose their locations instead of the repo:
  ```bash
  python -c "import importlib.util, os; spec = importlib.util.find_spec('spoon_ai'); print('spoon_ai:', os.path.dirname(spec.origin) if spec and spec.origin else 'not installed')"
  python -c "import importlib.util, os; spec = importlib.util.find_spec('spoon_toolkits'); print('spoon_toolkits:', os.path.dirname(spec.origin) if spec and spec.origin else 'not installed')"
  ```
- Provide those paths to the assistant as read-only references so it can scan the actual shipped code and examples inside the installed packages.
- If a package isn’t installed (output shows “not installed”), just share the corresponding repo directory (e.g., `core/spoon_ai` or `toolkit/spoon_toolkits`) instead.

## Method 4: Share the workspace directly

- Allow the assistant to read this repo (especially `core/`, `toolkit/`, `cookbook/docs/`). No extra cloning required.
- Need a local checkout first? From your working directory run:
  ```bash
  git clone https://github.com/XSpoonAi/spoon-core.git
  git clone https://github.com/XSpoonAi/spoon-toolkit.git
  ```
- Keep the repo up to date (`git pull`) so the assistant always sees current code and docs.
- Ask it to open real source first, e.g. `core/spoon_ai/**` and runnable samples in `core/examples/` and `toolkit/**/examples`.
- Tell the AI to derive function signatures and configs from code files or examples, not invented abstractions.

## Hallucination-reduction tips (tell the AI)

- "Read the source or examples you will copy from; cite file path and line range before coding."
- "Confirm tool/agent/LLM signatures from code, not from guesswork; show the imports you plan to use."
- "If unsure, fetch the specific file via MCP and restate the interface before implementing."

---

FILE: docs/how-to-guides/x402-payments.md

# x402 Integration Guide

The x402 documentation is now split by audience so you can jump directly to the right level of detail.

## 1. Core concepts

Read [Core Concepts: x402 payments](../core-concepts/x402-payments.md) to understand:

- Why SpoonOS uses x402 to gate agent actions.
- Required environment variables and configuration fallbacks.
- The end-to-end payment lifecycle (probe -> sign -> retry -> settle) plus the flow diagram.

## 2. API reference

Consult [API Reference: x402](../api-reference/spoon_ai/payments/) for:

- Python services (`X402PaymentService`, request/response models, helper methods).
- Built-in tools (`x402_create_payment`, `x402_paywalled_request`) with parameter tables.
- CLI commands (`requirements`, `sign`) and the FastAPI paywall endpoints.
- Environment and Turnkey integration notes.

## 3. Example

Follow [Example: x402 ReAct agent](../examples/x402-react-agent.md) to run the ReAct demo (`uv run python examples/x402_agent_demo.py`). It walks through:

- Preparing `.env` and funding the signer.
- Observing the agent call `http_probe`, then `x402_paywalled_request`, retrieve the protected page, and print the signed `X-PAYMENT` + settlement receipt.
- Troubleshooting common facilitator or configuration errors.

## Quick checklist

- Fund the signer via [Circle faucet](https://faucet.circle.com/).
- Keep `PRIVATE_KEY` populated; rely on Turnkey only when the local key is absent.
- Reuse `X402PaymentService.decode_payment_response()` anywhere you archive facilitator receipts.

---

FILE: docs/troubleshooting/common-issues.md

# Common Issues and Solutions

This guide covers the most frequently encountered issues when working with SpoonOS and their solutions.

## Installation Issues

### Python Version Compatibility

**Problem:** `ImportError` or `ModuleNotFoundError` when importing SpoonOS modules

**Symptoms:**
```bash
ImportError: No module named 'spoon_ai'
ModuleNotFoundError: No module named 'asyncio'
```

**Solution:**
1. Ensure Python 3.12+ is installed:
   ```bash
   python --version
   # Should show Python 3.12.0 or higher
   ```

2. Create a new virtual environment:
   ```bash
   python -m venv spoon-env
   source spoon-env/bin/activate  # Linux/macOS
   # or
   spoon-env\\Scripts\\activate     # Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Dependency Conflicts

**Problem:** Package version conflicts during installation

**Symptoms:**
```bash
ERROR: pip's dependency resolver does not currently have a solution
Conflicting dependencies: package-a requires package-b>=2.0, but package-c requires package-b<2.0
```

**Solution:**
1. Use a fresh virtual environment
2. Install packages one by one to identify conflicts
3. Check `requirements.txt` for version pinning issues
4. Use `pip install --upgrade` for outdated packages

## Configuration Issues

### API Key Problems

**Problem:** Authentication errors with LLM providers

**Symptoms:**
```bash
AuthenticationError: Invalid API key
Unauthorized: API key not found
```

**Solution:**
1. Verify API keys are set correctly:
   ```bash
   echo $OPENAI_API_KEY
   echo $ANTHROPIC_API_KEY
   ```

2. Check `.env` file format:
   ```bash
   # Correct format
   OPENAI_API_KEY=sk-your-actual-key-here

   # Incorrect (no quotes needed)
   OPENAI_API_KEY="sk-your-actual-key-here"
   ```

3. Validate API key format:
   - OpenAI: Starts with `sk-`
   - Anthropic: Starts with `sk-ant-`
   - Google: 39-character string

4. Test API key validity:
   ```bash
   curl -H "Authorization: Bearer $OPENAI_API_KEY" \\
        https://api.openai.com/v1/models
   ```

### Configuration File Errors

**Problem:** JSON parsing errors in `config.json`

**Symptoms:**
```bash
JSONDecodeError: Expecting ',' delimiter
ConfigurationError: Invalid configuration format
```

**Solution:**
1. Validate JSON syntax using online validator or:
   ```bash
   python -m json.tool config.json
   ```

2. Common JSON errors:
   ```json
   // Wrong: trailing comma
   {
     "key": "value",
   }

   // Correct
   {
     "key": "value"
   }
   ```

3. Use CLI validation:
   ```bash
   python main.py
   > validate-config
   ```

## Agent Issues

### Agent Loading Failures

**Problem:** Agent fails to load or initialize

**Symptoms:**
```bash
AgentError: Agent 'my_agent' not found
ImportError: cannot import name 'MyAgent'
```

**Solution:**

1. Verify agent class exists:
   ```python
   from spoon_ai.agents import SpoonReactAI
   # Should not raise ImportError
   ```

2. Check for typos in agent names and class names

3. List available agents:
   ```bash
   python main.py
   > list-agents
   ```

### Tool Loading Issues (CLI `config.json`)

**Problem:** Tools not available or failing to load

**Symptoms:**
```bash
ToolError: Tool 'crypto_tool' not found
ModuleNotFoundError: No module named 'spoon_toolkits'
```

**Solution:**
1. Install spoon-toolkit package:
   ```bash
   pip install spoon-toolkits
   ```

2. Verify environment variables for tools:
   ```bash
   echo $OKX_API_KEY
   echo $COINGECKO_API_KEY
   ```

3. List available tools:
   ```bash
   python main.py
   > list-toolkit-categories
   > list-toolkit-tools crypto
   ```

## LLM Provider Issues

### Provider Connection Failures

**Problem:** Cannot connect to LLM providers

**Symptoms:**
```bash
ConnectionError: Failed to connect to OpenAI API
TimeoutError: Request timed out
```

**Solution:**
1. Check internet connectivity:
   ```bash
   ping api.openai.com
   ping api.anthropic.com
   ```

2. Verify API endpoints are accessible:
   ```bash
   curl -I https://api.openai.com/v1/models
   ```

3. Check firewall and proxy settings

4. Test with different provider:
   ```bash
   python main.py
   > llm-status
   ```

## MCP (Model Context Protocol) Issues

### MCP Server Connection Problems

**Problem:** Cannot connect to MCP servers

**Symptoms:**
```bash
MCPError: Failed to connect to MCP server
ConnectionRefusedError: [Errno 111] Connection refused
```

**Solution:**
1. Verify MCP server is running:
   ```bash
   curl http://localhost:8765/health
   ```

2. Check MCP server configuration:
   ```json
   {
     "mcp_servers": [
       {
         "name": "my_server",
         "url": "http://localhost:8765",
         "transport": "sse"
       }
     ]
   }
   ```

3. Start MCP server:
   ```bash
   python mcp_server.py
   ```

4. Check server logs for errors

### MCP Tool Discovery Issues

**Problem:** MCP tools not discovered or available

**Symptoms:**
```bash
MCPError: No tools found on server
ToolError: MCP tool 'my_tool' not available
```

**Solution:**
1. Verify tools are registered on MCP server:
   ```python
   @mcp.tool()
   def my_tool():
       return "Hello from MCP"
   ```

2. Check MCP server tool listing:
   ```bash
   curl http://localhost:8765/tools
   ```

3. Restart MCP server after adding tools

4. Verify tool permissions and authentication

## Performance Issues

### Slow Response Times

**Problem:** Agent responses are very slow

**Symptoms:**
- Long delays before responses
- Timeout errors
- High CPU/memory usage

**Solution:**
1. Check system resources:
   ```bash
   python main.py
   > system-info
   ```

2. Optimize LLM configuration:
   ```json
   {
     "llm": {
       "temperature": 0.7,
       "max_tokens": 1000,
       "timeout": 30
     }
   }
   ```

3. Enable caching:
   ```json
   {
     "cache": {
       "enabled": true,
       "ttl": 3600
     }
   }
   ```

4. Reduce tool complexity and number of tools

### Memory Issues

**Problem:** High memory usage or out-of-memory errors

**Symptoms:**
```bash
MemoryError: Unable to allocate memory
Process killed (OOM)
```

**Solution:**
1. Monitor memory usage:
   ```bash
   python main.py
   > system-info
   ```

2. Reduce conversation history:
   ```bash
   python main.py
   > new-chat
   ```

3. Optimize agent configuration:
   ```json
   {
     "config": {
       "max_steps": 5,
       "max_tokens": 500
     }
   }
   ```

4. Use lighter LLM models (e.g., GPT-3.5 instead of GPT-4)

## Blockchain Integration Issues

### RPC Connection Problems

**Problem:** Cannot connect to blockchain RPC endpoints

**Symptoms:**
```bash
ConnectionError: Failed to connect to RPC
HTTPError: 403 Forbidden
```

**Solution:**
1. Verify RPC URL is correct:
   ```bash
   curl -X POST -H "Content-Type: application/json" \\
        --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}' \\
        $RPC_URL
   ```

2. Check RPC provider limits and authentication

3. Try alternative RPC endpoints:
   ```bash
   # Ethereum
   export RPC_URL="https://eth.llamarpc.com"
   export RPC_URL="https://rpc.ankr.com/eth"
   ```

4. Verify network connectivity and firewall settings

### Transaction Failures

**Problem:** Blockchain transactions fail or revert

**Symptoms:**
```bash
TransactionError: Transaction reverted
InsufficientFundsError: Not enough balance
```

**Solution:**
1. Check wallet balance:
   ```bash
   python main.py
   > token-by-symbol ETH
   ```

2. Verify gas settings:
   ```json
   {
     "blockchain": {
       "gas_limit": 21000,
       "gas_price": "20000000000"
     }
   }
   ```

3. Check transaction parameters and recipient address

4. Verify private key and wallet configuration

## Debugging Techniques

### Enable Debug Logging

```bash
# Set environment variables
export DEBUG=true
export LOG_LEVEL=debug

# Run with verbose output
python main.py
```

### Use System Diagnostics

```bash
python main.py
> system-info
> llm-status
> validate-config
```

### Check Configuration

```bash
# Validate configuration
python main.py
> validate-config

# Check migration status
python main.py
> check-config
```

### Test Individual Components

```python
# Test LLM connection
from spoon_ai.llm import LLMManager
llm = LLMManager()
response = await llm.generate("Hello, world!")

# Test tool execution
from spoon_toolkits.crypto import GetTokenPriceTool
tool = GetTokenPriceTool()
result = await tool.execute(symbol="BTC")
```

## Getting Help

### Documentation Resources
- [Installation Guide](../getting-started/installation.md)
- [Configuration Guide](../getting-started/configuration.md)
- [API Reference](../api-reference/index)
- [How-To Guides](../how-to-guides/)

### 📚 **Working Examples**

#### 🎯 [Intent Graph Demo](../examples/intent-graph-demo.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/intent_graph_demo.py)

**Perfect for troubleshooting:**
- Graph system setup and configuration
- Memory management and state persistence issues
- Parallel execution and routing problems
- Production deployment patterns

#### 🔍 [MCP Spoon Search Agent](../examples/mcp-spoon-search-agent.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/mcp/spoon_search_agent.py)

**Great for debugging:**
- MCP server connection and integration issues
- Tool discovery and loading problems
- API rate limiting and error handling
- Multi-tool orchestration challenges

#### 📊 [Graph Crypto Analysis](../examples/graph-crypto-analysis.md)
**GitHub**: [View Source](https://github.com/XSpoonAi/spoon-core/blob/main/examples/graph_crypto_analysis.py)

**Excellent for testing:**
- Real API integration and authentication
- Data processing and validation issues
- Performance optimization problems
- Complex workflow debugging

### Community Support
- GitHub Issues: Report bugs and feature requests
- Discord: Real-time community support
- Documentation: Comprehensive guides and working examples

### Diagnostic Information
When reporting issues, include:
- Python version (`python --version`)
- SpoonOS version
- Operating system
- Error messages and stack traces
- Configuration files (sanitized)
- Steps to reproduce

### Log Collection

```bash
# Enable debug logging
export DEBUG=true
export LOG_LEVEL=debug

# Capture logs
python main.py 2>&1 | tee spoon_debug.log

# Include relevant log sections in issue reports
```

## Prevention Tips

### Regular Maintenance
- Keep dependencies updated
- Rotate API keys regularly
- Monitor system resources
- Backup configuration files
- Test in development environment first

### Best Practices
- Use version control for configurations
- Implement proper error handling
- Monitor API usage and costs
- Set up alerts for critical issues
- Document custom configurations

### Environment Management
- Use separate environments for development/production
- Pin dependency versions in requirements.txt
- Use environment variables for sensitive data
- Regularly test backup and recovery procedures

## See Also

- [Debugging Guide](./debugging.md)
- [Performance Optimization](./performance.md)
- [System Requirements](../getting-started/installation.md)"}

## Prevention Tips

### Regular Maintenance
- Keep dependencies updated
- Rotate API keys regularly
- Monitor system resources
- Backup configuration files
- Test in development environment first

### Best Practices
- Use version control for configurations
- Implement proper error handling
- Monitor API usage and costs
- Set up alerts for critical issues
- Document custom configurations

### Environment Management
- Use separate environments for development/production
- Pin dependency versions in requirements.txt
- Use environment variables for sensitive data
- Regularly test backup and recovery procedures

## See Also

- [Debugging Guide](./debugging.md)
- [Performance Optimization](./performance.md)
- [System Requirements](../getting-started/installation.md)"}

---

FILE: docs/troubleshooting/debugging.md

# Debugging Guide

Comprehensive guide for debugging SpoonOS applications, agents, and tools.

## Debug Configuration

### Environment Variables

```bash
# Enable debug mode
export DEBUG=true
export LOG_LEVEL=debug

# Enable specific debug categories
export DEBUG_AGENTS=true
export DEBUG_TOOLS=true
export DEBUG_LLM=true
export DEBUG_MCP=true

# Enable request/response logging
export LOG_REQUESTS=true
export LOG_RESPONSES=true
```

### Configuration File Debug Settings

```json
{
  "debug": {
    "enabled": true,
    "log_level": "debug",
    "categories": ["agents", "tools", "llm", "mcp"],
    "log_requests": true,
    "log_responses": true,
    "save_logs": true,
    "log_file": "spoon_debug.log"
  }
}
```

## Logging Setup

### Python Logging Configuration

```python
# debug_setup.py
import logging
import sys
from datetime import datetime

def setup_debug_logging():
    """Configure comprehensive debug logging"""
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
    )
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG)
    console_handler.setFormatter(formatter)
    
    # File handler
    file_handler = logging.FileHandler(
        f'spoon_debug_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)
    
    # Configure SpoonOS loggers
    spoon_logger = logging.getLogger('spoon_ai')
    spoon_logger.setLevel(logging.DEBUG)
    
    toolkit_logger = logging.getLogger('spoon_toolkits')
    toolkit_logger.setLevel(logging.DEBUG)
    
    print("Debug logging configured")

if __name__ == "__main__":
    setup_debug_logging()
```

### Structured Logging

```python
# structured_logging.py
import structlog
import json
from datetime import datetime

def setup_structured_logging():
    """Configure structured logging with JSON output"""
    
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
    
    return structlog.get_logger()

# Usage example
logger = setup_structured_logging()
logger.info("Agent started", agent_name="debug_agent", tools_count=5)
logger.error("Tool execution failed", tool_name="crypto_tool", error="API timeout")
```

## Agent Debugging

### Agent State Inspection

```python
# agent_debugger.py
from spoon_ai.agents import SpoonReactAI
from spoon_ai.tools import ToolManager
import json

class DebuggableAgent(SpoonReactAI):
    """Agent with enhanced debugging capabilities"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.debug_info = {
            "steps": [],
            "tool_calls": [],
            "llm_requests": [],
            "errors": []
        }
    
    async def run(self, message: str, **kwargs):
        """Run with debug tracking"""
        self.debug_info["steps"].append({
            "timestamp": datetime.now().isoformat(),
            "action": "run_started",
            "message": message,
            "kwargs": kwargs
        })
        
        try:
            result = await super().run(message, **kwargs)
            
            self.debug_info["steps"].append({
                "timestamp": datetime.now().isoformat(),
                "action": "run_completed",
                "result_length": len(str(result))
            })
            
            return result
            
        except Exception as e:
            self.debug_info["errors"].append({
                "timestamp": datetime.now().isoformat(),
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            })
            raise
    
    def get_debug_info(self) -> dict:
        """Get comprehensive debug information"""
        return {
            "agent_name": self.name,
            "system_prompt": self.system_prompt,
            "config": self.config,
            "available_tools": [tool.name for tool in self.tools],
            "debug_info": self.debug_info
        }
    
    def save_debug_info(self, filename: str = None):
        """Save debug information to file"""
        if not filename:
            filename = f"debug_{self.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w') as f:
            json.dump(self.get_debug_info(), f, indent=2)
        
        print(f"Debug info saved to {filename}")

# Usage
async def debug_agent_example():
    agent = DebuggableAgent(
        name="debug_agent",
        system_prompt="You are a debugging assistant."
    )
    
    try:
        response = await agent.run("Hello, debug me!")
        print(f"Response: {response}")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        agent.save_debug_info()
        print(json.dumps(agent.get_debug_info(), indent=2))
```

### Step-by-Step Execution Tracing

```python
# execution_tracer.py
from spoon_ai.agents.base import BaseAgent
from functools import wraps
import inspect

def trace_execution(func):
    """Decorator to trace function execution"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        func_name = func.__name__
        
        # Log function entry
        logger.debug(
            "Function entry",
            function=func_name,
            args=args,
            kwargs=kwargs,
            agent=getattr(self, 'name', 'unknown')
        )
        
        try:
            # Execute function
            result = await func(self, *args, **kwargs)
            
            # Log successful completion
            logger.debug(
                "Function success",
                function=func_name,
                result_type=type(result).__name__,
                agent=getattr(self, 'name', 'unknown')
            )
            
            return result
            
        except Exception as e:
            # Log error
            logger.error(
                "Function error",
                function=func_name,
                error_type=type(e).__name__,
                error_message=str(e),
                agent=getattr(self, 'name', 'unknown')
            )
            raise
    
    return wrapper

class TracedAgent(BaseAgent):
    """Agent with method tracing"""
    
    @trace_execution
    async def run(self, message: str, **kwargs):
        return await super().run(message, **kwargs)
    
    @trace_execution
    async def chat(self, messages, **kwargs):
        return await super().chat(messages, **kwargs)
```

## Tool Debugging

### Tool Execution Monitoring

```python
# tool_debugger.py
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.errors import ToolError
import time
import traceback

class DebuggableTool(BaseTool):
    """Base tool with debugging capabilities"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.execution_history = []
        self.performance_stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "average_duration": 0,
            "total_duration": 0
        }
    
    async def execute(self, **kwargs):
        """Execute with comprehensive debugging"""
        start_time = time.time()
        execution_id = f"{self.name}_{int(start_time)}"
        
        # Log execution start
        logger.debug(
            "Tool execution started",
            tool=self.name,
            execution_id=execution_id,
            parameters=kwargs
        )
        
        try:
            # Validate parameters
            validated_params = self.validate_parameters(**kwargs)
            
            # Execute tool
            result = await self._execute_impl(**validated_params)
            
            # Calculate duration
            duration = time.time() - start_time
            
            # Update stats
            self._update_success_stats(duration)
            
            # Log execution success
            logger.debug(
                "Tool execution completed",
                tool=self.name,
                execution_id=execution_id,
                duration=duration,
                result_type=type(result).__name__
            )
            
            # Store execution history
            self.execution_history.append({
                "execution_id": execution_id,
                "timestamp": start_time,
                "duration": duration,
                "parameters": validated_params,
                "success": True,
                "result_type": type(result).__name__
            })
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            
            # Update stats
            self._update_error_stats(duration)
            
            # Log execution error
            logger.error(
                "Tool execution failed",
                tool=self.name,
                execution_id=execution_id,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e),
                traceback=traceback.format_exc()
            )
            
            # Store execution history
            self.execution_history.append({
                "execution_id": execution_id,
                "timestamp": start_time,
                "duration": duration,
                "parameters": kwargs,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
    
    async def _execute_impl(self, **kwargs):
        """Override this method in subclasses"""
        raise NotImplementedError
    
    def _update_success_stats(self, duration: float):
        """Update performance statistics for successful execution"""
        self.performance_stats["total_calls"] += 1
        self.performance_stats["successful_calls"] += 1
        self.performance_stats["total_duration"] += duration
        self.performance_stats["average_duration"] = (
            self.performance_stats["total_duration"] / 
            self.performance_stats["total_calls"]
        )
    
    def _update_error_stats(self, duration: float):
        """Update performance statistics for failed execution"""
        self.performance_stats["total_calls"] += 1
        self.performance_stats["failed_calls"] += 1
        self.performance_stats["total_duration"] += duration
        self.performance_stats["average_duration"] = (
            self.performance_stats["total_duration"] / 
            self.performance_stats["total_calls"]
        )
    
    def get_debug_info(self) -> dict:
        """Get comprehensive debug information"""
        return {
            "tool_name": self.name,
            "description": self.description,
            "performance_stats": self.performance_stats,
            "recent_executions": self.execution_history[-10:],  # Last 10 executions
            "total_executions": len(self.execution_history)
        }
```

### Parameter Validation Debugging

```python
# parameter_debugger.py
from spoon_ai.tools.base import BaseTool
from spoon_ai.tools.errors import ValidationError
import jsonschema

class ValidatedTool(DebuggableTool):
    """Tool with enhanced parameter validation and debugging"""
    
    # Define parameter schema
    parameter_schema = {
        "type": "object",
        "properties": {},
        "required": []
    }
    
    def validate_parameters(self, **kwargs) -> dict:
        """Validate parameters with detailed error reporting"""
        logger.debug(
            "Parameter validation started",
            tool=self.name,
            raw_parameters=kwargs,
            schema=self.parameter_schema
        )
        
        try:
            # Validate against schema
            jsonschema.validate(kwargs, self.parameter_schema)
            
            # Custom validation
            validated = self._custom_validation(**kwargs)
            
            logger.debug(
                "Parameter validation successful",
                tool=self.name,
                validated_parameters=validated
            )
            
            return validated
            
        except jsonschema.ValidationError as e:
            logger.error(
                "Schema validation failed",
                tool=self.name,
                validation_error=str(e),
                error_path=list(e.path),
                invalid_value=e.instance
            )
            raise ValidationError(f"Parameter validation failed: {e.message}")
        
        except Exception as e:
            logger.error(
                "Custom validation failed",
                tool=self.name,
                validation_error=str(e)
            )
            raise ValidationError(f"Parameter validation failed: {str(e)}")
    
    def _custom_validation(self, **kwargs) -> dict:
        """Override for custom validation logic"""
        return kwargs
```

## LLM Debugging

### Request/Response Logging

```python
# llm_debugger.py
from spoon_ai.llm.base import BaseLLMProvider
import json
import time

class DebuggableLLMProvider(BaseLLMProvider):
    """LLM provider with request/response logging"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.request_history = []
    
    async def generate(self, messages, **kwargs):
        """Generate with request/response logging"""
        request_id = f"req_{int(time.time() * 1000)}"
        start_time = time.time()
        
        # Log request
        logger.debug(
            "LLM request started",
            provider=self.provider_name,
            request_id=request_id,
            model=kwargs.get('model', self.default_model),
            message_count=len(messages),
            parameters=kwargs
        )
        
        # Log messages (truncated for privacy)
        for i, msg in enumerate(messages):
            content_preview = msg.get('content', '')[:100] + '...' if len(msg.get('content', '')) > 100 else msg.get('content', '')
            logger.debug(
                "LLM message",
                request_id=request_id,
                message_index=i,
                role=msg.get('role'),
                content_preview=content_preview
            )
        
        try:
            # Make request
            response = await super().generate(messages, **kwargs)
            
            duration = time.time() - start_time
            
            # Log response
            logger.debug(
                "LLM request completed",
                provider=self.provider_name,
                request_id=request_id,
                duration=duration,
                response_length=len(str(response)),
                tokens_used=response.get('usage', {}).get('total_tokens', 0)
            )
            
            # Store request history
            self.request_history.append({
                "request_id": request_id,
                "timestamp": start_time,
                "duration": duration,
                "model": kwargs.get('model', self.default_model),
                "message_count": len(messages),
                "success": True,
                "tokens_used": response.get('usage', {}).get('total_tokens', 0)
            })
            
            return response
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "LLM request failed",
                provider=self.provider_name,
                request_id=request_id,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            # Store request history
            self.request_history.append({
                "request_id": request_id,
                "timestamp": start_time,
                "duration": duration,
                "model": kwargs.get('model', self.default_model),
                "message_count": len(messages),
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
```

### Token Usage Monitoring

```python
# token_monitor.py
from collections import defaultdict
import time

class TokenUsageMonitor:
    """Monitor and analyze token usage patterns"""
    
    def __init__(self):
        self.usage_stats = defaultdict(lambda: {
            "total_requests": 0,
            "total_tokens": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_cost": 0.0,
            "average_tokens_per_request": 0,
            "requests_by_hour": defaultdict(int)
        })
    
    def record_usage(self, provider: str, model: str, usage: dict, cost: float = 0.0):
        """Record token usage for analysis"""
        key = f"{provider}:{model}"
        stats = self.usage_stats[key]
        
        # Update counters
        stats["total_requests"] += 1
        stats["total_tokens"] += usage.get("total_tokens", 0)
        stats["prompt_tokens"] += usage.get("prompt_tokens", 0)
        stats["completion_tokens"] += usage.get("completion_tokens", 0)
        stats["total_cost"] += cost
        
        # Update averages
        stats["average_tokens_per_request"] = (
            stats["total_tokens"] / stats["total_requests"]
        )
        
        # Track hourly usage
        hour = int(time.time() // 3600)
        stats["requests_by_hour"][hour] += 1
        
        logger.info(
            "Token usage recorded",
            provider=provider,
            model=model,
            tokens=usage.get("total_tokens", 0),
            cost=cost
        )
    
    def get_usage_report(self) -> dict:
        """Generate comprehensive usage report"""
        report = {
            "total_providers": len(self.usage_stats),
            "providers": {}
        }
        
        for key, stats in self.usage_stats.items():
            provider, model = key.split(":", 1)
            
            if provider not in report["providers"]:
                report["providers"][provider] = {
                    "models": {},
                    "total_requests": 0,
                    "total_tokens": 0,
                    "total_cost": 0.0
                }
            
            # Add model stats
            report["providers"][provider]["models"][model] = stats
            
            # Aggregate provider stats
            report["providers"][provider]["total_requests"] += stats["total_requests"]
            report["providers"][provider]["total_tokens"] += stats["total_tokens"]
            report["providers"][provider]["total_cost"] += stats["total_cost"]
        
        return report
    
    def print_usage_summary(self):
        """Print formatted usage summary"""
        report = self.get_usage_report()
        
        print("\
=== Token Usage Summary ===")
        print(f"Total Providers: {report['total_providers']}")
        
        for provider, provider_stats in report["providers"].items():
            print(f"\
{provider.upper()}:")
            print(f"  Total Requests: {provider_stats['total_requests']:,}")
            print(f"  Total Tokens: {provider_stats['total_tokens']:,}")
            print(f"  Total Cost: ${provider_stats['total_cost']:.4f}")
            
            for model, model_stats in provider_stats["models"].items():
                print(f"  {model}:")
                print(f"    Requests: {model_stats['total_requests']:,}")
                print(f"    Tokens: {model_stats['total_tokens']:,}")
                print(f"    Avg Tokens/Request: {model_stats['average_tokens_per_request']:.1f}")
                print(f"    Cost: ${model_stats['total_cost']:.4f}")
```

## MCP Debugging

### MCP Server Connection Debugging

```python
# mcp_debugger.py
from spoon_ai.tools.mcp_client import MCPClient
import asyncio
import aiohttp

class DebuggableMCPClient(MCPClient):
    """MCP client with enhanced debugging"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.connection_history = []
        self.tool_discovery_history = []
    
    async def connect(self):
        """Connect with connection debugging"""
        start_time = time.time()
        
        logger.debug(
            "MCP connection attempt",
            server_url=self.server_url,
            transport=self.transport
        )
        
        try:
            await super().connect()
            
            duration = time.time() - start_time
            
            logger.info(
                "MCP connection successful",
                server_url=self.server_url,
                duration=duration
            )
            
            self.connection_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": True
            })
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "MCP connection failed",
                server_url=self.server_url,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            self.connection_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
    
    async def discover_tools(self):
        """Discover tools with debugging"""
        start_time = time.time()
        
        logger.debug("MCP tool discovery started", server_url=self.server_url)
        
        try:
            tools = await super().discover_tools()
            
            duration = time.time() - start_time
            
            logger.info(
                "MCP tool discovery completed",
                server_url=self.server_url,
                tools_found=len(tools),
                duration=duration,
                tool_names=[tool.name for tool in tools]
            )
            
            self.tool_discovery_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": True,
                "tools_found": len(tools),
                "tool_names": [tool.name for tool in tools]
            })
            
            return tools
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "MCP tool discovery failed",
                server_url=self.server_url,
                duration=duration,
                error_type=type(e).__name__,
                error_message=str(e)
            )
            
            self.tool_discovery_history.append({
                "timestamp": start_time,
                "duration": duration,
                "success": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            })
            
            raise
```

## Performance Debugging

### Performance Profiler

```python
# performance_profiler.py
import cProfile
import pstats
import io
from functools import wraps
import time
import psutil
import os

class PerformanceProfiler:
    """Profile performance of SpoonOS components"""
    
    def __init__(self):
        self.profiles = {}
        self.memory_snapshots = []
    
    def profile_function(self, func_name: str = None):
        """Decorator to profile function performance"""
        def decorator(func):
            name = func_name or f"{func.__module__}.{func.__name__}"
            
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Memory snapshot before
                process = psutil.Process(os.getpid())
                memory_before = process.memory_info().rss
                
                # CPU profiling
                profiler = cProfile.Profile()
                profiler.enable()
                
                start_time = time.time()
                
                try:
                    result = await func(*args, **kwargs)
                    
                    # Stop profiling
                    end_time = time.time()
                    profiler.disable()
                    
                    # Memory snapshot after
                    memory_after = process.memory_info().rss
                    
                    # Store profile data
                    s = io.StringIO()
                    ps = pstats.Stats(profiler, stream=s)
                    ps.sort_stats('cumulative')
                    ps.print_stats()
                    
                    self.profiles[name] = {
                        "timestamp": start_time,
                        "duration": end_time - start_time,
                        "memory_before": memory_before,
                        "memory_after": memory_after,
                        "memory_delta": memory_after - memory_before,
                        "profile_stats": s.getvalue()
                    }
                    
                    logger.debug(
                        "Function profiled",
                        function=name,
                        duration=end_time - start_time,
                        memory_delta=memory_after - memory_before
                    )
                    
                    return result
                    
                except Exception as e:
                    profiler.disable()
                    raise
            
            return wrapper
        return decorator
    
    def take_memory_snapshot(self, label: str = None):
        """Take a memory usage snapshot"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        snapshot = {
            "timestamp": time.time(),
            "label": label or f"snapshot_{len(self.memory_snapshots)}",
            "rss": memory_info.rss,
            "vms": memory_info.vms,
            "percent": process.memory_percent(),
            "cpu_percent": process.cpu_percent()
        }
        
        self.memory_snapshots.append(snapshot)
        
        logger.debug(
            "Memory snapshot taken",
            label=snapshot["label"],
            rss_mb=snapshot["rss"] / 1024 / 1024,
            cpu_percent=snapshot["cpu_percent"]
        )
        
        return snapshot
    
    def generate_performance_report(self) -> str:
        """Generate comprehensive performance report"""
        report = ["\
=== Performance Report ==="]
        
        # Function profiles
        if self.profiles:
            report.append("\
Function Profiles:")
            for func_name, profile in self.profiles.items():
                report.append(f"\
{func_name}:")
                report.append(f"  Duration: {profile['duration']:.4f}s")
                report.append(f"  Memory Delta: {profile['memory_delta'] / 1024 / 1024:.2f} MB")
        
        # Memory snapshots
        if self.memory_snapshots:
            report.append("\
Memory Snapshots:")
            for snapshot in self.memory_snapshots:
                report.append(
                    f"  {snapshot['label']}: {snapshot['rss'] / 1024 / 1024:.2f} MB "
                    f"({snapshot['percent']:.1f}%) CPU: {snapshot['cpu_percent']:.1f}%"
                )
        
        return "\
".join(report)
```

## Debug CLI Commands

### Enhanced CLI with Debug Commands

```python
# debug_cli.py
from spoon_ai.cli.base import BaseCLI
import json

class DebugCLI(BaseCLI):
    """CLI with enhanced debugging commands"""
    
    def __init__(self):
        super().__init__()
        self.debug_mode = False
        self.profiler = PerformanceProfiler()
    
    def do_debug_on(self, args):
        """Enable debug mode"""
        self.debug_mode = True
        setup_debug_logging()
        print("Debug mode enabled")
    
    def do_debug_off(self, args):
        """Disable debug mode"""
        self.debug_mode = False
        print("Debug mode disabled")
    
    def do_debug_agent(self, args):
        """Show agent debug information"""
        if hasattr(self.current_agent, 'get_debug_info'):
            debug_info = self.current_agent.get_debug_info()
            print(json.dumps(debug_info, indent=2))
        else:
            print("Current agent does not support debugging")
    
    def do_debug_tools(self, args):
        """Show tool debug information"""
        if hasattr(self.current_agent, 'tools'):
            for tool in self.current_agent.tools:
                if hasattr(tool, 'get_debug_info'):
                    debug_info = tool.get_debug_info()
                    print(f"\
{tool.name}:")
                    print(json.dumps(debug_info, indent=2))
        else:
            print("No tools available for debugging")
    
    def do_debug_memory(self, args):
        """Take memory snapshot"""
        snapshot = self.profiler.take_memory_snapshot(args or "manual")
        print(f"Memory snapshot: {snapshot['rss'] / 1024 / 1024:.2f} MB")
    
    def do_debug_performance(self, args):
        """Show performance report"""
        report = self.profiler.generate_performance_report()
        print(report)
    
    def do_debug_save(self, args):
        """Save debug information to file"""
        filename = args or f"debug_session_{int(time.time())}.json"
        
        debug_data = {
            "timestamp": time.time(),
            "agent_info": self.current_agent.get_debug_info() if hasattr(self.current_agent, 'get_debug_info') else None,
            "tool_info": [tool.get_debug_info() for tool in getattr(self.current_agent, 'tools', []) if hasattr(tool, 'get_debug_info')],
            "memory_snapshots": self.profiler.memory_snapshots,
            "performance_profiles": self.profiler.profiles
        }
        
        with open(filename, 'w') as f:
            json.dump(debug_data, f, indent=2)
        
        print(f"Debug information saved to {filename}")
```

## Best Practices

### Debug-Friendly Code
- Add comprehensive logging at key points
- Use structured logging with context
- Implement debug modes in components
- Provide introspection methods
- Store execution history for analysis

### Performance Monitoring
- Profile critical code paths
- Monitor memory usage patterns
- Track API call performance
- Measure end-to-end latency
- Set up alerts for performance degradation

### Error Investigation
- Capture full stack traces
- Log relevant context information
- Implement error categorization
- Store error patterns for analysis
- Provide clear error messages

### Production Debugging
- Use log levels appropriately
- Implement feature flags for debug features
- Provide remote debugging capabilities
- Monitor system health metrics
- Set up automated error reporting

## See Also

- [Common Issues](./common-issues.md)
- [Performance Optimization](./performance.md)
- [Logging Configuration](../getting-started/configuration.md)
- [System Monitoring](../api-reference/index)"}

---

FILE: docs/troubleshooting/performance.md

# Performance Optimization Guide

Comprehensive guide for optimizing SpoonOS performance across agents, tools, and infrastructure.

## Performance Monitoring

### System Metrics

```bash
# Check system performance
python main.py
> system-info

# Monitor resource usage
top -p $(pgrep -f "python main.py")
htop
```

### Built-in Performance Monitoring

```python
# performance_monitor.py
import psutil
import time
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class PerformanceMetrics:
    timestamp: float
    cpu_percent: float
    memory_rss: int
    memory_percent: float
    disk_io_read: int
    disk_io_write: int
    network_sent: int
    network_recv: int

class PerformanceMonitor:
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.process = psutil.Process()
        self.initial_io = self.process.io_counters()
        self.initial_net = psutil.net_io_counters()

    def collect_metrics(self) -> PerformanceMetrics:
        """Collect current performance metrics"""
        current_io = self.process.io_counters()
        current_net = psutil.net_io_counters()

        metrics = PerformanceMetrics(
            timestamp=time.time(),
            cpu_percent=self.process.cpu_percent(),
            memory_rss=self.process.memory_info().rss,
            memory_percent=self.process.memory_percent(),
            disk_io_read=current_io.read_bytes - self.initial_io.read_bytes,
            disk_io_write=current_io.write_bytes - self.initial_io.write_bytes,
            network_sent=current_net.bytes_sent - self.initial_net.bytes_sent,
            network_recv=current_net.bytes_recv - self.initial_net.bytes_recv
        )

        self.metrics_history.append(metrics)
        return metrics

    def get_performance_summary(self, window_minutes: int = 5) -> Dict:
        """Get performance summary for the last N minutes"""
        cutoff_time = time.time() - (window_minutes * 60)
        recent_metrics = [m for m in self.metrics_history if m.timestamp > cutoff_time]

        if not recent_metrics:
            return {"error": "No metrics available"}

        return {
            "window_minutes": window_minutes,
            "sample_count": len(recent_metrics),
            "cpu": {
                "avg": sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics),
                "max": max(m.cpu_percent for m in recent_metrics),
                "min": min(m.cpu_percent for m in recent_metrics)
            },
            "memory": {
                "current_mb": recent_metrics[-1].memory_rss / 1024 / 1024,
                "peak_mb": max(m.memory_rss for m in recent_metrics) / 1024 / 1024,
                "avg_percent": sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
            },
            "io": {
                "total_read_mb": recent_metrics[-1].disk_io_read / 1024 / 1024,
                "total_write_mb": recent_metrics[-1].disk_io_write / 1024 / 1024
            }
        }
```

## Agent Performance Optimization

### Efficient Agent Configuration

```json
{
  "agents": {
    "optimized_agent": {
      "class": "SpoonReactAI",
      "config": {
        "max_steps": 5,
        "temperature": 0.7,
        "max_tokens": 1000,
        "timeout": 30,
        "stream": true,
        "cache_enabled": true
      }
    }
  }
}
```

### Memory-Efficient Agent Implementation

```python
# optimized_agent.py
from spoon_ai.agents import SpoonReactAI
from typing import List, Dict
import gc

class OptimizedAgent(SpoonReactAI):
    """Memory-optimized agent implementation"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.max_history_length = kwargs.get('max_history_length', 50)
        self.cleanup_interval = kwargs.get('cleanup_interval', 10)
        self.request_count = 0

    async def run(self, message: str, **kwargs):
        """Run with memory management"""
        try:
            result = await super().run(message, **kwargs)

            # Periodic cleanup
            self.request_count += 1
            if self.request_count % self.cleanup_interval == 0:
                self._cleanup_memory()

            return result

        except Exception as e:
            # Force cleanup on error
            self._cleanup_memory()
            raise

    def _cleanup_memory(self):
        """Clean up memory usage"""
        # Limit conversation history
        if hasattr(self, 'conversation_history'):
            if len(self.conversation_history) > self.max_history_length:
                # Keep only recent messages
                self.conversation_history = self.conversation_history[-self.max_history_length:]

        # Force garbage collection
        gc.collect()

        logger.debug("Memory cleanup performed", agent=self.name)
```

### Conversation History Management

```python
# history_manager.py
from collections import deque
from typing import Dict, List, Optional
import json
import hashlib

class ConversationHistoryManager:
    """Efficient conversation history management"""

    def __init__(self, max_length: int = 100, compression_enabled: bool = True):
        self.max_length = max_length
        self.compression_enabled = compression_enabled
        self.messages = deque(maxlen=max_length)
        self.compressed_history = {}

    def add_message(self, message: Dict):
        """Add message with automatic compression"""
        # Add to recent messages
        self.messages.append(message)

        # Compress old messages if enabled
        if self.compression_enabled and len(self.messages) == self.max_length:
            self._compress_old_messages()

    def get_recent_messages(self, count: int = 10) -> List[Dict]:
        """Get recent messages efficiently"""
        return list(self.messages)[-count:]

    def get_context_summary(self) -> str:
        """Get compressed context summary"""
        if not self.compressed_history:
            return ""

        # Generate summary from compressed history
        summaries = []
        for period, summary in self.compressed_history.items():
            summaries.append(f"Period {period}: {summary}")

        return "\
".join(summaries)

    def _compress_old_messages(self):
        """Compress older messages into summaries"""
        # Take first half of messages for compression
        to_compress = list(self.messages)[:self.max_length // 2]

        # Generate summary (simplified)
        summary = self._generate_summary(to_compress)

        # Store compressed summary
        period_key = len(self.compressed_history)
        self.compressed_history[period_key] = summary

        # Remove compressed messages from deque
        for _ in range(len(to_compress)):
            self.messages.popleft()

    def _generate_summary(self, messages: List[Dict]) -> str:
        """Generate summary of message batch"""
        # Simple summarization (could be enhanced with LLM)
        user_messages = [m['content'] for m in messages if m.get('role') == 'user']
        assistant_messages = [m['content'] for m in messages if m.get('role') == 'assistant']

        return f"User asked {len(user_messages)} questions, assistant provided {len(assistant_messages)} responses"
```

## Tool Performance Optimization

### Async Tool Implementation

```python
# async_tool.py
import asyncio
import aiohttp
from spoon_ai.tools.base import BaseTool
from typing import List, Dict, Any

class AsyncHTTPTool(BaseTool):
    """High-performance async HTTP tool"""

    name = "async_http_tool"
    description = "Optimized HTTP requests with connection pooling"

    def __init__(self):
        self.session = None
        self.connector = None

    async def __aenter__(self):
        # Create optimized connector
        self.connector = aiohttp.TCPConnector(
            limit=100,  # Total connection pool size
            limit_per_host=30,  # Per-host connection limit
            ttl_dns_cache=300,  # DNS cache TTL
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )

        # Create session with optimized settings
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={'User-Agent': 'SpoonOS/1.0'}
        )

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()

    async def execute(self, urls: List[str], method: str = "GET", **kwargs) -> List[Dict]:
        """Execute multiple HTTP requests concurrently"""
        if not self.session:
            async with self:
                return await self._make_requests(urls, method, **kwargs)
        else:
            return await self._make_requests(urls, method, **kwargs)

    async def _make_requests(self, urls: List[str], method: str, **kwargs) -> List[Dict]:
        """Make concurrent HTTP requests"""
        semaphore = asyncio.Semaphore(10)  # Limit concurrent requests

        async def make_request(url: str) -> Dict:
            async with semaphore:
                try:
                    async with self.session.request(method, url, **kwargs) as response:
                        return {
                            "url": url,
                            "status": response.status,
                            "data": await response.text(),
                            "headers": dict(response.headers)
                        }
                except Exception as e:
                    return {
                        "url": url,
                        "error": str(e),
                        "status": 0
                    }

        # Execute all requests concurrently
        tasks = [make_request(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return [r for r in results if not isinstance(r, Exception)]
```

### Caching Implementation

```python
# caching_tool.py
import asyncio
import hashlib
import json
import time
from typing import Any, Dict, Optional
from spoon_ai.tools.base import BaseTool

class CachedTool(BaseTool):
    """Tool with intelligent caching"""

    def __init__(self, cache_ttl: int = 3600, max_cache_size: int = 1000):
        self.cache_ttl = cache_ttl
        self.max_cache_size = max_cache_size
        self.cache: Dict[str, Dict] = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0
        }

    async def execute(self, **kwargs) -> Any:
        """Execute with caching"""
        # Generate cache key
        cache_key = self._generate_cache_key(**kwargs)

        # Check cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            self.cache_stats["hits"] += 1
            logger.debug("Cache hit", tool=self.name, cache_key=cache_key[:8])
            return cached_result

        # Cache miss - execute tool
        self.cache_stats["misses"] += 1
        logger.debug("Cache miss", tool=self.name, cache_key=cache_key[:8])

        result = await self._execute_impl(**kwargs)

        # Store in cache
        self._set_cache(cache_key, result)

        return result

    def _generate_cache_key(self, **kwargs) -> str:
        """Generate deterministic cache key"""
        # Sort kwargs for consistent key generation
        key_data = json.dumps(kwargs, sort_keys=True, default=str)
        return hashlib.sha256(key_data.encode()).hexdigest()

    def _get_from_cache(self, key: str) -> Optional[Any]:
        """Get value from cache if not expired"""
        if key not in self.cache:
            return None

        entry = self.cache[key]
        if time.time() - entry["timestamp"] > self.cache_ttl:
            # Expired - remove from cache
            del self.cache[key]
            return None

        return entry["value"]

    def _set_cache(self, key: str, value: Any):
        """Set value in cache with eviction"""
        # Evict oldest entries if cache is full
        if len(self.cache) >= self.max_cache_size:
            self._evict_oldest()

        self.cache[key] = {
            "value": value,
            "timestamp": time.time()
        }

    def _evict_oldest(self):
        """Evict oldest cache entries"""
        # Sort by timestamp and remove oldest 10%
        sorted_items = sorted(
            self.cache.items(),
            key=lambda x: x[1]["timestamp"]
        )

        evict_count = max(1, len(sorted_items) // 10)
        for i in range(evict_count):
            key = sorted_items[i][0]
            del self.cache[key]
            self.cache_stats["evictions"] += 1

    def get_cache_stats(self) -> Dict:
        """Get cache performance statistics"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = self.cache_stats["hits"] / total_requests if total_requests > 0 else 0

        return {
            "cache_size": len(self.cache),
            "max_cache_size": self.max_cache_size,
            "hit_rate": hit_rate,
            "total_requests": total_requests,
            **self.cache_stats
        }

    async def _execute_impl(self, **kwargs) -> Any:
        """Override this method in subclasses"""
        raise NotImplementedError
```

## LLM Performance Optimization

### Request Batching

```python
# batch_llm_provider.py
import asyncio
from typing import List, Dict, Any
from spoon_ai.llm.base import BaseLLMProvider

class BatchedLLMProvider(BaseLLMProvider):
    """LLM provider with request batching"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.batch_size = kwargs.get('batch_size', 5)
        self.batch_timeout = kwargs.get('batch_timeout', 1.0)
        self.pending_requests = []
        self.batch_lock = asyncio.Lock()

    async def generate(self, messages: List[Dict], **kwargs) -> Dict:
        """Generate with batching optimization"""
        # For single requests, use batching
        if len(messages) == 1:
            return await self._batched_generate(messages[0], **kwargs)
        else:
            # For multi-message requests, process directly
            return await super().generate(messages, **kwargs)

    async def _batched_generate(self, message: Dict, **kwargs) -> Dict:
        """Generate with request batching"""
        # Create request future
        request_future = asyncio.Future()
        request_data = {
            "message": message,
            "kwargs": kwargs,
            "future": request_future
        }

        async with self.batch_lock:
            self.pending_requests.append(request_data)

            # If batch is full or this is the first request, process batch
            if len(self.pending_requests) >= self.batch_size:
                await self._process_batch()
            elif len(self.pending_requests) == 1:
                # Start batch timer for first request
                asyncio.create_task(self._batch_timer())

        # Wait for result
        return await request_future

    async def _batch_timer(self):
        """Timer to process batch after timeout"""
        await asyncio.sleep(self.batch_timeout)

        async with self.batch_lock:
            if self.pending_requests:
                await self._process_batch()

    async def _process_batch(self):
        """Process pending requests as a batch"""
        if not self.pending_requests:
            return

        batch = self.pending_requests.copy()
        self.pending_requests.clear()

        try:
            # Process all requests concurrently
            tasks = [
                self._process_single_request(req["message"], req["kwargs"])
                for req in batch
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Set results for each future
            for req, result in zip(batch, results):
                if isinstance(result, Exception):
                    req["future"].set_exception(result)
                else:
                    req["future"].set_result(result)

        except Exception as e:
            # Set exception for all futures
            for req in batch:
                if not req["future"].done():
                    req["future"].set_exception(e)

    async def _process_single_request(self, message: Dict, kwargs: Dict) -> Dict:
        """Process a single request"""
        return await super().generate([message], **kwargs)
```

### Response Streaming

```python
# streaming_provider.py
import asyncio
from typing import AsyncGenerator, Dict, Any
from spoon_ai.llm.base import BaseLLMProvider

class StreamingLLMProvider(BaseLLMProvider):
    """LLM provider with streaming support"""

    async def generate_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """Generate streaming response"""
        # Implementation depends on provider API
        async for chunk in self._stream_implementation(messages, **kwargs):
            yield chunk

    async def _stream_implementation(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """Provider-specific streaming implementation"""
        # Example implementation
        response = await self._make_streaming_request(messages, **kwargs)

        async for line in response:
            if line.startswith('data: '):
                chunk_data = line[6:]  # Remove 'data: ' prefix
                if chunk_data.strip() == '[DONE]':
                    break

                try:
                    chunk_json = json.loads(chunk_data)
                    if 'choices' in chunk_json and chunk_json['choices']:
                        delta = chunk_json['choices'][0].get('delta', {})
                        if 'content' in delta:
                            yield delta['content']
                except json.JSONDecodeError:
                    continue
```

## Database and Storage Optimization

### Connection Pooling

```python
# db_pool.py
import asyncpg
import asyncio
from typing import Optional, Dict, Any

class DatabasePool:
    """Optimized database connection pool"""

    def __init__(self, connection_string: str, **pool_kwargs):
        self.connection_string = connection_string
        self.pool_kwargs = {
            'min_size': 5,
            'max_size': 20,
            'command_timeout': 60,
            'server_settings': {
                'jit': 'off',  # Disable JIT for faster startup
                'application_name': 'spoon_ai'
            },
            **pool_kwargs
        }
        self.pool: Optional[asyncpg.Pool] = None

    async def initialize(self):
        """Initialize connection pool"""
        if self.pool is None:
            self.pool = await asyncpg.create_pool(
                self.connection_string,
                **self.pool_kwargs
            )

    async def execute_query(self, query: str, *args) -> List[Dict[str, Any]]:
        """Execute query with connection pooling"""
        if self.pool is None:
            await self.initialize()

        async with self.pool.acquire() as connection:
            rows = await connection.fetch(query, *args)
            return [dict(row) for row in rows]

    async def execute_transaction(self, queries: List[tuple]) -> List[Any]:
        """Execute multiple queries in a transaction"""
        if self.pool is None:
            await self.initialize()

        async with self.pool.acquire() as connection:
            async with connection.transaction():
                results = []
                for query, args in queries:
                    result = await connection.fetch(query, *args)
                    results.append([dict(row) for row in result])
                return results

    async def close(self):
        """Close connection pool"""
        if self.pool:
            await self.pool.close()
            self.pool = None
```

### Efficient Data Serialization

```python
# serialization.py
import orjson  # Faster JSON library
import pickle
import gzip
from typing import Any, Union

class OptimizedSerializer:
    """High-performance data serialization"""

    @staticmethod
    def serialize_json(data: Any) -> bytes:
        """Fast JSON serialization"""
        return orjson.dumps(data)

    @staticmethod
    def deserialize_json(data: bytes) -> Any:
        """Fast JSON deserialization"""
        return orjson.loads(data)

    @staticmethod
    def serialize_compressed(data: Any) -> bytes:
        """Compressed pickle serialization"""
        pickled = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
        return gzip.compress(pickled)

    @staticmethod
    def deserialize_compressed(data: bytes) -> Any:
        """Compressed pickle deserialization"""
        decompressed = gzip.decompress(data)
        return pickle.loads(decompressed)

    @staticmethod
    def choose_serialization(data: Any) -> tuple[bytes, str]:
        """Choose optimal serialization method"""
        # Try JSON first (faster, more compatible)
        try:
            json_data = OptimizedSerializer.serialize_json(data)
            compressed_data = OptimizedSerializer.serialize_compressed(data)

            # Use JSON if it's not much larger
            if len(json_data) <= len(compressed_data) * 1.2:
                return json_data, 'json'
            else:
                return compressed_data, 'compressed'

        except (TypeError, ValueError):
            # Fall back to compressed pickle
            return OptimizedSerializer.serialize_compressed(data), 'compressed'
```

## Infrastructure Optimization

### Process Management

```python
# process_manager.py
import asyncio
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
from typing import Any, Callable, List

class OptimizedProcessManager:
    """Optimized process management for CPU-intensive tasks"""

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or mp.cpu_count()
        self.executor = None

    async def __aenter__(self):
        self.executor = ProcessPoolExecutor(max_workers=self.max_workers)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.executor:
            self.executor.shutdown(wait=True)

    async def run_cpu_intensive(self, func: Callable, *args, **kwargs) -> Any:
        """Run CPU-intensive function in separate process"""
        if self.executor is None:
            async with self:
                return await self._execute(func, *args, **kwargs)
        else:
            return await self._execute(func, *args, **kwargs)

    async def _execute(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function in process pool"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            lambda: func(*args, **kwargs)
        )

    async def map_parallel(self, func: Callable, items: List[Any]) -> List[Any]:
        """Map function over items in parallel"""
        if self.executor is None:
            async with self:
                return await self._map_execute(func, items)
        else:
            return await self._map_execute(func, items)

    async def _map_execute(self, func: Callable, items: List[Any]) -> List[Any]:
        """Execute map in process pool"""
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(self.executor, func, item)
            for item in items
        ]
        return await asyncio.gather(*tasks)
```

### Memory Management

```python
# memory_manager.py
import gc
import psutil
import os
from typing import Dict, Any

class MemoryManager:
    """Advanced memory management utilities"""

    def __init__(self, warning_threshold: float = 0.8, critical_threshold: float = 0.9):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.process = psutil.Process(os.getpid())

    def get_memory_info(self) -> Dict[str, Any]:
        """Get detailed memory information"""
        memory_info = self.process.memory_info()
        memory_percent = self.process.memory_percent()

        return {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024,
            "percent": memory_percent,
            "available_mb": psutil.virtual_memory().available / 1024 / 1024,
            "warning_level": self._get_warning_level(memory_percent)
        }

    def _get_warning_level(self, memory_percent: float) -> str:
        """Get memory warning level"""
        if memory_percent >= self.critical_threshold * 100:
            return "critical"
        elif memory_percent >= self.warning_threshold * 100:
            return "warning"
        else:
            return "normal"

    def cleanup_memory(self, force: bool = False) -> Dict[str, Any]:
        """Perform memory cleanup"""
        before_info = self.get_memory_info()

        # Force garbage collection
        collected = gc.collect()

        # Additional cleanup for critical memory usage
        if force or before_info["warning_level"] == "critical":
            # Clear caches
            self._clear_internal_caches()

            # Force another GC cycle
            collected += gc.collect()

        after_info = self.get_memory_info()

        return {
            "before_mb": before_info["rss_mb"],
            "after_mb": after_info["rss_mb"],
            "freed_mb": before_info["rss_mb"] - after_info["rss_mb"],
            "objects_collected": collected,
            "warning_level": after_info["warning_level"]
        }

    def _clear_internal_caches(self):
        """Clear internal caches"""
        # Clear function caches
        import functools
        for obj in gc.get_objects():
            if isinstance(obj, functools._lru_cache_wrapper):
                obj.cache_clear()

        # Clear regex cache
        import re
        re.purge()

    def monitor_memory(self) -> bool:
        """Monitor memory and return True if action needed"""
        info = self.get_memory_info()

        if info["warning_level"] == "critical":
            logger.warning(
                "Critical memory usage detected",
                memory_mb=info["rss_mb"],
                percent=info["percent"]
            )
            return True
        elif info["warning_level"] == "warning":
            logger.info(
                "High memory usage detected",
                memory_mb=info["rss_mb"],
                percent=info["percent"]
            )

        return False
```

## Configuration Optimization

### Production Configuration

```json
{
  "performance": {
    "agents": {
      "max_concurrent": 5,
      "memory_limit_mb": 512,
      "cleanup_interval": 10,
      "max_history_length": 50
    },
    "llm": {
      "connection_pool_size": 10,
      "request_timeout": 30,
      "retry_attempts": 3,
      "batch_size": 5,
      "cache_enabled": true,
      "cache_ttl": 3600
    },
    "tools": {
      "concurrent_limit": 10,
      "cache_enabled": true,
      "cache_size": 1000,
      "timeout": 30
    },
    "database": {
      "pool_min_size": 5,
      "pool_max_size": 20,
      "command_timeout": 60,
      "connection_timeout": 10
    }
  }
}
```

### Environment Variables

```bash
# Performance tuning
export PYTHONOPTIMIZE=1
export PYTHONDONTWRITEBYTECODE=1
export PYTHONUNBUFFERED=1

# Memory management
export MALLOC_TRIM_THRESHOLD_=100000
export MALLOC_MMAP_THRESHOLD_=131072

# Async settings
export PYTHONASYNCIODEBUG=0
export UV_THREADPOOL_SIZE=16

# Logging optimization
export LOG_LEVEL=INFO
export LOG_FORMAT=json
```

## Monitoring and Alerting

### Performance Alerts

```python
# alerts.py
import asyncio
from typing import Dict, Callable, Any

class PerformanceAlerting:
    """Performance monitoring and alerting"""

    def __init__(self):
        self.thresholds = {
            "memory_percent": 80,
            "cpu_percent": 80,
            "response_time": 5.0,
            "error_rate": 0.1
        }
        self.alert_handlers = []

    def add_alert_handler(self, handler: Callable[[str, Dict[str, Any]], None]):
        """Add alert handler function"""
        self.alert_handlers.append(handler)

    async def check_performance(self, metrics: Dict[str, Any]):
        """Check performance metrics against thresholds"""
        alerts = []

        # Check memory usage
        if metrics.get("memory_percent", 0) > self.thresholds["memory_percent"]:
            alerts.append({
                "type": "memory_high",
                "value": metrics["memory_percent"],
                "threshold": self.thresholds["memory_percent"],
                "message": f"Memory usage {metrics['memory_percent']:.1f}% exceeds threshold"
            })

        # Check CPU usage
        if metrics.get("cpu_percent", 0) > self.thresholds["cpu_percent"]:
            alerts.append({
                "type": "cpu_high",
                "value": metrics["cpu_percent"],
                "threshold": self.thresholds["cpu_percent"],
                "message": f"CPU usage {metrics['cpu_percent']:.1f}% exceeds threshold"
            })

        # Check response time
        if metrics.get("avg_response_time", 0) > self.thresholds["response_time"]:
            alerts.append({
                "type": "response_time_high",
                "value": metrics["avg_response_time"],
                "threshold": self.thresholds["response_time"],
                "message": f"Response time {metrics['avg_response_time']:.2f}s exceeds threshold"
            })

        # Send alerts
        for alert in alerts:
            await self._send_alert(alert)

    async def _send_alert(self, alert: Dict[str, Any]):
        """Send alert to all handlers"""
        for handler in self.alert_handlers:
            try:
                await handler(alert["type"], alert)
            except Exception as e:
                logger.error(f"Alert handler failed: {e}")
```

## Best Practices

### Code Optimization
- Use async/await for I/O operations
- Implement connection pooling
- Cache expensive computations
- Batch similar operations
- Use efficient data structures

### Memory Management
- Limit conversation history
- Implement periodic cleanup
- Monitor memory usage
- Use generators for large datasets
- Clear caches regularly

### Network Optimization
- Use connection pooling
- Implement request batching
- Enable compression
- Set appropriate timeouts
- Handle rate limiting

### Database Optimization
- Use connection pooling
- Implement query caching
- Optimize query patterns
- Use transactions efficiently
- Monitor query performance

## See Also

- [Debugging Guide](./debugging.md)
- [Common Issues](./common-issues.md)
- [System Requirements](../getting-started/installation.md)"}

---

FILE: README.md

# SpoonOS Developer Documentation

Welcome to the **SpoonOS Developer Documentation** - the comprehensive guide for building, deploying, and scaling Web3 AI agents with the SpoonOS Agentic Operating System.

This documentation site provides everything you need to get started with SpoonOS, the **Agentic OS for a Sentient Economy**.

## 🚀 What is SpoonOS?

SpoonOS is an **Agentic Operating System that enables AI agents to perceive, reason, plan, and execute**. It provides a robust framework for creating, deploying, and managing Web3 AI agents. SpoonOS fosters interoperability, data scalability, and privacy—empowering AI agents to engage in collaborative learning while ensuring secure and efficient data processing.

### Key Features

- 🧠 **Intelligent Agent Framework** - Advanced reasoning and action capabilities
- 🌐 **Web3-Native Architecture** - Built-in blockchain and DeFi integration
- 🔧 **Modular Tool System** - Extensible architecture with MCP protocol support
- 🤖 **Multi-Model AI Support** - Compatible with OpenAI, Anthropic, DeepSeek, and more
- 📊 **Graph-Based Workflows** - Complex workflow orchestration and management
- 💻 **Interactive CLI** - Powerful development and deployment interface
- 🔒 **Privacy-First Design** - Secure data processing and collaborative learning
- ⚡ **High Performance** - Optimized for scalability and efficiency

## 📚 Documentation Structure

- **Getting Started** - Installation, configuration, and quick start
- **Core Concepts** - Agents, tools, and system architecture
- **Guides** - Step-by-step tutorials and best practices
- **API Reference** - Complete API documentation
- **Examples** - Real-world use cases and sample code

## 🛠️ Development

### Installation

```bash
npm install
```

### Local Development

```bash
npm start
```

This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.

### Build

```bash
npm run build
```

This command generates static content into the `build` directory and can be served using any static contents hosting service.

### Deployment

Using SSH:

```bash
USE_SSH=true npm run deploy
```

Not using SSH:

```bash
GIT_USER=<Your GitHub username> npm run deploy
```

If you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.

## 🤝 Contributing

We welcome contributions to improve the documentation! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## 📄 License

This documentation is part of the SpoonOS project and follows the same license terms.

## 🔗 Links

- **Main Repository**: [XSpoonAi/spoon-core](https://github.com/XSpoonAi/spoon-core)
- **Official Website**: [SpoonOS Landing](https://spoonai.io)
- **Documentation**: [Developer Documentation](https://xspoonai.github.io/spoon-doc/)
- **Community**: [GitHub Discussions](https://github.com/XSpoonAi/spoon-core/discussions)

---

**Build, deploy and scale Web3 AI agents in an evolving paradigm.**

Built with ❤️ by the SpoonOS Team

---

FILE: src/pages/markdown-page.md

---
title: Markdown page example
---

# Markdown page example

You don't need React to write simple standalone pages.

---

FILE: Toolkit/crypto/data-tools.md

`spoon_toolkits.crypto.crypto_data_tools` is SpoonAI’s Bitquery-centric toolbox for price discovery, liquidity intelligence, lending-rate aggregation, and wallet forensics. Every class in this package plugs directly into the Spoon tool runtime, so agents can reuse the same `BaseTool` lifecycle (validation, structured outputs, telemetry) without additional glue code.


## Environment & Dependencies

```bash
export BITQUERY_API_KEY=your_rest_key              # reserved for future REST helpers
export BITQUERY_CLIENT_ID=your_oauth_client_id     # required by Bitquery OAuth
export BITQUERY_CLIENT_SECRET=your_oauth_secret
export RPC_URL=https://eth.llamarpc.com
```

- `Get*` price tools pull the RPC URL from the environment automatically; alert helpers default to a bundled Alchemy mainnet URL unless you pass a custom endpoint when instantiating them.
- `PredictPrice` requires `pandas`, `scikit-learn`, and `numpy`. Install toolkit extras via `pip install -r requirements.txt` at the project root or install modules individually.
- `LendingRateMonitorTool` performs concurrent HTTP requests using `aiohttp`; event loops must be active (use `nest_asyncio` in notebooks if necessary).
- `SuddenPriceIncreaseTool` currently relies exclusively on CoinGecko’s REST feed; the Bitquery enrichment hook exists in code but is not wired up yet, so providing `BITQUERY_API_KEY` has no effect today.

## Tool Catalog 

### Spot & Historical Pricing
- `GetTokenPriceTool` resolves supported ERC-20 pairs (ETH, USDC, USDT, DAI, WBTC by default) to the canonical Uniswap v3 pool, fetches slot0, and converts ticks into human prices.
- `Get24hStatsTool` uses the same provider stack for downstream analytics, while `GetKlineDataTool` is currently a placeholder that returns an empty list until a Graph/event-log integration ships.

### Alerting & Monitoring
- `PriceThresholdAlertTool` compares live prices with a configurable ±% drift versus prior day.
- `LpRangeCheckTool` reads Uniswap positions, current ticks, and warns when LPs approach range boundaries (`buffer_ticks`).
- `SuddenPriceIncreaseTool` filters CoinGecko’s REST feed for large-cap, high-volume tokens with rapid appreciation; Bitquery enrichment is planned but not enabled.
- `CryptoMarketMonitor` POSTs well-formed payloads to the monitoring daemon bundled at `http://localhost:8888/monitoring/tasks` (`blockchain_monitor.py`). Change the URL in that module if your scheduler lives elsewhere.

### Wallet Intelligence
- `WalletAnalysis`, `TokenHolders`, and `TradingHistory` share the Bitquery GraphQL templates embedded in their modules so you always know the dataset (`combined` vs `archive`) and filters before execution.
- Pagination and filters live directly inside each template string (e.g., `TokenHolders` enforces `Amount >= 10,000,000`). Edit the template if you need a different threshold or window; no exposed class constants exist today. These helpers return the Bitquery JSON fragments directly rather than wrapping results in `ToolResult`.

### DeFi Rates & Liquidity
- `LendingRateMonitorTool` merges DeFiLlama pools with first-party Aave subgraphs today (Compound/Morpho data arrives via DeFiLlama’s feed only), derives utilization, and wraps the result in `ToolResult`. Every response also includes an `arbitrage_opportunities` array summarizing large APY spreads.
- `UniswapLiquidity` emits the latest Mint/Burn events so you can approximate real-time liquidity deltas without running a listener yourself.
- `PredictPrice` is intentionally heavyweight (builds a RandomForest and scaler); cache the fitted estimator in your application if you call it frequently.

## Usage Patterns

### Synchronous price lookup
```python
from spoon_toolkits.crypto.crypto_data_tools import GetTokenPriceTool

tool = GetTokenPriceTool()
result = await tool.execute(symbol="ETH-USDC")

print(result.output["price"])         # structured response
```

### Long-running alert inside an agent
```python
import asyncio
from spoon_toolkits.crypto.crypto_data_tools import PriceThresholdAlertTool

alerts = PriceThresholdAlertTool()

async def monitor():
    while True:
        tool_result = await alerts.execute(symbol="ETH-USDC", threshold_percent=7)
        if tool_result.output.get("exceeded"):
            await send_notification(tool_result.output["message"])
        await asyncio.sleep(60)

asyncio.run(monitor())
```

Tips:
- Synchronous price tools return dictionaries nested inside `ToolResult.output`, whereas the Bitquery GraphQL helpers (`WalletAnalysis`, `TokenHolders`, `TradingHistory`, `UniswapLiquidity`) currently return the raw template output.
- Async utilities (`PriceThresholdAlertTool`, `LendingRateMonitorTool`) already shield you from rate spikes with short sleeps; avoid spawning excessive parallel loops unless you also raise Bitquery limits.

## Operational Notes

- Centralize credential management: add the Bitquery OAuth keys to your `.env` and load them before instantiating tools to prevent runtime `ValueError`s from `DexBaseTool.oAuth()`.
- Because Uniswap calls hit your RPC, failures there do **not** imply Bitquery downtime—inspect the `error` string inside `ToolResult.output` or the returned dictionary to isolate the failure domain.
- The monitoring helper in `blockchain_monitor.py` assumes a scheduler reachable at `http://localhost:8888/monitoring/tasks`. Override `api_url` in the module before deploying if your infra differs.
- `PredictPrice` fetches up to ~1000 rows per query. If Bitquery throttles you, lower the template limit or use a paid API plan.

---

FILE: Toolkit/crypto/evm.md

`spoon_toolkits.crypto.evm` gathers the core on-chain primitives agents need—native transfers, ERC-20 operations, swaps, quotes, bridges, and balance lookups—without shelling out to the JS plugin. Each class inherits `BaseTool`, so input validation, diagnostics, and async receipt waiting all behave consistently across Spoon agents.

## Environment & Dependencies

```bash
export EVM_PROVIDER_URL=https://mainnet.infura.io/v3/...
export EVM_PRIVATE_KEY=0xyourSignerKey
# Optional global fallback used by other crypto toolkits
export RPC_URL=https://eth.llamarpc.com
```

- All tools accept an `rpc_url` override, but only transaction senders (`EvmTransferTool`, `EvmErc20TransferTool`, `EvmSwapTool`, `EvmBridgeTool`) take `private_key`/signer inputs; read-only helpers rely solely on RPC access.
- `web3.py` is lazily imported inside each execute path, so include it (and `requests`) in your runtime environment.
- Aggregator-backed tools call public REST APIs (Bebop or LiFi). If you operate behind a proxy, ensure outbound HTTPS is allowed.

## Quick Start – Native Transfer

```python
from spoon_toolkits.crypto.evm import EvmTransferTool

transfer = EvmTransferTool()
tx = await transfer.execute(
    to_address="0xrecipient...",
    amount_ether="0.05",
    data="0x",                      # optional calldata
)

print(tx.output["hash"])
```

Receipts are awaited with reasonable timeouts; if the transaction reverts, `ToolResult.error` includes the tx hash so you can inspect it with an explorer.

## Toolkit Overview

| Tool | Module | Purpose |
| --- | --- | --- |
| `EvmTransferTool` | `transfer.py` | Send native tokens with optional data payloads, auto gas estimation, and nonce management. |
| `EvmErc20TransferTool` | `erc20.py` | Transfer ERC-20 tokens (balanceOf/decimals aware) with gas estimation and optional gas price overrides. |
| `EvmBalanceTool` | `balance.py` | Read native or ERC-20 balances for any address. |
| `EvmSwapTool` | `swap.py` | Execute same-chain swaps through the Bebop aggregator, including approvals when needed. |
| `EvmSwapQuoteTool` | `quote.py` | Fetch swap quotes without execution; compares Bebop and LiFi outputs when requested. |
| `EvmBridgeTool` | `bridge.py` | Bridge assets across chains via LiFi advanced routes and step transactions. |

## Tool Notes

### `EvmTransferTool`
- Defaults to `EVM_PROVIDER_URL`/`EVM_PRIVATE_KEY` but also checks `RPC_URL` so the tool can run inside broader Spoon stacks.
- Automatically estimates gas; if estimation fails it falls back to 21k for plain transfers or 100k when `data` is present.
- Explicit overrides exist for `gas_limit`, `gas_price_gwei`, and `nonce` so advanced workflows (batched transactions, EOA abstraction) remain possible.

### `EvmErc20TransferTool`
- Resolves token decimals via the contract; if the call fails it assumes 18 decimals, so pass `amount` accordingly.
- Builds and signs a `transfer` call, then waits for completion. Emits `{hash, token, amount, decimals}` in `output`.
- Accepts `gas_price_gwei`; otherwise uses the RPC’s `gas_price`.

### `EvmBalanceTool`
- Returns native balances (Ether-equivalent) when `token_address` is omitted, or ERC-20 balances converted using on-chain decimals. Outputs include only the formatted balance value (no decimals field).
- Useful for guardrails before making transfers or swaps.

### `EvmSwapTool`
- Calls Bebop’s router for supported chains (`1`, `10`, `137`, `42161`, `8453`, `59144`). Unsupported IDs return a descriptive error.
- Handles ERC-20 approvals automatically by checking allowance against `approvalTarget` and submitting an `approve` tx when required.
- Accepts `gas_price_gwei` overrides; otherwise reuses the aggregator-suggested gas price or the RPC default.
- `slippage_bps` is currently informational because Bebop does not accept slippage; expect fills to match aggregator routing.

### `EvmSwapQuoteTool`
- Works in quote-only contexts where you want to compare outputs without signing anything.
- Resolve chain ID via `chain_id` parameter or by pointing `rpc_url` at the target network.
- `aggregator="both"` (default) fetches Bebop and LiFi; response includes `quotes` plus a precalculated `best` entry ranked by min output amount.
- Requires RPC access for ERC-20 decimals when quoting token sales; native sells use zero address.

### `EvmBridgeTool`
- Wraps LiFi’s `/advanced/routes` + `/advanced/stepTransaction`. After selecting the recommended path, it executes the first step locally and waits for the on-chain receipt.
- Performs ERC-20 approvals before bridging when necessary; approvals and bridge txs both honor `gas_price_gwei`.
- `from_address` is implied from the signer, while `to_address` defaults to the same account but can target any destination address.
- Returns hashes plus source/destination chain IDs so orchestrators can track the bridge until completion on the far chain.

## Operational Tips

- Centralize signer management: store hot keys in a secure vault and inject them as environment variables just before launching the agent process.
- Throttle swap/bridge calls if you expect to fire many approvals quickly; both Bebop and LiFi enforce rate limits.
- Log the `ToolResult` payloads—especially `output["hash"]`—so you can reconcile actions if an agent crashes mid-run.
- When rotating RPC endpoints, prefer passing `rpc_url` explicitly to keep observability clear (environment-based fallbacks can mask misconfiguration).

---

FILE: Toolkit/crypto/neo.md

`spoon_toolkits.crypto.neo` bundles async Neo N3 helpers on top of the `neo-mamba` RPC client, covering addresses, assets, blocks, governance, and smart-contract introspection. Every tool subclasses `BaseTool` so agents get consistent parameter validation and `ToolResult` payloads.

## Environment & Network Selection

```bash
export NEO_MAINNET_RPC=https://mainmagnet.ngd.network:443   # overrides default mainnet RPC
export NEO_TESTNET_RPC=https://testmagnet.ngd.network:443   # overrides default testnet RPC
export NEO_RPC_TIMEOUT=20                                   # seconds (defaults to 60 if unset)
export NEO_RPC_ALLOW_INSECURE=false                         # set true to skip TLS verification
```

- Each tool accepts `network` (`"mainnet"` / `"testnet"`) so you can switch clusters per call. If you omit it, `NeoProvider` defaults to testnet.
- Address inputs may be Base58 (e.g., `Nf2C...`) or `0x` script hashes; `NeoProvider` normalizes them internally before hitting RPCs.
- `NEO_RPC_TIMEOUT` defaults to 60 seconds in `NeoProvider`. Set the env var to override that per deployment.

## Toolkit Map

| Module | Highlights |
| --- | --- |
| `address_tools.py` | Address counts/info, RPC validation, per-contract NEP-17 sent/received totals, and paginated transfer history. |
| `asset_tools.py` | Asset metadata by hash/name, bulk metadata lookups, and contract/address holding snapshots via `GetAssetInfoByAssetAndAddress`. |
| `block_tools.py` | Block count, block-by-height/hash, best-hash, rewards, and rolling recent block summaries. |
| `transaction_tools.py` | Transaction counts, raw tx retrieval (by block hash/height/address/transaction hash), and transfer event extraction. |
| `contract_tools.py` / `sc_call_tools.py` | Contract inventories, verified contract metadata, and smart-contract call traces filtered by contract/address/tx. |
| `nep_tools.py` | Specialized NEP-11/NEP-17 transfer feeds (by address, block height, contract hash) plus balance helpers. |
| `governance_tools.py` / `voting_tools.py` | Committee info, candidate/voter tallies, on-chain vote call traces. |
| `log_state_tools.py` | Application log/state fetchers for debugging contract execution. |
| `neo_provider.py`, `base.py` | Shared provider, RPC request helpers, normalization utilities, and graceful fallbacks when neo-mamba lacks a direct method. |

###  Official N3 RPC helpers

| Tool | RPC | Description |
| --- | --- | --- |
| `ValidateAddressTool` (`address_tools.py`) | `validateaddress` | Calls the official RPC to confirm if an address/script hash is valid for the selected network and returns metadata. |


## Usage Patterns

> **Note on outputs:** every `BaseTool` wraps the payload in a formatted string such as `"Address info: {...}"`. When you need the underlying dict, strip the prefix and parse it (e.g., with `ast.literal_eval` or `json.loads` after replacing single quotes).

### Address intelligence
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import GetAddressInfoTool

tool = GetAddressInfoTool()
result = await tool.execute(
    address="Nf2CXE8s1R6yoZ6e52xX5yb7Z9Uv7S3N1h",
    network="mainnet",
)
prefix, _, payload = result.output.partition(": ")
balances = literal_eval(payload)  # includes NEP-17 balances + script hash
```

```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import ValidateAddressTool

validation = await ValidateAddressTool().execute(
    address="NaU3shtZqnR1H6XnDTxghorgkXN687C444",
    network="testnet",
)
_, _, payload = validation.output.partition(": ")
metadata = literal_eval(payload)  # {"isvalid": True, "address": "...", ...}
```

### Asset metadata & holdings
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import (
    GetAssetInfoByHashTool,
    GetAssetInfoByAssetAndAddressTool,
)

asset_info = await GetAssetInfoByHashTool().execute(
    asset_hash="0xef4073a0f2b305a38ec4050e4d3d28bc40ea63f5",  # GAS
    network="mainnet",
)
_, _, asset_payload = asset_info.output.partition(": ")
asset_dict = literal_eval(asset_payload)

holding = await GetAssetInfoByAssetAndAddressTool().execute(
    asset_hash="0xef4073a0f2b305a38ec4050e4d3d28bc40ea63f5",
    address="Nf2CXE8s1R6yoZ6e52xX5yb7Z9Uv7S3N1h",
    network="mainnet",
)
_, _, holding_payload = holding.output.partition(": ")
holding_dict = literal_eval(holding_payload)  # contract/address balance snapshot
```

### Block & transaction lookups
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import (
    GetBlockByHeightTool,
    GetRawTransactionByTransactionHashTool,
)

block = await GetBlockByHeightTool().execute(block_height=4500000, network="mainnet")
_, _, block_payload = block.output.partition(": ")
block_dict = literal_eval(block_payload)

tx = await GetRawTransactionByTransactionHashTool().execute(
    transaction_hash="0x...",
    network="mainnet",
)
_, _, tx_payload = tx.output.partition(": ")
tx_dict = literal_eval(tx_payload)
```

### Governance snapshots
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import GetCommitteeInfoTool, GetVotesByCandidateAddressTool

committee = await GetCommitteeInfoTool().execute(network="mainnet")
_, _, committee_payload = committee.output.partition(": ")
committee_dict = literal_eval(committee_payload)

votes = await GetVotesByCandidateAddressTool().execute(candidate_address="NVSX...", network="mainnet")
_, _, votes_payload = votes.output.partition(": ")
votes_dict = literal_eval(votes_payload)
```

### Token volume totals per address & contract
```python
from ast import literal_eval
from spoon_toolkits.crypto.neo import GetTotalSentAndReceivedTool

volume = await GetTotalSentAndReceivedTool().execute(
    contract_hash="0xef4073a0f2b305a38ec4050e4d3d28bc40ea63f5",
    address="Nf2CXE8s1R6yoZ6e52xX5yb7Z9Uv7S3N1h",
    network="mainnet",
)
_, _, volume_payload = volume.output.partition(": ")
volume_dict = literal_eval(volume_payload)  # {"received": "...", "sent": "..."}
```
## Operational Notes

- Always `await` tool execution; the provider relies on async context managers to open/close RPC sessions cleanly. If you compose multiple calls, instantiate the tool once and re-use it to avoid repeated class construction overhead.

---

FILE: Toolkit/crypto/powerdata.md

`spoon_toolkits.crypto.crypto_powerdata` fuses CCXT-powered CEX feeds, OKX Web3 DEX data, TA-Lib/enhanced indicators, and an MCP server that can stream results over stdio or SSE. Use it when agents need richer analytics than simple price lookups.

## Environment & Settings

```bash
export OKX_API_KEY=...
export OKX_SECRET_KEY=...
export OKX_API_PASSPHRASE=...
export OKX_PROJECT_ID=...
export OKX_BASE_URL=https://web3.okx.com/api/v5/   # optional override

# Optional overrides (defaults shown)
export RATE_LIMIT_REQUESTS_PER_SECOND=10
export MAX_RETRIES=3
export RETRY_DELAY=1.0
export TIMEOUT_SECONDS=30
```

`data_provider.Settings` ingests these variables (plus indicator defaults such as SMA/EMA periods). Missing OKX keys raise immediately before any HTTP call, so configure them centrally—either via environment or by passing `env_vars` into the MCP helpers.

## What’s Inside the Toolkit

<table>
  <colgroup>
    <col style={{ width: "22%" }} />
    <col style={{ width: "22%" }} />
    <col style={{ width: "56%" }} />
  </colgroup>
  <thead>
    <tr>
      <th>Component</th>
      <th>File(s)</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>`CryptoPowerDataCEXTool`</td>
      <td>`tools.py`</td>
      <td>Pull OHLCV candles from 100+ CCXT exchanges and pipe them through the enhanced indicator stack.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataDEXTool`</td>
      <td>`tools.py`</td>
      <td>Hit OKX Web3 DEX APIs for on-chain pairs specified by `chain_index` + token address.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataPriceTool`</td>
      <td>`tools.py`</td>
      <td>Lightweight spot price snapshot (CEX or DEX) without fetching an entire candle set.</td>
    </tr>
    <tr>
      <td>`CryptoPowerDataIndicatorsTool`</td>
      <td>`tools.py`</td>
      <td>Enumerate every indicator name/parameter accepted by the enhanced TA registry (TA-Lib + custom extras).</td>
    </tr>
    <tr>
      <td>`Settings`, `OKXDEXClient`, `TechnicalAnalysis`</td>
      <td>`data_provider.py`</td>
      <td>Central place for rate limiting, retries, authenticated OKX calls, and TA-Lib helpers.</td>
    </tr>
    <tr>
      <td>MCP server runners (`start_crypto_powerdata_mcp_*`)</td>
      <td>`server.py`, `dual_transport_server.py`</td>
      <td>Start stdio or HTTP/SSE transports so UI agents can subscribe to continuous feeds.</td>
    </tr>
    <tr>
      <td>Analytics core</td>
      <td>`main.py`, `enhanced_indicators.py`, `talib_registry.py`</td>
      <td>Parse indicator configs, register TA functions, and expose them via FastMCP tools.</td>
    </tr>
  </tbody>
</table>

All tools inherit `CryptoPowerDataBaseTool`, which lazily initializes global settings and reuses throttled clients; you rarely need to micromanage sessions yourself.

## Indicator Configuration Cheatsheet

- Accepts either JSON strings (most MCP clients) or native dicts. Double-encoded JSON like `"\"{\\\"ema\\\": ...}\""` is auto-decoded.
- Mix-and-match multiple parameters per indicator:  
  `{"ema": [{"timeperiod": 12}, {"timeperiod": 26}], "macd": [{"fastperiod": 12, "slowperiod": 26, "signalperiod": 9}]}`
- Enhanced registry supports 150+ TA-Lib functions plus custom composites (VWAP, BB width/position, Aroon oscillators, etc.).
- Validation errors bubble back as descriptive `ToolResult.error` messages so you can surface them directly to users.

## Usage Patterns

### CEX candles + indicators

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataCEXTool

tool = CryptoPowerDataCEXTool()
result = await tool.execute(
    exchange="binance",
    symbol="BTC/USDT",
    timeframe="1h",
    limit=200,
    indicators_config='{"ema": [{"timeperiod": 12}, {"timeperiod": 26}], "rsi": [{"timeperiod": 14}]}',
)

ohlcv_rows = result.output            # only the candle rows are returned
# metadata is not exposed by the ToolResult wrapper yet
```

### DEX analytics on OKX Web3

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataDEXTool

dex = CryptoPowerDataDEXTool()
result = await dex.execute(
    chain_index="1",                      # Ethereum
    token_address="0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",  # WETH
    timeframe="1H",
    limit=150,
    indicators_config='{"macd": [{"fastperiod": 12, "slowperiod": 26, "signalperiod": 9}], "bb": [{"period": 20, "std": 2}]}',
)

candles = result.output               # ToolResult output already contains the payload
```

### Real-time price snapshot

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataPriceTool

price_tool = CryptoPowerDataPriceTool()
btc = await price_tool.execute(source="cex", exchange="okx", symbol="BTC/USDT")
dex_price = await price_tool.execute(source="dex", chain_index="42161", token_address="0xFF970A61A04b1cA14834A43f5de4533eBDDB5CC8")
```

### Discover supported indicators

```python
from spoon_toolkits.crypto.crypto_powerdata import CryptoPowerDataIndicatorsTool

catalog = await CryptoPowerDataIndicatorsTool().execute()
print(catalog.output["indicators"])   # list of indicator metadata with defaults
```

## MCP Server & Streaming

- `start_crypto_powerdata_mcp_stdio(env_vars=...)` spins up a FastMCP stdio server; pass `background=True` if you need it alongside other async work.
- `start_crypto_powerdata_mcp_sse(host, port, env_vars)` exposes identical tools over HTTP + Server-Sent Events (see `dual_transport_server.py` for endpoints `/mcp` and `/health`).
- `start_crypto_powerdata_mcp_auto` picks stdio vs SSE automatically based on environment; handy for container images.
- `CryptoPowerDataMCPServer` keeps track of running threads so you can query `status()` or stop everything on shutdown.
- `mcp_bridge.py` wires FastMCP methods into the dual transport so CLI agents and browser extensions consume the same tool definitions.

The current HTTP/SSE server keeps an `/mcp` SSE connection alive with heartbeats, but tool responses are delivered via JSON-RPC POST replies rather than pushed over the SSE channel. Keep calling the tools periodically if you need fresh data; the same OKX rate limiting (`rate_limit_requests_per_second`) and retry envelopes apply regardless of transport.

---

FILE: Toolkit/crypto/solana.md

---
id: solana
title: Solana Tools
---

# Solana Tools

`spoon_toolkits.crypto.solana` provides Python-based Solana blockchain tools built on top of `solana-py`, offering native SOL transfers, SPL token operations, and Jupiter-powered token swaps. Populate the tooling constants in `spoon_toolkits.crypto.solana.constants` (token program IDs, common mint addresses) before relying on SPL features or symbol shortcuts—the defaults are intentionally left as `None`.

## Environment & Dependencies

```bash
# Required runtime settings / env vars
export SOLANA_RPC_URL=https://api.mainnet-beta.solana.com
export SOLANA_PRIVATE_KEY=your_base58_or_base64_private_key


# Optional extras
export HELIUS_API_KEY=your_helius_key   # Enables enriched RPC + webhooks
export BIRDEYE_API_KEY=your_birdeye_key # Enables live price + portfolio data
```

The keypair loader accepts both base58 (phantom export) and base64 strings, and you can override any parameter per call via the tool arguments.

**Dependencies:**
- `solana-py` – RPC client used by transfers, swaps, and wallet reads
- `solders` – Fast keypair/pubkey primitives for signing
- `spl.token` – SPL Token program bindings (ATA creation, transfers)
- `httpx` – Async Jupiter and Birdeye integrations

## Tool Catalog

### Transfer Tools

#### `SolanaTransferTool`

Transfer SOL or SPL tokens to another address.

**Parameters:**
- `recipient` (str, **required**) - Destination Solana address
- `amount` (str/number, **required**) - Amount in human-readable units
- `token_address` (str, optional) - SPL token mint address; omit for SOL
- `rpc_url` (str, optional) - RPC endpoint override
- `private_key` (str, optional) - Sender private key override

**Returns:**
```python
ToolResult(output={
    "success": True,
    "signature": "5j7s...",
    "amount": "1.5",
    "recipient": "9jW8F..."
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaTransferTool

# Transfer SOL
transfer_tool = SolanaTransferTool()
result = await transfer_tool.execute(
    recipient="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    amount="0.1"
)
print(f"Signature: {result.output['signature']}")

# Transfer SPL token (USDC)
result = await transfer_tool.execute(
    recipient="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    amount="100",
    token_address="EPjFW..."
)
```

**Features:**
-  Automatic ATA (Associated Token Account) creation for recipients
-  Transfers execute immediately after submission; confirmations and decimal precision checks should be handled by the caller if needed.

### Swap Tools

#### `SolanaSwapTool`

Execute token swaps using Jupiter aggregator with intelligent token resolution.

**Parameters:**
- `input_token` (str, **required**) - Input token (symbol/mint/address)
- `output_token` (str, **required**) - Output token (symbol/mint/address)
- `amount` (str/number, **required**) - Amount to swap
- `slippage_bps` (int, optional) - Slippage tolerance in basis points (default: dynamic)
- `priority_level` (str, optional) - Transaction priority: `low`, `medium`, `high`, `veryHigh` (default)
- `rpc_url` (str, optional) - RPC endpoint override
- `private_key` (str, optional) - Wallet private key override

**Returns:**
```python
ToolResult(output={
    "success": True,
    "signature": "3xK9...",
    "input_token": "SOL",
    "input_mint": "So111...",
    "output_token": "USDC",
    "output_mint": "EPjFW...",
    "input_amount": "1.0",
    "output_amount": "150.23",
    "price_impact": 0.002,
    "slippage_bps": 50,
    "route_plan": [...],
    "fees": {"transaction_fee": 5000, "fee_sol": 0.000005}
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaSwapTool

swap_tool = SolanaSwapTool()

# Swap using symbols (automatically resolved from wallet)
result = await swap_tool.execute(
    input_token="SOL",
    output_token="USDC",
    amount="1.0",
    slippage_bps=50,  # 0.5% slippage
    priority_level="high"
)

# Swap using mint addresses
result = await swap_tool.execute(
    input_token="So11111111111111111111111111111111111111112",
    output_token="EPjF...",
    amount="1.0"
)
```

**Advanced Features:**

**1. Smart Token Resolution**

Supports multiple input formats:
- Symbol: `"SOL"`, `"USDC"`, `"$BONK"`
- Mint address: `"So111..."`
- Portfolio lookup: Searches your wallet's token holdings

```python
# All these work:
await swap_tool.execute(input_token="SOL", ...)        # Symbol
await swap_tool.execute(input_token="$BONK", ...)      # From wallet
await swap_tool.execute(input_token="So111...", ...)   # Mint address
```

**2. Priority Fee Levels**

| Level | Max Lamports | Use Case |
|-------|-------------|----------|
| `low` | 50 | Non-urgent swaps |
| `medium` | 200 | Normal operations |
| `high` | 1,000 | Time-sensitive |
| `veryHigh` | 4,000,000 | MEV protection (default) |

**3. Dynamic Quote Context Building**

The tool validates inputs, resolves decimals, and fetches Jupiter quotes before execution:
```python
# Internally handles:
# - Token existence validation
# - Decimal precision checks
# - Amount > 0 validation
# - Slippage bounds (1-10000 bps)
# - Jupiter quote fetching
# - Output amount formatting
```

### Wallet Tools

#### `SolanaWalletInfoTool`

Query comprehensive wallet information including SOL balance and SPL token holdings.

**Parameters:**
- `address` (str, optional) - Wallet address; defaults to configured wallet
- `include_tokens` (bool, optional) - Include SPL token balances (default: `True`)
- `token_limit` (int, optional) - Max tokens to return (default: 20)
- `rpc_url` (str, optional) - RPC endpoint override

**Returns:**
```python
ToolResult(output={
    "address": "9jW8F...",
    "truncated_address": "9jW8...BbCa",
    "sol_balance": 1.523456789,
    "lamports": 1523456789,
    "token_count": 5,
    "tokens": [
        {
            "mint": "EPjFW...",
            "balance": "150.23",
            "decimals": 6,
            "raw_balance": "150230000"
        },
        # ...
    ]
})
```

**Example:**
```python
from spoon_toolkits.crypto.solana import SolanaWalletInfoTool

wallet_tool = SolanaWalletInfoTool()

# Query configured wallet
result = await wallet_tool.execute()
print(f"SOL Balance: {result.output['sol_balance']}")
print(f"Token Count: {result.output['token_count']}")

# Query specific wallet
result = await wallet_tool.execute(
    address="9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa",
    include_tokens=True,
    token_limit=10
)
```

- **Features:**
- ✅ **Wallet Cache Scheduler** – Results are cached per `(rpc_url, address)` so repeated reads avoid RPC calls. The scheduler refresh cadence is 120s by default and can be forced manually.
- ✅ **Token metadata basics** – Each entry includes the mint, UI balance, decimals, and raw balance. Birdeye metadata (names, symbols, USD totals) is not injected automatically—use service helpers to enrich the cached data when an API key is present.
- ✅ **Portfolio cache hook** – The same cache powers the swap helper’s “smart token resolution,” so swaps can reference symbols (`SOL`, `$BONK`, etc.) without extra lookups.
- ✅ **Optional price data** – When `BIRDEYE_API_KEY` is present, the scheduler records token prices and wallet USD totals, which can be consumed through the service helpers.

## Service Helpers

The toolkit provides 30+ utility functions in `service.py`:

### Validation

```python
from spoon_toolkits.crypto.solana import (
    validate_solana_address,
    validate_private_key,
    is_native_sol
)

# Address validation
is_valid = validate_solana_address("9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa")

# Private key validation (base58/base64)
is_valid = validate_private_key("5j7s...")

# Check if token is native SOL
is_sol = is_native_sol("So11111111111111111111111111111111111111112")
```

### Conversion

```python
from spoon_toolkits.crypto.solana import (
    lamports_to_sol,
    sol_to_lamports,
    format_token_amount,
    parse_token_amount
)

# SOL <-> Lamports
lamports = sol_to_lamports(1.5)  # → 1500000000
sol = lamports_to_sol(1500000000)  # → 1.5

# Token amount formatting
ui_amount = format_token_amount(150230000, decimals=6)  # → 150.23
raw_amount = parse_token_amount(150.23, decimals=6)  # → 150230000
```

### Address Utilities

```python
from spoon_toolkits.crypto.solana import (
    get_associated_token_address,
    truncate_address,
    detect_pubkeys_from_string
)

# Get ATA for token
ata = get_associated_token_address(
    token_mint="EPjFW...",
    owner="9jW8F..."
)

# Shorten address for display
short = truncate_address("9jW8FPr6BSSsemWPV22UUCzSqkVdTp6HTyPqeqyuBbCa")
# → "9jW8...BbCa"

# Extract addresses from text
pubkeys = detect_pubkeys_from_string("Send 1 SOL to 9jW8F...")
```

### Error Parsing

```python
from spoon_toolkits.crypto.solana import parse_transaction_error

# Error normalization (currently pass-through)
friendly = parse_transaction_error("Error: 0x1")
# → "Error: 0x1"
```

## Keypair Management

```python
from spoon_toolkits.crypto.solana import (
    get_wallet_keypair,
    get_wallet_key,
    get_private_key,
    get_public_key
)

# Get full keypair (requires private key)
keypair_result = get_wallet_keypair(require_private_key=True)
if keypair_result.keypair:
    print(f"Public Key: {keypair_result.keypair.pubkey()}")

# Get public key only
keypair_result = get_wallet_keypair(require_private_key=False)
if keypair_result.public_key:
    print(f"Public Key: {keypair_result.public_key}")

# Dynamic private key support
keypair_result = get_wallet_key(
    require_private_key=True,
    private_key="5j7s..."  # Override env var
)
```

## Advanced: Wallet Cache Scheduler

**Unique to Python version** - Background service that keeps wallet data fresh.

```python
from spoon_toolkits.crypto.solana import get_wallet_cache_scheduler

scheduler = get_wallet_cache_scheduler()

# Start background refresh (runs every 60s)
await scheduler.ensure_running(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F...",
    include_tokens=True
)

# Get cached data (no RPC call)
cached = await scheduler.get_cached(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F..."
)

if cached:
    wallet_data = cached["data"]
    print(f"SOL: {wallet_data['sol_balance']}")

# Force immediate refresh
fresh_data = await scheduler.force_refresh(
    rpc_url="https://api.mainnet-beta.solana.com",
    wallet_address="9jW8F...",
    include_tokens=True
)
```

**Benefits:**
-  Automatic background updates
-  Instant wallet info access for swap token resolution

## Constants

```python
from spoon_toolkits.crypto.solana import (
    TOKEN_ADDRESSES,
    DEFAULT_SLIPPAGE_BPS,
    JUPITER_PRIORITY_LEVELS
)

# Provide your own well-known addresses; the shipped defaults are None placeholders.
TOKEN_ADDRESSES["USDC"] = "Wdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v"
TOKEN_ADDRESSES["BONK"] = "DezX..."

# Default configuration helpers
printEPjF(DEFAULT_SLIPPAGE_BPS)       # e.g. 100 (1%)
print(JUPITER_PRIORITY_LEVELS)    # {"low": 50, "medium": 200, "high": 1000, "veryHigh": 4000000}
```

## Operational Notes

### Error Handling

Always check `ToolResult.error`:

```python
result = await swap_tool.execute(...)

if result.error:
    print(f" Swap failed: {result.error}")
    if result.diagnostic:
        print(f"Details: {result.diagnostic}")
else:
    print(f" Swap successful: {result.output['signature']}")
```

### Rate Limiting

Jupiter API limits:
- Quote endpoint: ~10 req/s
- Swap endpoint: ~5 req/s

Use the wallet cache scheduler to minimize RPC calls:

```python
# Bad: Queries RPC every time
for token in tokens:
    await wallet_tool.execute(address="...", include_tokens=True)

# Good: Use cache scheduler
scheduler = get_wallet_cache_scheduler()
await scheduler.ensure_running("...", "...", True)
for token in tokens:
    cached = await scheduler.get_cached("...", "...")
```

---

FILE: Toolkit/data-platforms/chainbase.md

`spoon_toolkits.data_platforms.chainbase` wraps the Chainbase REST API in async `BaseTool` classes and mounts them on a FastMCP server, so agents can query block, transaction, account, and token data without writing raw HTTP calls.

## Environment & Configuration

```bash
export CHAINBASE_API_KEY=your_chainbase_key             # required for every request
export CHAINBASE_HOST=0.0.0.0                           # optional when running the MCP server
export CHAINBASE_PORT=8000                              # optional (default 8000)
export CHAINBASE_PATH=/sse                              # optional SSE path
```

- Every tool loads `CHAINBASE_API_KEY` at execution time and aborts with a descriptive error if it is missing.
- HTTP calls go to `https://api.chainbase.online/v1/...`; you can override host/port/path only when launching the bundled FastMCP server.

## Package Layout

| Module | Purpose |
| --- | --- |
| `chainbase_tools.py` | Source of the `BaseTool` implementations (`GetLatestBlockNumberTool`, `GetAccountTokensTool`, `ContractCallTool`, etc.). |
| `balance.py`, `basic.py`, `token_api.py` | Individual FastMCP sub-servers exposing account, base, and token endpoints as MCP tools/resources (with docs baked in). |
| `__init__.py` | Aggregates all MCP sub-servers via `FastMCP` and re-exports the tool classes. Running the module spins up an SSE server. |
| `README.md` | Lists supported chains (`chain_id` values) and shows quick-start snippets. |

## Tooling Highlights

### Blocks & Transactions
- `GetLatestBlockNumberTool` – current block height by `chain_id`.
- `GetBlockByNumberTool` – detailed block payload (transactions, miner, timestamp).
- `GetTransactionByHashTool` – fetch by tx hash or `(block_number, tx_index)` combo.
- `GetAccountTransactionsTool` – paginated tx history per address with optional block/timestamp filters.

### Accounts & Portfolios
- `GetAccountBalanceTool` – native coin balance with optional `to_block`.
- `GetAccountTokensTool` – ERC-20 balances, limit/page params, optional contract filter.
- `GetAccountNFTsTool` – NFT holdings, same pagination semantics.

### Contracts & Tokens
- `ContractCallTool` – invoke read-only contract functions by supplying ABI JSON and params.
- `GetTokenMetadataTool` – ERC-20 metadata (name, symbol, decimals, total supply).

On success every tool returns Chainbase’s raw JSON payload: `{"code": ..., "message": ..., "data": [...]}`. When anything fails (missing API key, HTTP error, etc.) the wrapper returns `{"error": "..."}` instead of raising, so always check for an `error` key before consuming `data`.

## Usage Examples

### Fetch the latest Ethereum block
```python
from spoon_toolkits.data_platforms.chainbase import GetLatestBlockNumberTool

height_tool = GetLatestBlockNumberTool()
block = await height_tool.execute(chain_id=1)
print(block["data"])
```

### Enumerate ERC-20 holdings for a wallet
```python
from spoon_toolkits.data_platforms.chainbase import GetAccountTokensTool

tokens = await GetAccountTokensTool().execute(
    chain_id=1,
    address="0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045",
    limit=50,
)

for token in tokens.get("data", []):
    print(token["symbol"], token["balance"])
```

### Run a read-only contract call
```python
from spoon_toolkits.data_platforms.chainbase import ContractCallTool

result = await ContractCallTool().execute(
    chain_id=1,
    contract_address="0x6B175474E89094C44Da98b954EedeAC495271d0F",  # DAI
    function_name="totalSupply",
    abi='[{"inputs":[],"name":"totalSupply","outputs":[{"type":"uint256"}],"stateMutability":"view","type":"function"}]',
    params=[],
)
print(result["data"])
```

## FastMCP Server Mode

To expose these tools over SSE (useful for MCP-compatible frontends), run:

```bash
python -m spoon_toolkits.data_platforms.chainbase
# or, in code:
from spoon_toolkits.data_platforms.chainbase import mcp_server
mcp_server.run(transport="sse", host="0.0.0.0", port=8000, path="/sse")
```

Internally, the server mounts three subdomains (`Balance`, `Basic`, `TokenAPI`) so you can enable only the scopes you need.

## Operational Notes

- Chainbase enforces per-key rate limits; handle `{"error": "...429..."}` responses by backing off or narrowing ranges (`limit`, `page`).
- Because the SDK just wraps REST, you can set `CHAINBASE_API_KEY` at runtime (e.g., from a vault) before invoking any tool to support multi-tenant agents.
- For deterministic pipelines, inspect the `code` and `message` fields returned by Chainbase before assuming `data` is present—errors come back with HTTP 200 but non-zero `code`.

---

FILE: Toolkit/data-platforms/desearch.md

`spoon_toolkits.data_platforms.desearch` wraps the official DeSearch SDK in async helpers, builtin Spoon tools, and FastMCP servers so agents can query real-time web, social, and academic sources without writing raw HTTP logic. The async helpers (used by MCP and builtin tools) decorate responses with a `{"error": "..."}` payload when the SDK raises, so callers should check for that key before dereferencing results.

## Environment & Configuration

```bash
export DESEARCH_API_KEY=your_actual_key            # required
export DESEARCH_BASE_URL=https://api.desearch.ai   # optional (not currently consumed)
export DESEARCH_TIMEOUT=30                         # optional (read but not yet wired into SDK calls)
```

- `env.py` loads `.env` via `python-dotenv`, so store the variables beside the process that imports the package.
- `DESEARCH_BASE_URL`/`DESEARCH_TIMEOUT` are parsed for future use but the current helpers always talk to the SDK defaults; changing these env vars today has no effect without modifying the code.
- The helpers enforce `limit >= 10`, mirroring the SDK requirements.

## Package Layout

| Module | Purpose |
| --- | --- |
| `__init__.py` | Mounts `ai_search` and `web_search` FastMCP sub-servers and re-exports helper coroutines plus `mcp_server`. |
| `ai_search_official.py` | Async tools for AI/meta search, Reddit/Twitter feeds, and academic datasets. Decorated with `@mcp.tool()`. |
| `web_search_official.py` | Web search plus Twitter post/link lookups using the official SDK. |
| `builtin_tools.py` | `BaseTool` wrappers (`DesearchAISearchTool`, etc.) for spoon-core usage without MCP. These return the same dicts as the async helpers (including any `"error"` payloads), so callers must handle error cases explicitly. |
| `cache.py` | `time_cache` decorator that memoizes tool responses for ~5 minutes. |
| `example.py`, `test_integration.py`, `README.md` | Usage demos, live API smoke tests, and a deeper quick-start. |

## Tooling Highlights

### AI and Social Search
- `search_ai_data(query, platforms, limit)` aggregates results from web, Reddit, Wikipedia, YouTube, Twitter, ArXiv, and HackerNews.
- `search_social_media(query, platform, limit)` targets Twitter or Reddit directly.
- `search_academic(query, platform, limit)` narrows to ArXiv or Wikipedia research corpora.

### Web and Twitter Search
- `search_web(query, num_results, start)` calls `basic_web_search` and returns snippet-rich results.
- `search_twitter_posts(query, limit, sort)` pulls live tweets with sort control (Top, Latest, etc.).
- `search_twitter_links(query, limit)` surfaces URLs that are trending on Twitter.

### Builtin Tools and Caching
- `DesearchAISearchTool`, `DesearchWebSearchTool`, `DesearchAcademicSearchTool`, and `DesearchTwitterSearchTool` validate API keys up front and expose JSON schemas for planners.
- `time_cache` decorates every MCP tool, preventing duplicate outbound calls during iterative planning loops.

## Usage Examples

### Mount the FastMCP server inside a Spoon agent
```python
from spoon_toolkits.data_platforms.desearch import mcp_server

agent_config = {
    "tools": [mcp_server],   # exposes ai_search.* and web_search.* namespaces
}
```

### Call async helpers directly
```python
from spoon_toolkits.data_platforms.desearch import (
    search_ai_data,
    search_web,
)

async def summarize(topic: str):
    ai = await search_ai_data(query=f"{topic} 2024", platforms="web,reddit,wikipedia", limit=12)
    web = await search_web(query=topic, num_results=5)
    return {"ai": ai, "web": web}
```

### Use builtin tools when working in spoon-core
```python
from spoon_toolkits.data_platforms.desearch.builtin_tools import DesearchAISearchTool

tool = DesearchAISearchTool()
response = await tool.execute(query="Solana MEV research", platforms="web,reddit", limit=10)

if not response.get("success"):
    raise RuntimeError(response.get("error"))

print(response["data"]["results"].keys())
```

## FastMCP Server Mode

Run the package as a server to expose the tools over SSE (compatible with MCP-aware clients):

```bash
python -m spoon_toolkits.data_platforms.desearch
# or in code:
from spoon_toolkits.data_platforms.desearch import mcp_server
mcp_server.run(transport="sse", host="0.0.0.0", port=8000, path="/sse")
```

Internally, the server mounts `ai_search` and `web_search` namespaces, so you can scope access per agent.

## Operational Notes

- Use the provided `test_integration.py` before shipping: it validates your API key and SDK wiring.
- Responses are dictionaries (e.g., `{"query": ..., "results": ..., "count": ...}`); check for `"error"` keys when handling failures.
- `DESEARCH_TIMEOUT` is parsed but currently unused; adjust it only after wiring the value into the helper functions or SDK client initialization in your fork.
- Remove cached entries by restarting the process if you need uncached data while debugging.
- Rate limits come from the DeSearch API; stagger large batches or broaden queries rather than hammering the same endpoint.

---

FILE: Toolkit/data-platforms/thirdweb.md

`spoon_toolkits.data_platforms.third_web` wraps the Thirdweb Insight REST API in async `BaseTool` classes so Spoon agents can fetch contract events, multichain transfers, transactions, and block data without crafting HTTP requests by hand. Some tools require you to supply a `client_id` argument, while others read `THIRDWEB_CLIENT_ID` from the environment—make sure to supply credentials in the format each tool expects.

## Environment & Configuration

```bash
export THIRDWEB_CLIENT_ID=your_client_id          # used by tools that read from env
```

- Tools fall into two credential styles:
  1. `GetContractEventsFromThirdwebInsight`, `GetBlocksFromThirdwebInsight`, and `GetWalletTransactionsFromThirdwebInsight` require a `client_id` argument on every call and never look at `THIRDWEB_CLIENT_ID`.
  2. `GetMultichainTransfersFromThirdwebInsight`, `GetTransactionsTool`, `GetContractTransactionsTool`, and `GetContractTransactionsBySignatureTool` exclusively read `THIRDWEB_CLIENT_ID` and do not expose a per-call override.
- Requests use a 100-second timeout where implemented; a few helpers (notably the block fetcher) currently omit the timeout parameter and will rely on `requests` defaults until updated.
- Each tool catches exceptions and returns either a formatted status string (prefixed with ✅/❌) or a dict like `{"error": "..."}`—errors do not raise, and successful calls may return strings rather than raw JSON.

## Package Layout

| Module | Purpose |
| --- | --- |
| `third_web_tools.py` | Houses every `BaseTool` plus lightweight async test helpers. Some `execute` methods return human-readable strings (with emojis and counts) instead of the raw Insight JSON object—inspect the tool docstrings before assuming `dict` output. Import from `spoon_toolkits.data_platforms.third_web.third_web_tools`. |

## Tooling Highlights

### Events and Transfers
- `GetContractEventsFromThirdwebInsight` - fetch decoded events for a contract + signature (`Transfer(address,address,uint256)`, etc.) with paging metadata. Requires you to pass `client_id` explicitly; returns a status string summarizing the page and event count plus the JSON dump.
- `GetMultichainTransfersFromThirdwebInsight` - scan recent transfers for a list of chain IDs (defaults to USDT events exposed by Insight). Reads `THIRDWEB_CLIENT_ID` from the environment and returns the raw Insight JSON dict.

### Transactions and Blocks
- `GetTransactionsTool` - consolidate recent transactions across multiple chains. Reads `THIRDWEB_CLIENT_ID` and returns the raw Insight JSON dict.
- `GetContractTransactionsTool` - view activity for a single contract. Reads `THIRDWEB_CLIENT_ID`.
- `GetContractTransactionsBySignatureTool` - narrow contract activity down to a specific function signature. Reads `THIRDWEB_CLIENT_ID`.
- `GetBlocksFromThirdwebInsight` - stream the latest blocks per chain with optional sort field and order. Requires a per-call `client_id` and currently returns a formatted status string (`"✅ Success ..."`) rather than the JSON dict; also omits the explicit 100-second timeout, so the default `requests` timeout applies.
- `GetWalletTransactionsFromThirdwebInsight` - list wallet transactions across multiple chains, sorted by block number or timestamp. Requires a per-call `client_id` and returns a status string similar to the block tool.

Depending on the helper, `client_id` may need to be passed explicitly, and successful responses may be either raw JSON dicts or formatted status strings. If you need structured data, parse the portion after the newline in the status strings (they contain the serialized JSON response). Always check for `❌` or an `"error"` key to detect failures because network/Insight errors are caught instead of raised.

## Usage Examples

### Fetch contract events for a signature
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetContractEventsFromThirdwebInsight

tool = GetContractEventsFromThirdwebInsight()
result = await tool.execute(
    client_id="your-client-id",
    chain_id=1,
    contract_address="0xdAC17F958D2ee523a2206206994597C13D831ec7",
    event_signature="Transfer(address,address,uint256)",
    limit=5,
)
if result.startswith("❌"):
    raise RuntimeError(result)

# Returned string includes JSON after the newline
summary, _, raw_json = result.partition("\n")
print(summary)
print(raw_json)
```

### Aggregate transfers across chains
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetMultichainTransfersFromThirdwebInsight

tool = GetMultichainTransfersFromThirdwebInsight()
transfers = await tool.execute(chains=[1, 137, 8453], limit=10)
print(transfers["data"][0])
```

### Inspect wallet transactions with sorting
```python
from spoon_toolkits.data_platforms.third_web.third_web_tools import GetWalletTransactionsFromThirdwebInsight

wallet_tool = GetWalletTransactionsFromThirdwebInsight()
history = await wallet_tool.execute(
    client_id="your-client-id",
    wallet_address="0xabc...",
    chains=[1, 137],
    limit=10,
    sort_by="block_timestamp",
    sort_order="desc",
)
if history.startswith("❌"):
    raise RuntimeError(history)

_, _, raw_history = history.partition("\n")
print(raw_history)
```

## Operational Notes

- Set `THIRDWEB_CLIENT_ID` in your runtime environment for the env-driven tools, and pass `client_id` for the helpers that require it; missing credentials surface as `ValueError` strings in the result.
- HTTP errors are caught and reported in the returned string/dict instead of bubbling up through `requests.raise_for_status()`, so alerting logic should look for `❌` prefixes or `"error"` keys.
- Test helpers at the bottom of `third_web_tools.py` (`test_get_contract_events`, etc.) offer quick sanity checks - run them with `python third_web_tools.py` once your credentials are configured.
- Insight endpoints enforce per-client rate limits; stagger large batch pulls by adjusting `limit` or splitting chain lists across multiple requests.

---

FILE: Toolkit/github/analysis-tools.md

`spoon_toolkits.github.github_analysis_tool` bundles three ready-made `BaseTool` classes that call the GitHub GraphQL API via `GitHubProvider`. Each tool simply returns the raw list of entities (`issues`, `pull requests`, `commits`) interpolated into a single string (e.g., `"GitHub issues: [{...}, ...]"`), so you get the data without crafting queries yourself but will need to parse/count manually if you want aggregates.

## Environment

```bash
export GITHUB_TOKEN=ghp_your_personal_access_token   # repo scope recommended
```

- Each tool pulls `GITHUB_TOKEN` unless you pass `token="..."` explicitly. Missing credentials surface as `ToolResult(error="GitHub token is required...")`.
- GitHub’s GraphQL endpoint enforces a rate limit of 5,000 points/hour per token. These tools request up to 100 nodes per call; batch long-range reports accordingly.

## Tool Catalog

| Tool | Parameters | Raw data returned | Use cases |
| --- | --- | --- | --- |
| `GetGitHubIssuesTool` | `owner`, `repo`, `start_date`, `end_date`, optional `token` | List of up to 100 issues with `title`, `state`, labels, comment totals, author, timestamps | Bug triage dashboards, changelog assembly, governance transparency |
| `GetGitHubPullRequestsTool` | Same window parameters | List of up to 100 pull requests with merge info, review counts, commit totals, labels | Contributor scorecards, weekly PR digests |
| `GetGitHubCommitsTool` | Same window parameters | Default branch history (first 100 commits) including message, author name/email, additions/deletions, `changedFiles` | Release notes, productivity metrics, feature tracking |

All parameters accept ISO `YYYY-MM-DD` strings. The window is inclusive and aligned to UTC. Because the code returns only the raw list embedded in a string, there is no built-in `total_count` or `date_range` metadata—add that logic in your calling code if required.

## Usage Patterns

### Issues snapshot
```python
from spoon_toolkits.github.github_analysis_tool import GetGitHubIssuesTool

issues_tool = GetGitHubIssuesTool()
issues_result = await issues_tool.execute(
    owner="XSpoonAi",
    repo="spoon-core",
    start_date="2024-01-01",
    end_date="2024-02-01",
)

if issues_result.error:
    raise RuntimeError(issues_result.error)

raw_issues = issues_result.output  # e.g., "GitHub issues: [{...}, {...}]"
print(raw_issues)

# Optionally convert the trailing list into Python data:
from ast import literal_eval
issues_payload = literal_eval(raw_issues.replace("GitHub issues: ", "", 1))
for issue in issues_payload:
    print(issue["title"], issue["state"])
```

### Weekly contributor digest (issues → PRs → commits)
```python
from datetime import date, timedelta
from spoon_toolkits.github.github_analysis_tool import (
    GetGitHubIssuesTool,
    GetGitHubPullRequestsTool,
    GetGitHubCommitsTool,
)

today = date.today()
week_ago = today - timedelta(days=7)
window = dict(
    owner="XSpoonAi",
    repo="spoon-core",
    start_date=week_ago.isoformat(),
    end_date=today.isoformat(),
)

issues = await GetGitHubIssuesTool().execute(**window)
prs = await GetGitHubPullRequestsTool().execute(**window)
commits = await GetGitHubCommitsTool().execute(**window)

if any(result.error for result in (issues, prs, commits)):
    raise RuntimeError("GitHub API call failed")

summary = {
    "issues": len(literal_eval(issues.output.replace("GitHub issues: ", "", 1))),
    "pull_requests": len(literal_eval(prs.output.replace("GitHub pull requests: ", "", 1))),
    "commits": len(literal_eval(commits.output.replace("GitHub commits: ", "", 1))),
}
```

## Error Handling Tips

- Inspect `ToolResult.error` before consuming `output`. Authentication failures, invalid repo names, or GraphQL validation errors are surfaced there.
- If GitHub throttles you, the GraphQL API returns `errors: [{"type": "RATE_LIMITED", ...}]`; the tool passes that text through the `error` field.
- Dates outside the repository lifetime simply return an empty list (so the output string becomes `"GitHub <entity>: []"`); no error is raised.

Use these tools when you need standardized analytics quickly. If you outgrow the default fields, switch to the lower-level `GitHubProvider` documented separately to craft custom queries.

---

FILE: Toolkit/github/provider.md

`spoon_toolkits.github.github_provider.GitHubProvider` gives you a thin but flexible GraphQL client built on `gql`. Use it when the stock analysis tools don’t expose the fields or filtering you need.

## Environment & Auth

```bash
export GITHUB_TOKEN=ghp_your_personal_access_token   # repo scope recommended
```

- The constructor reads `GITHUB_TOKEN` automatically and raises `ValueError` immediately if no token is available. Pass `GitHubProvider(token="ghp_...")` to override per request.
- The underlying transport hits `https://api.github.com/graphql`. Rate limits (5,000 points/hour) and GraphQL errors are raised as Python exceptions, so always wrap calls in `try/except`.

## Initialization

```python
from spoon_toolkits.github.github_provider import GitHubProvider

provider = GitHubProvider()
```

Provide the token only once per provider instance. Each method is `async`, so you can reuse the same instance across awaits.

When you need deterministic cleanup (for example in long-running services), use the provider as a context manager; it will close the underlying `gql.Client` transport automatically:

```python
with GitHubProvider() as provider:
    repo = await provider.fetch_repository_info("XSpoonAi", "spoon-core")
```

## Built-in Methods

| Method | What it queries | Useful fields returned |
| --- | --- | --- |
| `fetch_issues(owner, repo, start_date, end_date)` | `issues` connection filtered by creation date | `title`, `state`, `labels`, `comments.totalCount`, `author.login`, timestamps |
| `fetch_pull_requests(...)` | `pullRequests` connection in the same date window | `mergedAt`, `reviews.totalCount`, `commits.totalCount`, labels, author |
| `fetch_commits(...)` | `defaultBranchRef.target.history` limited to 100 commits (GraphQL limit) | `message`, `committedDate`, `additions`, `deletions`, `changedFiles`, `author` |
| `fetch_repository_info(owner, repo)` | `repository` node | `stargazerCount`, `forkCount`, `watcherCount`, open issue/pr counts, topics, license |

The default history queries request 100 nodes. To page further, copy the query strings in `github_provider.py` and extend them with `after` cursors.

## Example: Custom GraphQL + Provider Methods

```python
from spoon_toolkits.github.github_provider import GitHubProvider
from gql import gql

provider = GitHubProvider()

# Use the prebuilt helper
repo = await provider.fetch_repository_info("XSpoonAi", "spoon-core")
print("Stars:", repo["stargazerCount"])

# Extend with your own query
custom_query = gql("""
  query($owner: String!, $repo: String!) {
    repository(owner: $owner, name: $repo) {
      releases(last: 5) {
        nodes {
          name
          tagName
          publishedAt
        }
      }
    }
  }
""")

result = provider.client.execute(custom_query, variable_values={"owner": "XSpoonAi", "repo": "spoon-core"})
```

## Error Handling & Best Practices

- Each method wraps `client.execute` and raises `Exception` with the underlying GraphQL message (`RATE_LIMITED`, `NOT_FOUND`, `FORBIDDEN`, etc.). Catch `Exception` at call sites to degrade gracefully.
- For long-running agents, instantiate the provider once and reuse it—this avoids fetching the schema repeatedly.
- GraphQL schemas are fetched on first transport use (`fetch_schema_from_transport=True`). When developing offline, consider setting that flag to `False` in a forked provider to skip the extra call.

Reach for `GitHubProvider` when you need to compose bespoke queries, add pagination, or stitch multiple GraphQL responses together within a single agent step.

---

FILE: Toolkit/index.md

---
id: index
title: Toolkit Overview
---

# Toolkit Overview

Welcome to the Spoon Toolkit catalog. Everything here is a ready-to-use agent helper that ships with the SpoonOS runtime. The toolkit is organized by domain so you can jump straight to the integrations you need:

- **Crypto** – Market data, EVM utilities, Neo RPC helpers, Chainbase-powered analytics, and more. Use these when your agent needs to trade, monitor, or analyze on-chain activity.
- **Data Platforms** – Providers such as Chainbase, Thirdweb, and Desearch for structured blockchain data without writing bespoke HTTP clients.
- **GitHub Intelligence** – Repository scanners and provider hooks that summarize repos, issues, and activity.
- **Social Media** – Discord, Telegram, Twitter, and Email helpers for outreach and notification workflows.
- **Storage** – Off-chain storage bridges (AIOZ, Oort, Foureverland) so agents can archive artifacts or publish outputs.

Navigate the sidebar on the left to dive into any module. Each page documents the environment variables, key classes, and concrete usage snippets for that toolkit.

---

FILE: Toolkit/memory/mem0.md

# Memory Tools (Mem0)

Spoon-toolkit provides Mem0-powered tools that plug into spoon-core agents for long-term memory. They wrap `spoon_ai.memory.mem0_client.SpoonMem0` and expose a consistent tool interface. `ToolResult.output` carries the raw Mem0 response (not a formatted string); validation failures or an unready client return `ToolResult.error` (e.g., “Client not ready”, “No content provided.”) instead of raising.

## Available tools
- `AddMemoryTool` — store text or conversation snippets.
- `SearchMemoryTool` — semantic/natural-language search over stored memories.
- `GetAllMemoryTool` — list memories (with paging/filters).
- `UpdateMemoryTool` — update an existing memory by memory_id.
- `DeleteMemoryTool` — delete a memory by memory_id.

All tools accept `mem0_config` (api_key/user_id/collection/metadata/filters/etc.) or an injected `SpoonMem0` client. If `mem0ai` or `MEM0_API_KEY` is missing, client initialization may fail; otherwise an unready client yields `ToolResult.error` rather than an exception.

Parameter merging behavior:
- `user_id` defaults to `mem0_config.user_id`/`agent_id` (or the client’s user_id) and is also injected into filters if missing.
- `collection`, `metadata`, and `filters` from the injected client/config are merged into each call; per-call metadata/filters override on conflict.
- `async_mode` is only forwarded when passed to `AddMemoryTool.execute`; it is not auto-propagated from `mem0_config` by these wrappers (rely on `SpoonMem0` defaults if needed).

## Quick usage (agent-side)
```python
from spoon_ai.agents.spoon_react import SpoonReactAI
from spoon_ai.chat import ChatBot
from spoon_ai.tools.tool_manager import ToolManager
from spoon_toolkits.memory import (
    AddMemoryTool, SearchMemoryTool, GetAllMemoryTool, UpdateMemoryTool, DeleteMemoryTool
)

MEM0_CFG = {
    "user_id": "defi_user_001",
    "metadata": {"project": "demo"},
    "async_mode": False,   # sync writes to avoid read-after-write delays
    "collection": "demo_mem",
}

class MemoryAgent(SpoonReactAI):
    mem0_config = MEM0_CFG
    available_tools = ToolManager([
        AddMemoryTool(mem0_config=MEM0_CFG),
        SearchMemoryTool(mem0_config=MEM0_CFG),
        GetAllMemoryTool(mem0_config=MEM0_CFG),
        UpdateMemoryTool(mem0_config=MEM0_CFG),
        DeleteMemoryTool(mem0_config=MEM0_CFG),
    ])

agent = MemoryAgent(
    llm=ChatBot(llm_provider="openrouter", model_name="anthropic/claude-3.5-sonnet", enable_long_term_memory=False),
)
```

## Demo flow (from `spoon-core/examples/mem0_tool_agent.py`)
The example walks through capture → recall → update → delete:

1) **Capture**: `add_memory` with user preferences. Immediately verify with `get_all_memory` (same `user_id`) and then `search_memory` to show stored content.
2) **Recall**: Build a fresh agent with the same `mem0_config` and `search_memory` to retrieve prior preferences.
3) **Update**: `add_memory` new preference + `update_memory` a specific record (by id) to reflect the pivot; then `search_memory` again to confirm recency.
4) **Delete**: `delete_memory` the updated record; `get_all_memory` to show remaining memories.

Run the example:
```bash
python spoon-core/examples/mem0_tool_agent.py
```

## Tool parameters (summary)
- Shared: `user_id`, `metadata`, `filters`, `collection` inherited via `mem0_config` or passed per-call.
- Add: `content` or `messages`, `role` (default `user`), `async_mode`.
- Search: `query`, `limit`/`top_k`, optional filters.
- GetAll: `page`, `page_size`/`limit`/`top_k`, filters.
- Update: `memory_id` (required), `text`, `metadata`.
- Delete: `memory_id` (required).

## Best practices
- Keep a stable `user_id`/`collection` for scoping; pass it explicitly on each call for consistency.
- Use `async_mode=False` when you need immediate read-after-write in demos/tests.
- If you pass a custom `SpoonMem0` via `mem0_client`, you can reuse a single client across tools to avoid repeated pings/initialization.

---

FILE: Toolkit/social-media/index.md

---
title: Social Media Toolkit
---

# Social Media Toolkit

`spoon_toolkits.social_media` centralizes outbound and bidirectional messaging for Discord, Telegram, Twitter/X, and Email. All adapters share the same base classes, request/response models, and MCP-friendly interfaces, so you can swap channels without rewriting agent logic.

## Shared Architecture

- **Base classes** – Every tool inherits from `SocialMediaToolBase`, which defines `send_message()` and `validate_config()`. Notification-only adapters extend `NotificationToolBase`; interactive bots (Discord/Telegram) extend `InteractiveToolBase` and add `start_bot()` / `stop_bot()`.
- **Models** – Most `execute()` methods accept a `MessageRequest` subclass (e.g., `DiscordMessageRequest`, `EmailMessageRequest`) and return a `MessageResponse` with `{success: bool, message: str, data?: dict}`. Twitter exposes dedicated entry points `execute_tweet`, `execute_reply`, and `execute_like` instead of a single `execute()`.
- **Helper coroutines** – Each adapter exports convenience helpers: Discord/Telegram/Email provide `send_*` functions that create the tool, run `validate_config()`, and return `{success, message}`; Twitter offers `post_tweet`, `reply_to_tweet`, and `like_tweet` helpers that also include a `data` payload on success.
- **Config validation** – `validate_config()` logs a warning when required credentials are missing; call it during startup to fail fast rather than silently dropping messages.

## Environment Reference

| Channel | Required variables | Optional variables |
| --- | --- | --- |
| **Discord** | `DISCORD_BOT_TOKEN` | `DISCORD_DEFAULT_CHANNEL_ID` |
| **Telegram** | `TELEGRAM_BOT_TOKEN` | `TELEGRAM_DEFAULT_CHAT_ID`* |
| **Twitter/X** | `TWITTER_CONSUMER_KEY`, `TWITTER_CONSUMER_SECRET`, `TWITTER_ACCESS_TOKEN`, `TWITTER_ACCESS_TOKEN_SECRET` | `TWITTER_BEARER_TOKEN`, `TWITTER_USER_ID` |
| **Email (SMTP)** | `EMAIL_SMTP_SERVER`, `EMAIL_SMTP_PORT`, `EMAIL_SMTP_USER`, `EMAIL_SMTP_PASSWORD`, `EMAIL_FROM` | `EMAIL_DEFAULT_RECIPIENTS` |

Set these variables before importing the corresponding tool. Missing credentials surface when `validate_config()` runs or when the client attempts its first API call.

\*The current Telegram implementation falls back to a hard-coded placeholder chat ID when `chat_id` isn’t provided. Supply the chat explicitly or override `default_chat_id` after instantiation until configuration support is added.

## Discord

- **Module**: `discord_tool.py`
- **Client**: `spoon_ai.social_media.discord.DiscordClient`
- **Key methods**:
  - `send_message(message, channel_id=None)` – pushes text to a specific channel or falls back to `DISCORD_DEFAULT_CHANNEL_ID`.
  - `start_bot(agent=None)` / `stop_bot()` – wraps the async gateway loop for interactive bots; pass your Spoon agent at construction time so inbound events are routed through planners.
  - `execute(DiscordMessageRequest)` – MCP entry point; accepts `message` and optional `channel_id`.
- **Usage**:

```python
from spoon_toolkits.social_media.discord_tool import DiscordTool

discord = DiscordTool(agent=my_agent)
if discord.validate_config():
    await discord.send_message("Graph agent finished ingesting blocks")
```

Errors (invalid token, missing intents, closed WebSocket) bubble up as exceptions; wrap calls with retries or queue them through a supervisor task to avoid duplicate sessions.

## Telegram

- **Module**: `telegram_tool.py`
- **Client**: `spoon_ai.social_media.telegram.TelegramClient`
- **Key methods**:
  - `send_message(message, chat_id=None)` – proactive messaging. If the tool was instantiated without an agent, it spins a temporary client just for this send call.
  - `start_bot()` / `stop_bot()` – register webhook/polling handlers so agents can react to user prompts in real time.
  - `execute(TelegramMessageRequest)` – MCP-friendly send wrapper.
- **Usage**:

```python
from spoon_toolkits.social_media.telegram_tool import TelegramTool

telegram = TelegramTool()
await telegram.send_message("🔔 Validator risk threshold exceeded")
```

Rate limits are per bot token; Telegram will throttle after ~30 messages/s. `send_message` logs failures and returns `False` instead of raising, so check the boolean result and trigger your own retry/backoff logic. Provide a `chat_id` unless you have overridden `default_chat_id`, because the bundled placeholder (`"0000000000"`) is not a usable channel.

## Twitter / X

- **Module**: `twitter_tool.py`
- **Client**: `spoon_ai.social_media.twitter.TwitterClient`
- **Key methods**:
  - `execute_tweet(TwitterTweetRequest)` / `execute_reply(TwitterReplyRequest)` / `execute_like(TwitterLikeRequest)` – MCP-friendly entry points for posting, replying, and liking.
  - `send_message(message, tags=None)` – convenience layer that appends hashtags or mentions before delegating to the client.
  - `post_tweet(message)` – publish a new status.
  - `reply_to_tweet(tweet_id, message)` – threaded replies.
  - `like_tweet(tweet_id)` – reaction utility for engagement workflows.
  - `read_timeline(count=None)` / `get_tweet_replies(tweet_id, count)` – read helpers for downstream analysis.
- **Usage**:

```python
from spoon_toolkits.social_media.twitter_tool import TwitterTool

twitter = TwitterTool()
if twitter.validate_config():
    twitter.post_tweet("📈 Treasury rebalance complete. New target: BTC 40%, ETH 35%.")
```

Be mindful of Twitter v2 rate limits (tweets: 300 per 3 hours; likes/replies have separate quotas). Wrap the client in a queue or add exponential backoff to avoid HTTP 429 responses.

## Email (SMTP)

- **Module**: `email_tool.py`
- **Client**: `spoon_ai.social_media.email.EmailNotifier`
- **Key methods**:
  - `send_message(message, to_emails=None, subject="Crypto Monitoring Alert", html_format=True)` – send transactional emails; defaults to `EMAIL_DEFAULT_RECIPIENTS` when `to_emails` is omitted.
  - `execute(EmailMessageRequest)` – MCP wrapper returning `MessageResponse`.
- **Usage**:

```python
from spoon_toolkits.social_media.email_tool import EmailTool

email_tool = EmailTool()
await email_tool.send_message(
    message="<h2>Risk Alert</h2><p>New honeypot detected.</p>",
    to_emails=["security@example.com"],
    subject="🚨 Token Risk Detected",
)
```

Most SMTP providers enforce connection limits and per-minute quotas; reuse the same tool instance to keep TLS sessions warm, and handle `smtplib` exceptions to trigger retries or fallback to another channel.

## MCP & Convenience Functions

Each adapter exposes an `execute()` method that accepts the corresponding Pydantic request model, making it trivial to register the tool with FastMCP:

```python
from spoon_toolkits.social_media.discord_tool import DiscordTool, DiscordMessageRequest

discord = DiscordTool()
response = await discord.execute(DiscordMessageRequest(message="Hello MCP"))
```

For lightweight scripts, use the helper coroutines:

```python
from spoon_toolkits.social_media.discord_tool import send_discord_message
from spoon_toolkits.social_media.email_tool import send_email
from spoon_toolkits.social_media.twitter_tool import post_tweet

await send_discord_message("Indexing complete")
await send_email("Pipeline healthy", ["ops@example.com"])
tweet_result = await post_tweet("Publishing release notes #SpoonAI")
```

Discord/Telegram/Email helpers return `{success, message}` dictionaries. Twitter helpers also include a `data` object with the API response (tweet ID, etc.) when successful.

## Operational Tips

- **Credential hygiene**: Load all tokens via environment variables or a secrets manager; never hardcode them in agent configs.
- **Retry/backoff**: Social APIs frequently throttle. Catch exceptions from `send_message`, apply exponential backoff, or offload to a job queue.
- **Bot lifecycle**: For Discord/Telegram `start_bot()`, run the bot in a dedicated task or process and call `stop_bot()` during shutdown to avoid orphaned connections.
- **Multi-channel redundancy**: Pair channels (e.g., Telegram + Email) for critical alerts so a single API outage doesn’t silence notifications.

---

FILE: Toolkit/storage/aioz.md

---
title: AIOZ Storage
---

# AIOZ Storage

`spoon_toolkits.storage.aioz` adapts AIOZ's S3-compatible service for SpoonOS agents.

## Environment

```bash
export AIOZ_ENDPOINT_URL=https://gateway.aioz.io
export AWS_ACCESS_KEY=your_access_key
export AWS_SECRET_KEY=your_secret_key
```

Set optional `BUCKET_NAME` when using the helper coroutines in `aioz_tools.py`; the tools themselves only require the three variables above.

## Tools

- `AiozListBucketsTool` – list all accessible buckets.
- `UploadFileToAiozTool` – push a local file to a bucket.
- `DownloadFileFromAiozTool` – fetch an object to a local path.
- `DeleteAiozObjectTool` – remove a single object.
- `GenerateAiozPresignedUrlTool` – produce temporary download URLs.

Bucket/object mutations emit emoji-prefixed status strings (✅ success / ❌ failure). Wrap calls in agents that interpret the prefix to decide next steps.

### Return semantics & shared helpers
- Success responses look like `✅ Uploaded 'file' to 'bucket'`.
- Failures bubble up `botocore` messages but remain human readable: `❌ Failed to upload ... (Error: ...)`.
- `AiozListBucketsTool` returns a newline-separated string (emojis included) such as `📁 bucket-a`; parse the string if you need structured output.
- `GenerateAiozPresignedUrlTool` returns the raw URL string on success (no emoji prefix). Failures return `❌ ...` strings coming from `_generate_presigned_url`.
- `GenerateAiozPresignedUrlTool` accepts `expires_in` in seconds; the default is 3600. AIOZ supports up to 7 days—set higher values when sharing large datasets.

## Usage Examples

### Upload & presign
```python
from spoon_toolkits.storage.aioz.aioz_tools import UploadFileToAiozTool, GenerateAiozPresignedUrlTool

uploader = UploadFileToAiozTool()
status = await uploader.execute(bucket_name="research-artifacts", file_path="/tmp/report.pdf")
print(status)

# Object keys default to the filename (here: report.pdf). Rename after upload if needed.
presigner = GenerateAiozPresignedUrlTool()
url = await presigner.execute(bucket_name="research-artifacts", object_key="report.pdf", expires_in=900)
print(url)
```

### Module self-test
Running the module directly executes the async test harness in `aioz_tools.py`, which performs list/upload/presign/download/delete flows using the configured credentials and `BUCKET_NAME`. Use this as a credential check, but be aware it will create/delete real objects rather than accept CLI arguments.

## Operation Tips
- **Endpoint style**: `S3Tool` forces path-style URLs; keep bucket names DNS-safe to avoid signature issues.
- **Access denied**: If you see `❌ Failed ... (Error: An error occurred (AccessDenied))`, verify the access key/secret pair. AIOZ keys are distinct from AWS IAM keys.
- **Large uploads**: For >5GB objects, extend `AIOZMultipartUploadTool` (not shipped yet) using `S3Tool`'s `_create_multipart_upload` helpers; standard uploads will fail with `EntityTooLarge`.
- **Presign mismatch**: If the generated URL returns 403, ensure the bucket policy allows `s3:GetObject` for presigned requests and that your system clock is accurate.

## MCP / Agent integration

Each class inherits `BaseTool`. Register them in FastMCP like:
```python
from spoon_toolkits.storage.aioz.aioz_tools import AiozListBucketsTool

tool = AiozListBucketsTool()
result = await tool.execute()
```
To expose via FastMCP server, import the tool into your MCP registry or call the module’s `main` script. Within Spoon agents, include `Aioz*` tools in the tool set and pass bucket parameters directly from planner prompts.

---

FILE: Toolkit/storage/foureverland.md

# 4EVERLAND Storage

`spoon_toolkits.storage.foureverland` integrates the 4EVERLAND decentralized storage service.

## Environment

```bash
export FOUREVERLAND_ENDPOINT_URL=https://endpoint.4everland.org
export FOUREVERLAND_ACCESS_KEY=your_access_key
export FOUREVERLAND_SECRET_KEY=your_secret_key
export FOUREVERLAND_BUCKET_NAME=default_bucket  # optional for helper scripts
```

## Tools

- `ListFourEverlandBuckets`
- `UploadFileToFourEverland`
- `DownloadFileFromFourEverland`
- `DeleteFourEverlandObject`
- `GenerateFourEverlandPresignedUrl`

Object listing currently requires calling the 4EVERLAND console or a custom script; there is no dedicated `ListObjectsFourEverland` helper yet.

All inherit from `FourEverlandStorageTool`, which itself extends `S3Tool`. That means you can expect identical method signatures and status string formats across the storage adapters, simplifying multi-provider automation.

## Return formats & shared behavior
- Most methods return strings starting with `✅`/`❌`. Agents can parse the first character to branch logic quickly.
- `ListFourEverlandBuckets` returns a newline-separated string with emoji prefixes (e.g., `📁 bucket-name`). Parse the string manually if structured data is required.
- `GenerateFourEverlandPresignedUrl` returns the presigned URL directly (no emoji prefix) and accepts `expires_in` (default 3600). 4EVERLAND caps presigned URLs at 24h—higher values raise a validation error.
- Exceptions from boto3 bubble up only when the helper must return structured data; otherwise they’re converted to the `❌ ... (Error: ...)` string.
- Upload helpers derive the destination key from `os.path.basename(file_path)`; rename via `copy_object` after upload if you need nested prefixes.

## Usage examples

```python
from spoon_toolkits.storage.foureverland.foureverland_tools import (
    UploadFileToFourEverland,
    ListFourEverlandBuckets,
)

uploader = UploadFileToFourEverland()
print(await uploader.execute(bucket_name="governance-data", file_path="/tmp/summary.json"))

lister = ListFourEverlandBuckets()
print(await lister.execute())
```

## Provider-specific notes

- **Endpoint nuance**: 4EVERLAND requires HTTPS; unsigned HTTP calls fail with `SSL required`. Ensure `FOUREVERLAND_ENDPOINT_URL` includes `https://`.
- **Bucket namespace**: Bucket names are global per account. If `CreateBucket` fails with `BucketAlreadyOwnedByYou`, delete or reuse the existing one.
- **Presign errors**: 4EVERLAND enforces lowercase bucket names; uppercase characters cause signature mismatches (403).

## MCP / agent integration

Register any `FourEverland*` tool with FastMCP:
```python
from spoon_toolkits.storage.foureverland.foureverland_tools import GenerateFourEverlandPresignedUrl

tool = GenerateFourEverlandPresignedUrl()
url = await tool.execute(bucket_name="datasets", object_key="daily.csv")
```
Because these classes inherit `BaseTool`, they plug into Spoon agent tool lists directly. For service-style exposure, mount them in your MCP server alongside other storage providers.

---

FILE: Toolkit/storage/oort.md

---
title: OORT Storage
---

# OORT Storage

`spoon_toolkits.storage.oort` provides S3-style tools for the OORT network.

## Environment

```bash
export OORT_ENDPOINT_URL=https://s3.oortech.com
export OORT_ACCESS_KEY=your_access_key
export OORT_SECRET_KEY=your_secret_key
```

## Tools

- `OortCreateBucketTool` / `OortDeleteBucketTool`
- `OortListBucketsTool`
- `OortListObjectsTool`
- `OortUploadFileTool` / `OortDownloadFileTool`
- `OortDeleteObjectTool` / `OortDeleteObjectsTool`
- `OortGeneratePresignedUrlTool`

These tools inherit retry and formatting logic from `S3Tool`; responses are concise strings suited for immediate user feedback or logging. Use `object_keys` arrays for batch deletions when cleaning up agent artifacts.

## Return & error semantics

- Bucket/object operations return emoji-prefixed strings (`✅ Created bucket ...`, `❌ Failed to delete object ... (Error: AccessDenied)`); parse them if you need structured status codes.
- `OortListObjectsTool` returns a newline-separated string of bullet points (e.g., `• key (Size: 123)`), and `OortListBucketsTool` emits emoji-prefixed bucket names. Convert these strings yourself if your workflow requires JSON. Errors raised by `_list_objects` propagate as `RuntimeError`, so wrap the call if you need graceful handling.
- Batch delete accepts an `object_keys` list and replies with the count requested for deletion (`🗑️ Deleted N objects ...`). It does not currently surface per-key AWS errors.
- Presigned URLs honour `expires_in` (default 3600). OORT supports up to 12 hours per token. The `OortGeneratePresignedUrlTool` returns the raw URL string without emoji prefixes.
- Boto3 exceptions only bubble up when returning structured data (list objects) or presigned URL failures; otherwise they are embedded in the failure string.
- Upload helpers derive the destination key from `os.path.basename(file_path)`. Use `_copy_object` if you need to rename or place objects under prefixes.

## Usage examples

```python
from spoon_toolkits.storage.oort.oort_tools import (
    OortCreateBucketTool,
    OortUploadFileTool,
    OortDeleteObjectsTool,
)

creator = OortCreateBucketTool()
print(await creator.execute(bucket_name="agent-artifacts"))

uploader = OortUploadFileTool()
print(await uploader.execute(bucket_name="agent-artifacts", file_path="/tmp/logs.zip"))

deleter = OortDeleteObjectsTool()
print(await deleter.execute(bucket_name="agent-artifacts", object_keys=["logs.zip", "old-report.pdf"]))
```

CLI check:
```
python spoon_toolkits/storage/oort/oort_tools.py list-buckets
```
When run without arguments the module executes the async tests defined at the bottom (create/upload/download flows). Treat it as an integration check rather than an argument-driven CLI.

## Operational Tips

- **Batch delete**: `OortDeleteObjectsTool` leverages `_delete_objects`; pass up to 1000 keys per call, but expect only a count in the response. Retry the entire batch or reissue with smaller sets if something fails.
- **Bucket lifecycle**: Newly created buckets may take a few seconds to propagate; handle `BucketAlreadyExists` by retrying with backoff or generating unique names.
- **Endpoint**: Keep `OORT_ENDPOINT_URL` aligned with your account’s region; otherwise you may see `PermanentRedirect`.
- **Permissions**: Ensure your key pair has `s3:ListAllMyBuckets` and object-level permissions; lacking one results in `❌ ... AccessDenied`.

## MCP / agent usage

```python
from spoon_toolkits.storage.oort.oort_tools import OortGeneratePresignedUrlTool

tool = OortGeneratePresignedUrlTool()
url = await tool.execute(bucket_name="agent-artifacts", object_key="logs.zip", expires_in=600)
```

Because every tool extends `BaseTool`, add them to your agent’s toolset or mount them in a FastMCP server to share across workspaces.

